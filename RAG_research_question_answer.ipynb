{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "###install required libraries \n",
    "# !pip install langchain langchain_huggingface chromadb transformers\n",
    "# !pip install \"unstructured[pdf]\" \n",
    "# !pip install langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"description \n",
    "The provided code uses Langchain's DirectoryLoader to load all PDF documents from a specified directory. \n",
    "It uses the glob=\"*.pdf\" pattern to match and load only .pdf files. The load() method returns a list of Document \n",
    "objects containing the content from the PDFs, which can then be used for further processing or analysis.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Import the necessary class from langchain.document_loaders to load PDFs from a directory\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# Create an instance of DirectoryLoader to load PDF documents from a specified directory\n",
    "loader = DirectoryLoader(\n",
    "    \"./research_ppr\",  # Path to the directory containing the PDFs\n",
    "    glob=\"*.pdf\",  # Specify a pattern to match the PDF files (e.g., \"*.pdf\" for all PDFs)\n",
    ")\n",
    "\n",
    "# Load the documents from the directory\n",
    "documents = loader.load()  # This loads the matched PDF files and returns them as a list of Document objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'research_ppr/Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='information\\n\\nReview Recurrent Neural Networks: A Comprehensive Review of Architectures, Variants, and Applications\\n\\nIbomoiye Domor Mienye 1,∗,†\\n\\n, Theo G. Swart 1,†\\n\\nand George Obaido 2,†\\n\\n1\\n\\nInstitute for Intelligent Systems, University of Johannesburg, Johannesburg 2006, South Africa; tgswart@uj.ac.za\\n\\n2 Center for Human-Compatible Artificial Intelligence (CHAI), Berkeley Institute for Data Science (BIDS),\\n\\nUniversity of California, Berkeley, CA 94720, USA; gobaido@berkeley.edu\\n\\nCorrespondence: ibomoiyem@uj.ac.za †\\n\\nThese authors contributed equally to this work.\\n\\nAbstract: Recurrent neural networks (RNNs) have significantly advanced the field of machine learn- ing (ML) by enabling the effective processing of sequential data. This paper provides a comprehensive review of RNNs and their applications, highlighting advancements in architectures, such as long short-term memory (LSTM) networks, gated recurrent units (GRUs), bidirectional LSTM (BiLSTM), echo state networks (ESNs), peephole LSTM, and stacked LSTM. The study examines the application of RNNs to different domains, including natural language processing (NLP), speech recognition, time series forecasting, autonomous vehicles, and anomaly detection. Additionally, the study discusses recent innovations, such as the integration of attention mechanisms and the development of hybrid models that combine RNNs with convolutional neural networks (CNNs) and transformer architec- tures. This review aims to provide ML researchers and practitioners with a comprehensive overview of the current state and future directions of RNN research.\\n\\nKeywords: deep learning; GRU; LSTM; machine learning; NLP; RNN\\n\\nCitation: Mienye, I.D.; Swart, T.G.;\\n\\n1. Introduction\\n\\nObaido, G. Recurrent Neural\\n\\nNetworks: A Comprehensive Review\\n\\nof Architectures, Variants, and\\n\\nApplications. Information 2024, 15, 517.\\n\\nhttps://doi.org/10.3390/info15090517\\n\\nAcademic Editor: María N. Moreno\\n\\nDeep learning (DL) has reshaped the field of artificial intelligence (AI), driving ad- vancements in a wide array of applications, from image recognition and natural language processing (NLP) to autonomous driving and medical diagnostics [1–5]. This rapid growth is fueled by the increasing availability of big data, advancements in computing power, and the development of sophisticated algorithms [6–9]. As DL models continue to evolve, they are increasingly being deployed in critical sectors, demonstrating their ability to outperform traditional machine learning (ML) techniques in handling complex tasks.\\n\\nGarcía\\n\\nReceived: 21 July 2024\\n\\nRevised: 22 August 2024\\n\\nAccepted: 23 August 2024\\n\\nPublished: 25 August 2024\\n\\nRecurrent neural networks (RNNs) are a class of deep learning models that are funda- mentally designed to handle sequential data [10,11]. Unlike feedforward neural networks, RNNs possess the unique feature of maintaining a memory of previous inputs by using their internal state (memory) to process sequences of inputs [12]. This makes them ideally suited for applications such as natural language processing, speech recognition, and time series forecasting, where context and the order of data points are crucial.\\n\\nCopyright: © 2024 by the authors.\\n\\nLicensee MDPI, Basel, Switzerland.\\n\\nThis article is an open access article\\n\\ndistributed under\\n\\nthe terms and\\n\\nconditions of the Creative Commons\\n\\nAttribution (CC BY) license (https://\\n\\ncreativecommons.org/licenses/by/\\n\\nThe inception of RNNs dates back to the 1970s, with foundational work by Werbos [13], which introduced the concept of backpropagation through time (BPTT) that laid the foun- dation for training recurrent neural networks. However, RNNs struggled with practical applications due to the vanishing gradient problem, where gradients either grow or shrink exponentially during backpropagation [14]. Meanwhile, the introduction of Long Short- Term Memory (LSTM) networks by Hochreiter and Schmidhuber [15] was a turning point for RNNs, allowing for the learning of dependencies over much longer periods. Addi- tionally, gated recurrent units (GRUs), proposed by Cho et al. [16], offered a simplified alternative to LSTM while maintaining comparable performance.\\n\\n4.0/).\\n\\nInformation 2024, 15, 517. https://doi.org/10.3390/info15090517\\n\\nhttps://www.mdpi.com/journal/information\\n\\nInformation 2024, 15, 517\\n\\nOver the years, these RNN architectures have been applied in different fields, achieving excellent performance [17–19]. Despite their advancements and adoption in various fields, RNNs have continued to evolve. Specifically, the increasing complexity of data and tasks in recent years has driven continuous innovations in RNN architectures and variants. These developments have expanded the application of RNNs from simple sequence prediction to complex tasks such as multimodal learning and real-time decision-making systems.\\n\\nRecent studies and reviews have highlighted the significant progress made in the field of RNNs. For example, Lipton et al. [20] provided an overview of the theoretical foundations and applications of RNNs, while Yu et al. [21] focused on the LSTM cell and different variants. Additionally, Tarwani et al. [22] reviewed the application and role of RNNs in natural language processing. However, many of these reviews do not fully capture the latest advancements and applications, given the rapid pace of innovation in this field. Additionally, there remains a gap in the literature that comprehensively covers the latest advancements in RNN architectures and their applications across a broader range of fields. Therefore, this paper aims to fill that gap by providing a comprehensive review of RNNs, assessing their theoretical advancements and practical implementations, as well as cutting-edge applications, thus helping shape future research on neural networks.\\n\\nThe rest of this paper is organized as follows: Section 2 reviews related works. Section 3 covers the fundamentals of RNNs, including basic architecture and components. Section 4 explores advanced RNN variants, such as LSTM and GRU. Section 5 highlights innovations in RNN architectures and training methodologies. Section 6 presents some publicly available datasets used for RNN studies. Section 7 discusses various applications of RNNs in the literature. Section 8 addresses challenges and future research directions. Finally, Section 9 concludes the study.\\n\\n2. Related Works\\n\\nRNNs have been applied in different applications, achieving state-of-the-art perfor- mance, especially in time-series applications. Early developments in RNNs, including locally recurrent, globally feedforward networks, as reviewed by Tsoi and Back [23], and the block-diagonal recurrent neural networks proposed by Mastorocostas and Theocharis [24], laid important groundwork for understanding complex sequence modeling.\\n\\nSeveral reviews have been conducted on RNNs and their applications, each contribut- ing to the understanding and development of the field. For instance, Dutta et al. [25] provided a comprehensive overview of the theoretical foundations of RNNs and their applications in sequence learning. Their review highlighted the challenges associated with training RNNs, particularly the vanishing gradient problem, and discussed the ad- vancements in LSTM and GRU architectures. However, the review primarily focused on the theoretical aspects and applications of RNNs and did not extensively cover the latest innovations and practical applications in emerging fields such as bioinformatics and autonomous systems. Quradaa et al. [26] presented a start-of-the-art review of RNNs, covering the core architectures with a focus on applications in code clones.\\n\\nSimilarly, the review by Tarwani et al. [22] provided an in-depth analysis of RNNs in the context of NLP. While this review offered valuable insights into the advancements in NLP, it lacked a broader perspective on other application domains and recent architec- tural innovations. Another significant review by Goodfellow et al. [27] focused on the fundamentals of deep learning, including RNNs, and discussed their applications across various domains. This review provided a solid foundation but did not delve deeply into the specific advancements in RNN architectures and their specialized applications.\\n\\nGreff et al. [28] conducted an extensive study comparing various LSTM variants to determine their effectiveness in different applications. While this review provided a thorough comparison, it primarily focused on LSTM architectures and did not address other RNN variants or the latest hybrid models. In similar research, Al-Selwi et al. [29] reviewed LSTM applications in the literature, covering articles from the 2018–2023 time period. Zaremba et al. [30] reviewed the use of RNNs in language modeling, highlighting\\n\\n2 of 34\\n\\nInformation 2024, 15, 517\\n\\nsignificant achievements and the ongoing challenges in this field. Their work offered valuable insights into the application of RNNs in NLP but was limited to language modeling and did not explore other potential applications. Bai et al. [31] provided a critical review of RNNs and their variants, comparing them with other sequence modeling techniques like CNNs and attention-based models. Che et al. [32] focused on the application of RNNs in healthcare, particularly for electronic health records (EHRs) analysis and disease prediction. This review highlighted the potential of RNNs in medical applications.\\n\\nFurthermore, more recent studies have explored various new aspects and applications of RNNs. For example, Chung et al. [33] explored the advancements in RNN architectures over the past decade, focusing on improvements in training techniques, optimization methods, and new architectural innovations. This review provided an extensive survey of recent developments and their implications for future research. Badawy et al. [34] provided a comprehensive overview of the use of RNNs in healthcare, particularly for predictive analytics and patient monitoring. They discussed the integration of RNNs with other ML techniques and the challenges in deploying these models in clinical settings.\\n\\nIsmaeel et al. [35] examined the application of RNNs in smart city technologies, includ- ing traffic prediction, energy management, and urban planning. Their review discussed the potential and limitations of RNNs in these areas and suggested avenues for future research. Meanwhile, Mers et al. [36] reviewed the applications of RNNs in pavement performance forecasting and conducted a comprehensive performance comparison of the various RNN models, including simple RNNs, LSTM, GRU, and hybrid LSTM–fully connected neural networks (LSTM-FCNNs).\\n\\nChen et al. [37] focused on the use of RNNs in environmental monitoring and climate modeling, discussing their effectiveness in predicting environmental changes and managing natural resources. They also highlighted the challenges in modeling complex environmental systems with RNNs. Linardos et al. [38] investigated the advancements in RNNs for natural disaster prediction and management, highlighting the successes and challenges in using RNNs for early warning systems, disaster response, and recovery planning. Zhang et al. [39] discussed RNN applications in robotics, particularly focusing on path planning, motion control, and human–robot interaction. They discussed the integration of RNNs with other DL techniques in robotics. The different related studies are tabulated in Table 1, including their main contributions.\\n\\nThis research addresses the limitations in the existing literature by providing a more comprehensive review that includes the latest developments in RNN architectures, such as hybrid models and neural architecture search, as well as their applications across a wider range of domains. Additionally, this review contributes to a more holistic understanding of the current state and future directions of RNN research by integrating discussions on scalability, robustness, and interoperability.\\n\\nTable 1. Summary of related reviews on RNNs.\\n\\nReference\\n\\nYear\\n\\nDescription\\n\\nZaremba et al. [30] Chung et al. [33]\\n\\nGoodfellow et al. [27] Greff et al. [28] Tarwani et al. [22] Chen et al. [37]\\n\\nBai et al. [31]\\n\\n2014 2014\\n\\n2016 2016 2017 2018\\n\\n2018\\n\\nInsights into RNNs in language modeling Survey of advancements in RNN training, optimiza- tion, and architectures Review on deep learning, including RNNs Extensive comparison of LSTM variants In-depth analysis of RNNs in NLP Effectiveness of RNNs in environmental monitoring and climate modeling Comparison of RNNs with other sequence modeling techniques like CNNs and attention mechanisms Potential of RNNs in medical applications\\n\\nChe et al. [32]\\n\\n2018\\n\\n3 of 34\\n\\nInformation 2024, 15, 517\\n\\nTable 1. Cont.\\n\\nReference\\n\\nYear\\n\\nDescription\\n\\nZhang et al. [39]\\n\\nDutta et al. [25]\\n\\nLinardos et al. [38]\\n\\nBadawy et al. [34]\\n\\nIsmaeel et al. [35]\\n\\nMers et al. [36]\\n\\nQuradaa et al. [26]\\n\\n2020\\n\\n2022\\n\\n2022\\n\\n2023\\n\\n2023\\n\\n2023\\n\\n2024\\n\\nRNN applications in robotics, including path plan- ning, motion control, and human–robot interaction Overview of RNNs, challenges in training, and ad- vancements in LSTM and GRU for sequence learning RNNs for early warning systems, disaster response, and recovery planning in natural disaster prediction Integration of RNNs with other ML techniques for pre- dictive analytics and patient monitoring in healthcare Application of RNNs in smart city technologies, in- cluding traffic prediction, energy management, and urban planning Performance comparison of various RNN models in pavement performance forecasting Start-of-the-art review of RNNs, covering core archi- tectures with a focus on applications in code clones Review of LSTM applications from 2018 to 2023\\n\\nAl-Selwi et al. [29]\\n\\n2024\\n\\n3. Fundamentals of RNNs 3.1. Basic Architecture and Working Principle of Standard RNNs\\n\\nRNNs are designed to process sequential data by maintaining a hidden state that captures information about previous inputs [40]. The basic architecture consists of an input layer, a hidden layer, and an output layer. Unlike feedforward neural networks, RNNs have recurrent connections, as shown in Figure 1, allowing information to cycle within the networks. At each time step, t, the RNN takes an input vector, xt, and updates its hidden state, ht, using the following equation:\\n\\nht = σh(Wxhxt + Whhht−1 + bh),\\n\\nwhere Wxh is the weight matrix between the input and hidden layer, Whh is the weight matrix for the recurrent connection, bh is the bias vector, and σh is the activation function, typically the hyperbolic tangent function (tanh) or the rectified linear unit [41,42]. The output at each time step, t, is given by the following:\\n\\nyt = σy(Whyht + by),\\n\\nwhere Why is the weight matrix between the hidden and output layers, by is the bias vector, and σy is the activation function for the output layer.\\n\\nFigure 1. Basic RNN architecture.\\n\\n4 of 34\\n\\n(1)\\n\\n(2)\\n\\nInformation 2024, 15, 517\\n\\n3.2. Activation Functions\\n\\nThe core of RNN operations involves the recurrent computation of the hidden state, which integrates the current input with the previous hidden state [43]. This recurrent computation allows RNNs to exhibit dynamic temporal behavior. The choice of activation function σh plays a crucial role in the behavior of the network, introducing non-linearity that enables the network to learn and represent complex patterns in the data [44,45]. One commonly used activation function in RNNs is the hyperbolic tangent (tanh). The tanh function squashes the input values to the range of [−1, 1], making it zero-centered and suitable for modeling sequences with both positive and negative values [46]. The tanh is represented mathematically as follows:\\n\\nσh(z) = tanh(z) =\\n\\nez − e−z ez + e−z\\n\\nAnother widely used activation function is the rectified linear unit (ReLU). The ReLU function outputs the input directly if it is positive; otherwise, it outputs zero [47]. This simplicity helps mitigate the vanishing gradient problem to some extent by allowing gradients to flow through the network more effectively. Furthermore, the Leaky ReLU is a variant of the ReLU designed to address the “dying ReLU” problem, where neurons can become inactive and stop learning [48]. The leaky ReLU allows a small, non-zero gradient when the input is negative, thus keeping the neurons active during the training process. Additionally, the exponential linear unit (ELU) is another variant designed to bring the mean activation closer to zero, which speeds up learning by reducing bias shifts [49]. The ELU tends to improve learning characteristics over the ReLU by allowing the activations to take on negative values when the input is negative. These activation functions are represented as follows:\\n\\nσh(z) = max(0, z)\\n\\nσh(z) =\\n\\n(cid:40)\\n\\nif z > 0 αz otherwise\\n\\nz\\n\\nσh(z) =\\n\\n(cid:40)\\n\\nif z > 0\\n\\nz α(ez − 1) otherwise\\n\\nwhere α is a small constant, typically 0.01. Meanwhile, the sigmoid function squashes the input values to the range [0, 1]. It is similar to tanh but outputs values in a differ- ent range, making it useful for problems where the output needs to be interpreted as probabilities [50–52]. Similarly, the softmax function is commonly used in the output layer of classification networks to convert raw scores into probabilities [53]. It is particularly useful in multi-class classification problems. The sigmoid and softmax functions are repre- sented mathematically as follows:\\n\\nσh(z) =\\n\\nσh(zi) =\\n\\n1 1 + e−z ezi ∑j ezj\\n\\nwhere zi is the i-th element of the input vector z. Each of these activation functions has its advantages and is chosen based on the specific requirements of the task at hand. Meanwhile, the hidden state update in RNNs can be seen as a function, ht = f (xt, ht−1), which captures the dependencies between the input sequence and the recurrent connections. The choice of σh significantly affects how well the network learns these dependencies and generalizes to new data.\\n\\n5 of 34\\n\\n(3)\\n\\n(4)\\n\\n(5)\\n\\n(6)\\n\\n(7)\\n\\n(8)\\n\\nInformation 2024, 15, 517\\n\\n3.3. The Vanishing and Exploding Gradient Problems\\n\\nTraining RNNs presents significant challenges due to the vanishing and exploding gradient problems. During the training process, the BPTT algorithm is used to compute the gradients of the loss function with respect to the weights [54]. As the gradients are propa- gated backwards in time, they can either diminish (vanish) or grow exponentially (explode), making it difficult for the network to learn long-term dependencies or causing instability during training. Mathematically, the hidden state at time step t can be expanded as follows:\\n\\nht = σh(Wxhxt + Whhσh(Wxhxt−1 + Whhht−2 + bh) + bh).\\n\\nWhen calculating the gradients, we encounter terms involving the product of many\\n\\nJacobian matrices:\\n\\n∂ht ∂ht−n\\n\\n=\\n\\nt−1 ∏ k=t−n\\n\\nJk,\\n\\nwhere Jk is the Jacobian matrix of the hidden state at time step k. If the eigenvalues of Jk are less than 1, the product of these matrices will tend to zero as n increases, leading to vanishing gradients [55,56]. Conversely, if the eigenvalues of Jk are greater than 1, the gradients can grow exponentially, leading to exploding gradients, which can cause the model parameters to become unstable and result in numerical overflow during training. The vanishing gradient problem prevents the network from effectively learning long-term dependencies, as the gradient signal becomes too weak to update the weights meaningfully for earlier layers. On the other hand, the exploding gradient problem can cause the model to converge too quickly to a poor local minimum or make the training process fail entirely due to excessively large updates.\\n\\nTo mitigate these problems, various RNN variants have been developed, such as LSTM and GRUs. These architectures introduce gating mechanisms that regulate the flow of information and gradients through the network, allowing for the better handling of long-term dependencies. Additionally, gradient clipping is a common technique used to prevent exploding gradients by capping the gradients at a maximum threshold during backpropagation, ensuring that they do not grow uncontrollably [57,58].\\n\\n3.4. Bidirectional RNNs\\n\\nBidirectional RNNs (BiRNNs) enhance the architecture by processing the sequence in both forward and backward directions. This allows the network to have access to future context, as well as past context, improving its performance in tasks for which understanding both the preceding and succeeding elements is crucial [43,59]. In BiRNNs, two hidden ←− h t): states are maintained: one for the forward pass (\\n\\n−→ h t) and one for the backward pass (\\n\\n−→ h t = σh(Wxhxt + Whh ←− h t = σh(Wxhxt + Whh\\n\\n−→ h t−1 + bh), ←− h t+1 + bh).\\n\\nThe output yt is then computed by concatenating the forward and backward hidden states:\\n\\nyt = σy(Why[\\n\\n−→ h t;\\n\\n←− h t] + by),\\n\\nwhere [; ] denotes concatenation. Furthermore, BiRNNs are effective for tasks such as named entity recognition, machine translation, and speech recognition, where context from both directions improves the model’s performance [60,61]. Through accessing information from both the past and future, BiRNNs can provide a more comprehensive understanding of the input sequence. For instance, in language modeling, understanding the surrounding words can significantly enhance the accuracy of predicting the next word [62,63]. In machine translation, knowing the entire sentence allows the network to translate words more accurately, considering the entire context. Additionally, BiRNNs are also used in\\n\\n6 of 34\\n\\n(9)\\n\\n(10)\\n\\n(11)\\n\\n(12)\\n\\n(13)\\n\\nInformation 2024, 15, 517\\n\\nvarious time series applications, such as stock price prediction and medical diagnosis, where understanding the temporal dependencies in both directions is beneficial [64]. However, BiRNNs require more computational resources than unidirectional RNNs due to the need to process the sequence twice (forward and backwards) [59,65].\\n\\n3.5. Deep RNNs\\n\\nDeep RNNs extend the basic architecture by stacking multiple RNN layers on top of each other, which allows the network to learn more complex representations [66]. Each layer’s hidden state serves as the input to the subsequent layer, enhancing the model’s capacity to capture hierarchical features. For a deep RNN with L layers, the hidden states at layer l and time step t are updated as follows:\\n\\nh\\n\\n(l) t = σh(W\\n\\n(l) xh h\\n\\n(l−1) t\\n\\n+ W\\n\\n(l) hh h\\n\\n(l) t−1 + b\\n\\n(l) h ),\\n\\n(0) t = xt represents the input at the first layer. The output at the topmost layer is\\n\\nwhere h then computed using the same procedure as in basic RNNs:\\n\\nyt = σy(Whyh\\n\\n(L) t + by).\\n\\nDeep RNNs can model more complex sequences and capture longer dependencies than shallow RNNs [67]. However, they are also more prone to the vanishing gradient problem, which can be mitigated by using advanced variants like LSTM or GRUs. Deep RNNs have been successfully applied in various domains, including natural language processing, speech recognition, and video analysis. In NLP, deep RNNs can model com- plex linguistic structures and capture long-range dependencies, improving tasks such as machine translation and text generation. However, training deep RNNs can be challenging due to the increased complexity and the risk of overfitting [68,69]. Techniques such as dropout, layer normalization, and residual connections are often employed to improve the training process and generalization of deep RNNs [70–72]. Dropout helps prevent overfit- ting by randomly setting a fraction of the units to zero during training, which encourages the network to learn more robust features, while batch normalization helps stabilize and accelerate training by normalizing the inputs to each layer [73]. Residual connections, which add shortcut connections that bypass one or more layers, help mitigate the vanishing gradient problem in very deep networks [74].\\n\\n4. Advanced Variants of RNNs\\n\\nRNN architectures can vary significantly, with some featuring internal recurrence within neurons and others having external recurrence between layers. These variations impact the network’s ability to learn and process sequences, influencing their application to specific tasks.\\n\\n4.1. Long Short-Term Memory Networks\\n\\nLSTM networks were introduced by Hochreiter and Schmidhuber [15] to address the vanishing gradient problem inherent to basic RNNs. The key innovation in LSTM is the use of gating mechanisms to control the flow of information through the network. This allows LSTM networks to maintain and update their internal state over long periods, making them effective for tasks requiring the modeling of long-term dependencies. Each LSTM cell contains three gates: the input gate, forget gate, and output gate, which regulate the cell state ct and hidden state ht [75]. These gates determine how much of the input to consider, how much of the previous state to forget, and how much of the cell state to output. The LSTM update equations are as follows:\\n\\nit = σ(Wxixt + Whiht−1 + bi),\\n\\nft = σ(Wx f xt + Wh f ht−1 + b f ),\\n\\n7 of 34\\n\\n(14)\\n\\n(15)\\n\\n(16)\\n\\n(17)\\n\\nInformation 2024, 15, 517\\n\\not = σ(Wxoxt + Whoht−1 + bo),\\n\\ngt = tanh(Wxgxt + Whght−1 + bg),\\n\\nct = ft ⊙ ct−1 + it ⊙ gt,\\n\\nht = ot ⊙ tanh(ct),\\n\\nwhere it is the input gate, ft is the forget gate, ot is the output gate, gt is the cell input, ct is the cell state, ht is the hidden state, σ represents the sigmoid function, tanh is the hyperbolic tangent function, and ⊙ denotes element-wise multiplication [75].\\n\\nFigure 2 illustrates the internal architecture of an LSTM cell, which effectively manages long-term dependencies in sequence data by employing three crucial gating mechanisms: the input gate (it), forget gate (ft), and output gate (ot). Each of these gates plays a distinct role in controlling the flow of information through the cell. The input gate controls how much of the new input xt is written to the cell state ct. The forget gate decides how much of the previous cell state ct−1 should be retained. The output gate determines how much of the cell state ct is used to compute the hidden state ht. The cell input gt is a candidate value that is added to the cell state after being modulated via the input gate. The use of these gating mechanisms allows LSTM networks to selectively remember or forget information, enabling them to handle long-term dependencies more effectively than traditional RNNs. The internal recurrence within the LSTM cell is managed through the cell state ct, which acts as a conveyor belt, transferring relevant information across different time steps. This recurrence mechanism allows the LSTM to maintain and update its memory over long sequences, effectively capturing long-term dependencies. Additionally, the element-wise multiplication operations between the gates and their respective inputs ensure that the interactions between different components of the LSTM are smooth and efficient. This enables the LSTM to perform complex transformations on the input data while maintaining the stability of the learning process. Meanwhile, LSTM networks utilize internal recurrence within each cell to manage long-term dependencies, with the recurrence happening through the cell state as information is passed from one time step to the next [21]. Other LSTM variants include bidirectional LSTM (BiLSTM) and stacked LSTM.\\n\\nFigure 2. Architecture of the LSTM network [41].\\n\\n4.2. Bidirectional LSTM\\n\\nBidirectional LSTM, shown in Figure 3, extends the standard LSTM architecture by processing the sequence in both forward and backward directions, similar to BiRNNs [76]. This approach allows the network to capture context from both the past and the future, enhancing its ability to understand dependencies in the sequence more comprehensively. In BiLSTM, two separate hidden states are maintained for each time step: one for the ←− h t). These hidden states are computed forward pass ( as described in Equations (11) and (12). BiLSTM features external recurrence between layers as they process the input sequence in both forward and backward directions, maintaining separate hidden states for each direction.\\n\\n−→ h t) and one for the backward pass (\\n\\n8 of 34\\n\\n(18)\\n\\n(19)\\n\\n(20)\\n\\n(21)\\n\\nInformation 2024, 15, 517\\n\\nFigure 3. Architecture of BiLSTM network [41].\\n\\nStacked LSTM\\n\\nStacked LSTM involves stacking multiple LSTM layers, in which the output of one LSTM layer serves as the input to the next, as shown in Figure 4. This deep architecture allows the network to capture more complex patterns and dependencies in the data by learning hierarchical representations at different levels of abstraction. For a stacked LSTM with L layers, the hidden states at layer l and time step t are updated as described in Equations (14) and (15). Stacked LSTM incorporates external recurrence by connecting multiple LSTM layers, in which each layer passes its output as input to the next, allowing the network to capture more complex temporal patterns.\\n\\nFigure 4. A stacked LSTM [41].\\n\\nStacking LSTM layers allows the network to learn increasingly complex features and representations. The lower layers can capture local patterns and short-term dependencies, while the higher layers can capture more abstract features and long-term dependencies [21]. This hierarchical learning is advantageous for tasks such as language modeling, where different levels of syntactic and semantic information need to be captured, or for video analysis, where temporal dependencies at different time scales must be understood. While stacked LSTM offers improved modeling capabilities, they also come with increased computational complexity and a higher risk of overfitting.\\n\\n4.3. Gated Recurrent Units\\n\\nGated recurrent units are another variant designed to address the vanishing gradient problem while simplifying the LSTM architecture. Introduced by Cho et al. [16], GRUs combine the forget and input gates into a single update gate and merge the cell state and hidden state, reducing the number of gates and parameters and thus simplifying the model and making it computationally more efficient. The GRU architecture consists of two gates: the update gate, zt, and the reset gate, rt [77]. Figure 5 shows the GRU architecture. The gates control the flow of information to ensure that relevant information is retained and irrelevant information is discarded. Similar to LSTM, GRUs rely on internal recurrence within each unit as they maintain and update the hidden state across time steps to capture temporal dependencies. The updated equations for GRUs are as follows:\\n\\nzt = σ(Wxzxt + Whzht−1 + bz),\\n\\nrt = σ(Wxrxt + Whrht−1 + br), h′ t = tanh(Wxhxt + rt ⊙ (Whhht−1) + bh), ht = (1 − zt) ⊙ ht−1 + zt ⊙ h′ t,\\n\\n9 of 34\\n\\n(22)\\n\\n(23)\\n\\n(24)\\n\\n(25)\\n\\nInformation 2024, 15, 517\\n\\nwhere zt is the update gate, rt is the reset gate, and h′ t is the candidate hidden state. The update gate zt determines how much of the previous hidden state ht−1 should be carried forward to the current hidden state ht, while the reset gate, rt, controls how much of the previous hidden state to forget [75]. The candidate hidden state h′ t represents the new content to be added to the current hidden state, modulated via the reset gate. Furthermore, the simplified architecture of GRUs allows them to be computationally more efficient than LSTM while still addressing the vanishing gradient problem. This efficiency makes GRUs well-suited for tasks where computational resources are limited or when training needs to be faster. GRUs have been successfully applied in various sequence modeling tasks. Their ability to capture long-term dependencies with fewer parameters makes them a popular choice in many applications. Additionally, studies have shown that GRUs can achieve performance comparable to LSTM [78–80], making them an attractive alternative for many use cases.\\n\\nFigure 5. Architecture of the GRU network.\\n\\nComparison with LSTM\\n\\nGRUs have fewer parameters compared to LSTM due to the absence of a separate cell state and combined gating mechanisms [81]. This often leads to faster training times and comparable performance to LSTM in many tasks. However, despite their advantages, the choice between GRUs and LSTM often depends on the specific task and dataset. Some tasks may benefit more from the additional complexity and gating mechanisms of LSTM, while others may perform equally well with the simpler GRU architecture.\\n\\n4.4. Other Notable Variants\\n\\nWhile LSTM and GRUs are the most widely used RNN variants, other architectures like peephole LSTM, echo state networks, and independently recurrent neural networks offer unique advantages for specific applications.\\n\\n4.4.1. Peephole LSTM\\n\\nPeephole LSTM, introduced by Gers and Schmidhuber [82], enhances standard LSTM by allowing the gates to have access to the cell state through peephole connections. This additional connection enables the LSTM to better regulate the gating mechanisms based on the current cell state, improving timing decisions in applications such as speech recognition and financial time series prediction [83]. In the following equations, the input gate (it), forget gate (ft), and output gate (ot) are enhanced with peephole connections:\\n\\nit = σ(Wxixt + Whiht−1 + Wcict−1 + bi),\\n\\nwhere it is the input gate, and Wci is the peephole weight connecting the cell state ct−1 to the input gate.\\n\\nft = σ(Wx f xt + Wh f ht−1 + Wc f ct−1 + b f ),\\n\\n10 of 34\\n\\n(26)\\n\\n(27)\\n\\nInformation 2024, 15, 517\\n\\nwhere ft is the forget gate, and Wc f is the peephole weight connecting the cell state ct−1 to the forget gate.\\n\\not = σ(Wxoxt + Whoht−1 + Wcoct + bo),\\n\\nwhere ot is the output gate, and Wco is the peephole weight connecting the cell state ct to the output gate.\\n\\n4.4.2. Echo State Networks\\n\\nEcho state networks (ESNs), proposed by Jaeger [84], represent a class of RNNs in which the hidden layer, also known as the reservoir, is fixed and randomly connected, while only the output layer is trained. This architecture significantly simplifies the training process, making ESNs particularly suitable for real-time signal processing, time series prediction, and adaptive control systems. The state update and output computation in ESNs are achieved through the following equations:\\n\\nht = tanh(Winxt + Wresht−1),\\n\\nwhere ht is the hidden state, Win is the input weight matrix, and Wres is the fixed, randomly initialized reservoir weight matrix. When it is assumed that Wout is the trained output weight matrix, the output of the network can be represented as follows:\\n\\nyt = Woutht,\\n\\nESNs have gained significant attention due to their ability to handle complex tem- poral dynamics with a relatively simple and efficient training process [85,86]. However, traditional ESNs are limited due to the fixed nature of the reservoir, which can restrict their adaptability and performance in more complex tasks. To address these limitations, several advancements have been proposed:\\n\\nDeep Echo-State Networks: Recent research has extended the ESN architecture to deeper variants, known as deep echo-state networks (DeepESNs). In DeepESNs, multi- ple reservoir layers are stacked, allowing the network to capture hierarchical temporal features across different timescales [87]. Each layer in a DeepESN processes the output from the previous layer’s reservoir, enabling the model to learn more abstract and complex representations of the input data. The state update for a DeepESN can be generalized as follows: t = tanh(Wl hl where l denotes the layer number, hl in is the in- put weight matrix for layer l, and hl−1 is the hidden state from the previous layer. DeepESNs have demonstrated improved performance in tasks requiring the mod- eling of complex temporal patterns, such as speech recognition and financial time series forecasting [88]. Ensemble Deep ESNs: In ensemble deep ESNs, multiple DeepESNs are trained in- dependently, and their outputs are combined to form the final prediction [89]. This ensemble approach leverages the diversity of the reservoirs and the deep architecture to improve robustness and accuracy, particularly in time series forecasting applica- tions. For instance, Gao et al. [90] demonstrated the effectiveness of Deep ESN en- sembles in predicting significant wave heights, where the ensemble approach helped mitigate the impact of reservoir initialization variability and improved the model’s generalization ability. Input Processing with Signal Decomposition: Another critical aspect of effectively utilizing RNNs and ESNs is the preprocessing of input signals. Given the complex and often noisy nature of real-world time series data, signal decomposition techniques such as the empirical wavelet transform (EWT) have been employed to enhance the input to ESNs [91]. The EWT decomposes the input signal into different frequency\\n\\nDeep Echo-State Networks: Recent research has extended the ESN architecture to deeper variants, known as deep echo-state networks (DeepESNs). In DeepESNs, multi- ple reservoir layers are stacked, allowing the network to capture hierarchical temporal features across different timescales [87]. Each layer in a DeepESN processes the output from the previous layer’s reservoir, enabling the model to learn more abstract and complex representations of the input data. The state update for a DeepESN can be generalized as follows: t = tanh(Wl hl where l denotes the layer number, hl in is the in- put weight matrix for layer l, and hl−1 is the hidden state from the previous layer. DeepESNs have demonstrated improved performance in tasks requiring the mod- eling of complex temporal patterns, such as speech recognition and financial time series forecasting [88]. Ensemble Deep ESNs: In ensemble deep ESNs, multiple DeepESNs are trained in- dependently, and their outputs are combined to form the final prediction [89]. This ensemble approach leverages the diversity of the reservoirs and the deep architecture to improve robustness and accuracy, particularly in time series forecasting applica- tions. For instance, Gao et al. [90] demonstrated the effectiveness of Deep ESN en- sembles in predicting significant wave heights, where the ensemble approach helped mitigate the impact of reservoir initialization variability and improved the model’s generalization ability. Input Processing with Signal Decomposition: Another critical aspect of effectively utilizing RNNs and ESNs is the preprocessing of input signals. Given the complex and often noisy nature of real-world time series data, signal decomposition techniques such as the empirical wavelet transform (EWT) have been employed to enhance the input to ESNs [91]. The EWT decomposes the input signal into different frequency\\n\\n\\n\\n\\n\\n11 of 34\\n\\n(28)\\n\\n(29)\\n\\n(30)\\n\\nInformation 2024, 15, 517\\n\\n12 of 34\\n\\ncomponents, allowing the ESN to process each component separately and improve the model’s ability to capture underlying patterns. The combination of the EWT with ESNs has shown promising results in various applications, including time series forecasting, where it helps reduce noise and enhance the predictive performance of the model.\\n\\n4.4.3. Independently Recurrent Neural Network\\n\\nindependently recurrent neural networks (IndRNNs), proposed by Li et al. [92], have used independent recurrent units to address the gradient vanish- ing and exploding problems, making it easier to train very deep RNNs. This architecture is useful for long sequence tasks such as video sequence analysis and long text generation. When it is assumed that ht is the hidden state, Wxh is the input weight matrix, and u is a vector of recurrent weights. The state update equation for IndRNN is as follows:\\n\\nMost recently,\\n\\nht = σ(Wxhxt + u ⊙ ht−1),\\n\\n(32)\\n\\nThe various RNN architectures are summarized in Table 2.\\n\\nTable 2. Comparative overview of RNN architectures.\\n\\nRNN Type Key Features\\n\\nGradient Stability\\n\\nTypical Applications\\n\\nBasic RNN\\n\\nSimple structure with short-term memory\\n\\nHigh risk of vanishing gradients\\n\\nSimple sequence tasks like text generation\\n\\nLSTM\\n\\nLong-term memory with input, forget, and output gates\\n\\nStable, handles vanish- ing gradients well\\n\\nLanguage speech recognition\\n\\ntranslation,\\n\\nGRU\\n\\nSimplified LSTM with fewer gates\\n\\nStable, handles van- ishing gradients effec- tively\\n\\nTasks training than LSTM\\n\\nrequiring faster\\n\\nBidirectional RNN\\n\\nProcesses data in both forward and backward directions for better context understanding\\n\\nMedium stability, de- pends on depth\\n\\nSpeech recognition and sentiment analysis\\n\\nDeep RNN\\n\\nMultiple RNN layers are stacked to learn hi- erarchical features\\n\\nVariable, and the risk of vanishing gradients increases with depth\\n\\nComplex sequence model- ing like video processing\\n\\nESN\\n\\nFixed hidden layer weights, trained only at the output\\n\\nNot applicable as train- ing bypasses typical gradient issues\\n\\nTime series prediction and system control\\n\\nPeephole LSTM\\n\\nAdds peephole connec- tions to LSTM gates\\n\\nStable and similar to LSTM\\n\\nRecognition of complex temporal patterns like mu- sical notation\\n\\nIndRNN\\n\\nAllows training of deeper networks by maintaining indepen- dence between time steps\\n\\nReduces risk of vanish- ing and exploding gra- dients\\n\\nVery long sequences, such as in video processing or long text generation\\n\\n5. Innovations in RNN Architectures and Training Methodologies\\n\\nIn recent years, there have been significant innovations in RNN architectures and train-\\n\\ning methodologies aimed at enhancing performance and addressing existing limitations.\\n\\nInformation 2024, 15, 517\\n\\n5.1. Hybrid Architectures\\n\\nCombining RNNs with other neural network architectures has led to hybrid models that leverage the strengths of each component. For example, integrating CNNs with RNNs has proven effective in video analysis, where CNNs handle spatial features while RNNs capture temporal dynamics [93,93]. This approach allows the model to process both spatial and temporal information, enhancing its ability to recognize patterns and make predictions. Furthermore, incorporating attention mechanisms into RNNs has also improved their ability to model long-range dependencies. Attention mechanisms enable the network to focus on relevant parts of the input sequence, which is useful in tasks such as machine translation and text summarization. The attention mechanism can be described as follows:\\n\\nat = softmax(ut),\\n\\nct =\\n\\nT ∑ i=1\\n\\nat,ihi,\\n\\nwhere at is the attention weight, ut is the score function, and ct is the context vector.\\n\\n5.2. Neural Architecture Search\\n\\nNeural architecture search (NAS) has automated the design of RNN architectures, enabling the discovery of more efficient and powerful models [94,95]. NAS techniques, such as those pioneered by Zoph and Le [96], explore various combinations of layers, activation functions, and hyperparameters to find optimal configurations that outperform manually designed architectures. The NAS process can be formulated as an optimization problem:\\n\\nA∗ = arg max A∈S\\n\\nAccuracy(A),\\n\\nwhere A represents an architecture, S is the search space, and A∗ is the optimal architecture.\\n\\n5.3. Advanced Optimization Techniques\\n\\nAdvanced optimization techniques have been developed to improve the training efficiency and stability of RNNs. Gradient clipping is a technique used to prevent the gradients from becoming too large, which can destabilize training [58,97].\\n\\ng ←\\n\\ng\\n\\nmax(1,\\n\\n∥g∥ τ )\\n\\n,\\n\\nwhere g is the gradient, and τ is the threshold value. Furthermore, adaptive learning rates, such as those used in the Adam optimizer, adjust the learning rate during training to accelerate convergence and improve performance [98]. The Adam optimizer updates the parameters using the following:\\n\\nmt = β1mt−1 + (1 − β1)gt,\\n\\nvt = β2vt−1 + (1 − β2)g2 t , vt 1 − βt 2\\n\\nmt 1 − βt 1\\n\\nˆmt =\\n\\nˆvt =\\n\\n,\\n\\n,\\n\\nθt = θt−1 − α\\n\\nˆmt√\\n\\nˆvt + ϵ\\n\\n,\\n\\nwhere mt and vt are the first- and second-moment estimates, β1 and β2 are the decay rates, α is the learning rate, and ϵ is a small constant [98]. Also, second-order optimization methods, such as the Hessian-free optimizer, have also been explored to improve the convergence speed and stability of training deep networks.\\n\\n13 of 34\\n\\n(33)\\n\\n(34)\\n\\n(35)\\n\\n(36)\\n\\n(37)\\n\\n(38)\\n\\n(39)\\n\\n(40)\\n\\nInformation 2024, 15, 517\\n\\n14 of 34\\n\\n5.4. RNNs with Attention Mechanisms\\n\\nIntegrating attention mechanisms into RNNs allows these networks to selectively focus on important parts of the input sequence, addressing the limitations of traditional RNNs in modeling long-term dependencies [99–101]. This hybrid approach combines the strengths of RNNs and attention mechanisms, enhancing their capability to handle complex sequence tasks. Attention-enhanced RNNs have shown significant improvements in tasks such as speech recognition and text summarization. For example, Bahdanau et al. [102] demonstrated the use of attention mechanisms in neural machine translation, which al- lowed RNNs to focus on relevant words in the source sentence, improving translation accuracy. Similarly, Luong et al. [103] proposed global and local attention mechanisms, further enhancing the performance of RNNs in various sequence-to-sequence tasks.\\n\\n5.5. RNNs Integrated with Transformer Models\\n\\nTransformers, introduced by Vaswani et al. [104] in 2017, employ self-attention mecha- nisms and have proven to be highly effective in capturing long-range dependencies. Unlike RNNs, transformers process sequences in parallel, which can lead to better performance on long sequences. The self-attention mechanism is defined as follows:\\n\\nAttention(Q, K, V) = softmax\\n\\n(cid:32)\\n\\nQKT √ dk\\n\\n(cid:33)\\n\\nV,\\n\\n(41)\\n\\nwhere Q, K, and V are the query, key, and value matrices, respectively, and dk is the dimension of the keys. Considering that both transformer and RNN architecture have limitations, studies have integrated both methods to obtain robust models, as shown in the recent literature [104]. Therefore, researchers can develop more powerful and efficient models for a wide range of applications by leveraging the sequential processing capabilities of RNNs and the parallel, attention-based mechanisms of transformers. This integrated approach addresses the limitations of each architecture and enhances the overall performance in sequence modeling tasks.\\n\\n6. Public Datasets for RNN Research\\n\\nThis section provides an overview of publicly available datasets that are commonly used in the study and evaluation of RNNs. These datasets cover a variety of applications, ranging from natural language processing to time series forecasting, reflecting the diverse capabilities of RNNs. Each of these datasets provides a unique challenge for RNNs, allowing researchers to explore the strengths and limitations of different RNN architectures across various real-world tasks. Table 3 summarizes the publicly available datasets for RNN research.\\n\\nTable 3. Public datasets for studying RNNs.\\n\\nDataset Name\\n\\nApplication\\n\\nDescription\\n\\nPenn Treebank [105]\\n\\nNatural language processing\\n\\nA corpus of English sentences annotated for part-of-speech tagging, parsing, and named en- tity recognition; widely used for language mod- eling with RNNs\\n\\nIMDB Reviews [106]\\n\\nSentiment analysis\\n\\nA dataset of movie reviews used for binary sen- timent classification; suitable for studying the effectiveness of RNNs in text sentiment classifi- cation tasks\\n\\nMNIST Sequential [107]\\n\\nImage recognition\\n\\nA version of the MNIST dataset formatted as sequences for studying sequence-to-sequence learning with RNNs\\n\\nInformation 2024, 15, 517\\n\\n15 of 34\\n\\nTable 3. Cont.\\n\\nDataset Name\\n\\nApplication\\n\\nDescription\\n\\nTIMIT Speech Corpus [108]\\n\\nSpeech recognition\\n\\nAn annotated speech database used for auto- matic speech recognition systems\\n\\nReuters-21578 Text Categorization Collection [109]\\n\\nText categorization\\n\\nA collection of newswire articles that is a com- mon benchmark for text categorization and NLP tasks with RNNs\\n\\nUCI ML Reposi- tory: Time Series Data [110]\\n\\nTime series analysis\\n\\nContains various time series datasets, including stock prices and weather data, ideal for forecast- ing with RNNs.\\n\\nCORe50 Dataset [111]\\n\\nObject Recognition\\n\\nUsed for continuous object recognition, ideal for RNN models dealing with video input se- quences where object persistence and temporal context are important\\n\\n7. Applications of RNNs in Peer-Reviewed Literature\\n\\nRNNs and their variants have been extensively studied and applied across various domains in the peer-reviewed literature. This section provides a comprehensive review of these applications.\\n\\n7.1. Natural Language Processing\\n\\nRNNs have transformed the field of NLP by enabling more sophisticated and context- aware models. Several studies have demonstrated the effectiveness of RNNs in various NLP tasks.\\n\\n7.1.1. Text Generation\\n\\nRNNs have been used extensively for text-generation tasks. Souri et al. [112] demon- strated the use of RNNs to generate coherent and contextually relevant Arabic text. Their model was trained on a large corpus of text data, allowing it to learn the probability distribu- tion of word sequences, which proved effective in generating human-like text. Meanwhile, several researchers have proposed novel approaches to enhancing the performance of RNNs in text generation. For instance, Islam [113] introduced a sequence-to-sequence framework that improved the generation quality using LSTM. This method allowed the network to handle longer sequences and maintain coherence over extended text.\\n\\nGajendran et al. [114] demonstrated the effectiveness of RNNs in generating character- level text. Their work showed that BiLSTM could capture a wide range of patterns, from character-level dependencies to higher-level syntactic structures, making them versatile for different text generation tasks, including the generation of code, literature, and poetry. More recently, advancements in RNN-based text generation have focused on improving the diversity and coherence of generated text. Hu et al. [115] proposed the use of variational autoencoders (VAEs) combined with RNNs to enhance the creativity of text generation. Their approach enabled the generation of diverse and contextually rich sentences by learning a latent space representation of the text.\\n\\nMeanwhile, Holtzman et al. [116] introduced the concept of “controlled text generation” using RNNs, which allowed users to influence the style and content of the generated text. This method provided more flexibility and control over the text generation process, making it useful for applications such as creative writing and personalized content generation. Additionally, with the advent of more sophisticated models like transformers, RNN-based text generation has evolved to incorporate attention mechanisms.\\n\\nYin et al. [117] proposed an approach combining RNN with an attention mechanism, which allows the model to focus on relevant parts of the input sequence during the genera-\\n\\nInformation 2024, 15, 517\\n\\ntion process. This significantly improved the quality and coherence of the generated text by dynamically adjusting the focus of the model based on the context. Hussein and Savas [118] employed LSTM for text generation. Similarly, Baskaran et al. [119] employed LSTM for text generation, achieving excellent performance. These studies showed that LSTM networks are capable of generating texts that are contextually relevant and linguistically accurate.\\n\\nFurthermore, studies have continued to explore and enhance the capabilities of RNNs in text generation. Keskar et al. [120] introduced a large-scale language model known as Conditional Transformer Language (CTRL), which can be conditioned on specific control codes to generate text in various styles and domains. This work highlights the growing trend of combining RNNs with transformer architectures to leverage their strengths in sequence modeling and text generation. Additionally, Guo [121] explored the integra- tion of reinforcement learning with RNNs for text generation. The approach aimed to optimize the generation process by rewarding the model for producing high-quality, con- textually appropriate text, thereby improving both the coherence and relevance of the generated content.\\n\\nIn text generation tasks, the LSTM networks have proven to be the most effective among RNN architectures. The LSTM’s ability to manage long-term dependencies through its gating mechanisms makes it well-suited for generating coherent and contextually rel- evant text over extended sequences. Studies such as those by Souri et al. [112] and Gajendran et al. [114] highlight the versatility of LSTM in handling both word-level and character-level text generation tasks, respectively. While more recent models, such as those incorporating transformers, have gained popularity, LSTM-based models continue to be preferred for scenarios requiring robust sequence modeling with fewer computa- tional resources, especially when dealing with smaller datasets where the complexity of transformers might not be necessary.\\n\\n7.1.2. Sentiment Analysis\\n\\nIn sentiment analysis, RNNs have been shown to outperform traditional models by capturing the context and details of sentiment expressed in text. Yadav et al. [122] used LSTM-based models to analyze customer reviews and social media posts, achiev- ing notable improvements in accuracy over conventional methods. Building on this, Abimbola et al. [123] proposed a hybrid LSTM-CNN model for document-level sentiment classification, which first captures the sentiment of individual sentences and then aggre- gates them to determine the overall sentiment of the document. This hierarchical approach allows for a more detailed understanding of sentiment, especially in long and complex texts. Zulqarnain et al. [124] utilized the concept of attention mechanisms and GRU to enhance sentiment analysis. By allowing the model to focus on specific parts of the input text that are most indicative of sentiment, attention mechanisms significantly improved the inter- pretability and performance of sentiment analysis models. This advancement enabled the models to highlight which words or phrases contribute the most to sentiment prediction. Additionally, several studies have explored the integration of RNNs with CNNs to leverage the strengths of both architectures. For instance, Pujari et al. [125] combined CNNs and RNNs to capture both local features and long-range dependencies in text, resulting in a hybrid model that achieved state-of-the-art performance in sentiment classification tasks. Meanwhile, Wankhade et al. [126] employed the fusion of CNN and BiLSTM with an attention mechanism, leading to enhanced sentiment classification. Furthermore, Sangeetha and Kumaran [127] utilized BiLSTM to enhance the sentiment analysis capability by processing text in both forward and backward directions. This approach captures the context from both past and future words, providing a more comprehensive understanding of the sentiment expressed in the text.\\n\\nIn addition to these architectural innovations, there has been a focus on improving the robustness of RNN-based sentiment analysis models. For example, He and McAuley [128] developed an adversarial training framework that enhances the model’s ability to handle noisy and adversarial text inputs, thereby improving its generalization to real-world data.\\n\\n16 of 34\\n\\nInformation 2024, 15, 517\\n\\nAlso, the use of transfer learning and pre-trained language models, such as BERT and GPT, has been increasingly popular in sentiment analysis [129–131]. These models, fine-tuned for sentiment classification tasks, have demonstrated exceptional performance by leveraging large-scale pre-training on diverse text corpora and then adapting to specific sentiment analysis datasets.\\n\\nFurthermore, BiLSTM can be considered the most effective variant of RNNs in senti- ment analysis due to their ability to process text in both forward and backward directions. This bidirectional processing allows the model to capture the full context of a sentence, making it effective in understanding intrinsic sentiment expressed in text. Studies by Sangeetha and Kumaran [127] demonstrate the superiority of BiLSTM in achieving higher accuracy in sentiment classification tasks. The ability of BiLSTM to integrate with attention mechanisms, as shown in work by Wankhade et al. [126], further enhances their perfor- mance by allowing the model to focus on the most relevant parts of the text, thus improving interpretability and classification accuracy.\\n\\n7.1.3. Machine Translation\\n\\nTo address the challenge of translating long sentences, Wu et al. [132] introduced the concept of deep RNNs with multiple layers in both the encoder and decoder. Their model, known as Google Neural Machine Translation (GNMT), improved translation accuracy and fluency by capturing more complex patterns and dependencies within the text. GNMT became a major achievement in neural machine translation, setting a new benchmark for translation systems. Sennrich et al. [133] presented a method for incorporating subword units into RNN-based translation models. This approach, known as Byte-Pair Encoding (BPE), enabled the translation models to handle rare and out-of-vocabulary words more effectively by splitting them into smaller, more frequent subword units. This method improved the robustness and generalization of the translation models.\\n\\nWith the advent of transformer models, Vaswani et al. [104] revolutionized the field of machine translation by introducing a fully attention-based architecture that eliminated the need for recurrence entirely. Transformers demonstrated superior performance in translation tasks by allowing for parallel processing of sequences and capturing long-range dependencies more efficiently. Despite this shift, RNN-based models with attention mecha- nisms continued to be relevant, particularly in scenarios where computational resources were limited or sequential processing was preferred. For example, Kang et al. [134] com- bined RNN with an attention mechanism to obtain a bilingual attention-based machine translation model. While Zulqarnain et al. [124] utilized GRU in a multi-stage feature attention mechanism model.\\n\\nSeveral studies have also combined RNNs with transformer models to utilize the strengths of both architectures. For instance, Yang et al. [135] proposed a hybrid model that integrates RNNs into the transformer architecture to enhance its ability to capture sequential dependencies while maintaining the efficiency of parallel processing. This hybrid approach achieved state-of-the-art performance in several translation benchmarks. Meanwhile, more recent studies have explored the integration of pre-trained language models like BERT and GPT into machine translation systems. Song et al. [136] demonstrated that incorporating BERT into the encoder of a translation model enhanced its understanding of the source language, leading to more accurate and fluent translations. Table 4 summarizes the discussed applications of RNNs in natural language processing.\\n\\nHybrid models that combine the strengths of RNNs, particularly LSTM, with trans- former architectures are considered the best approach to machine translation. While transformers have set new benchmarks in translation accuracy due to their parallel pro- cessing capabilities and efficient handling of long-range dependencies, integrating RNNs with attention mechanisms, as seen in studies by Yang et al. [135] and Song et al. [136], has shown that these hybrid models can outperform purely transformer-based approaches in certain scenarios. This is especially true in resource-constrained environments where\\n\\n17 of 34\\n\\nInformation 2024, 15, 517\\n\\nApplication Domain\\n\\nText generation\\n\\nSentiment analysis\\n\\nMachine Translation\\n\\n18 of 34\\n\\nthe sequential processing of RNNs, enhanced by attention mechanisms, can lead to more accurate and computationally efficient translations.\\n\\nTable 4. Summary of applications of RNNs in natural language processing.\\n\\nReference\\n\\nYear Methods and Application\\n\\nSouri et al. [112]\\n\\nHoltzman et al. [116]\\n\\nHu et al. [115]\\n\\n2018\\n\\n2019\\n\\n2020\\n\\nRNNs for generating coherent and contextually relevant Arabic text Controlled text generation using RNNs content control VAEs text generation\\n\\nfor\\n\\nstyle and\\n\\ncombined with RNNs\\n\\nto enhance creativity in\\n\\nGajendran et al. [114] Hussein and Savas [118] Baskaran et al. [119]\\n\\nIslam [113]\\n\\nYin et al. [117]\\n\\nGuo [121]\\n\\nKeskar et al. [120]\\n\\n2020 Character-level text generation using BiLSTM for various tasks 2024 2024\\n\\nLSTM for text generation LSTM for text generation, achieving excellent performance Sequence-to-sequence framework using LSTM for improved text generation quality Attention mechanisms with RNNs for improved text generation quality Integration of text generation Conditional Transformer Language (CTRL) for generating text in various styles\\n\\n2019\\n\\n2018\\n\\nreinforcement\\n\\nlearning with RNNs\\n\\n2015\\n\\n2019\\n\\nfor\\n\\nHe and McAuley [128]\\n\\n2016\\n\\nAdversarial training framework for robustness in sentiment analysis\\n\\nPujari et al. [125]\\n\\n2024 Hybrid CNN-RNN model for sentiment classification\\n\\nWankhade et al. [126]\\n\\nSangeetha and Kumaran [127]\\n\\nYadav et al. [122]\\n\\n2024\\n\\n2023\\n\\n2023\\n\\nFusion of CNN and BiLSTM with attention mechanism for senti- ment classification BiLSTM for both directions LSTM-based models for sentiment analysis in customer reviews and social media posts\\n\\nsentiment analysis by processing text\\n\\nin\\n\\nZulqarnain et al. [124] Samir et al. [129] Prottasha et al. [130]\\n\\nAbimbola et al. [123]\\n\\nMujahid et al. [131]\\n\\n2024 Attention mechanisms and GRU for enhanced sentiment analysis 2021 Use of pre-trained models like BERT for sentiment analysis 2022\\n\\nTransfer learning with BERT and GPT for sentiment analysis Hybrid LSTM-CNN model classification Analyzing sentiment with pre-trained models fine-tuned for specific tasks\\n\\nfor document-level sentiment\\n\\n2024\\n\\n2023\\n\\nSennrich et al. [133]\\n\\nWu et al. [132]\\n\\nVaswani et al. [104]\\n\\n2015\\n\\n2016\\n\\n2017\\n\\nByte-Pair Encoding for handling rare words in translation models Google Neural Machine Translation with deep RNNs for im- proved accuracy Fully attention-based transformer models for superior translation performance\\n\\nYang et al. [135]\\n\\nSong et al. [136]\\n\\nKang et al. [134]\\n\\n2017 Hybrid model integrating RNNs into the transformer architecture Incorporating BERT into translation models for enhanced under- standing and fluency Bilingual attention-based machine translation model combining RNN with attention\\n\\n2019\\n\\n2023\\n\\nZulqarnain et al. [124]\\n\\n2024 Multi-stage feature attention mechanism model using GRU\\n\\n7.2. Speech Recognition\\n\\nRNNs have also made significant contributions to the field of speech recognition, leading to more accurate and efficient systems. Hinton et al. [137] explored the use of deep neural networks, including RNNs, for speech-to-text systems. Their research showed that RNNs could capture the temporal dependencies in speech signals, leading to significant improvements in transcription accuracy compared to previous methods.\\n\\nInformation 2024, 15, 517\\n\\nHannun et al. [138] introduced DeepSpeech, a state-of-the-art speech recognition system based on RNNs. DeepSpeech employed a deep LSTM network trained on a vast amount of labeled speech data, thereby improving transcription accuracy. This system was designed to handle noisy environments and diverse accents, making it robust for various real-world applications. Similarly, Amodei et al. [139] presented DeepSpeech2, which extended the capabilities of the original DeepSpeech model by incorporating bidirectional RNNs and a more extensive dataset. DeepSpeech2 achieved notable performance improve- ments, demonstrating that RNNs could effectively handle variations in speech patterns and accents.\\n\\nMeanwhile, Chiu et al. [140] proposed the use of RNN-transducer (RNN-T) models for end-to-end speech recognition. RNN-T models integrate both acoustic and language models into a single RNN framework, allowing for more efficient and accurate transcription. This integration reduced the complexity and latency of real-time speech recognition systems, making them more practical for deployment in real-world applications. Furthermore, Zhang et al. [141] proposed the use of convolutional recurrent neural networks (CRNNs) for speech recognition. CRNNs combine the strengths of CNNs for feature extraction and RNNs for sequence modeling, resulting in a hybrid architecture that is robust in both accuracy and computational efficiency. Specifically, this model was effective in handling long audio sequences and varying speech rates.\\n\\nRecently, Dong et al. [142] introduced the Speech-Transformer, a model that lever- ages the self-attention mechanism to process audio sequences in parallel, improving both speed and accuracy. This model demonstrated that transformer-based architectures could effectively handle the sequential nature of speech data, providing a competitive alterna- tive to traditional RNN-based models. Bhaskar and Thasleema [143] developed a speech recognition model using LSTM. The model achieved visual speech recognition using fa- cial expressions. Other studies that explored the use of different RNN variants in speech recognition include [144–147].\\n\\nIn the field of speech recognition, LSTM networks have been consistently recognized as the most effective RNN variant due to their ability to capture long-range dependencies in sequential data. LSTM, as utilized in systems like DeepSpeech by Hannun et al. [138], has demonstrated superior performance in handling the temporal dependencies inherent in speech signals. This capability is particularly crucial for maintaining context over long audio sequences, which directly translates to improved transcription accuracy. While newer models like the Speech-Transformer [142] leverage attention mechanisms for faster processing, LSTM networks remain a cornerstone in speech recognition due to their proven robustness and ability to handle complex variations in speech patterns. This makes them the preferred choice in scenarios where maintaining sequential order and context is critical, despite the growing popularity of transformer-based architectures.\\n\\n7.3. Time Series Forecasting\\n\\nRNNs have been extensively used in time series prediction due to their ability to model temporal dependencies and trends in sequential data. In financial forecasting, Fischer and Krauss [148] conducted a comprehensive study using deep RNNs to predict stock returns. Their results indicated that RNNs could outperform traditional ML models, such as support vector machines and random forests, in financial forecasting tasks. The study demonstrated that deep RNNs could learn intricate patterns in stock price movements, contributing to better forecasting accuracy.\\n\\nWith the advancement of deep learning techniques, Nelson et al. [149] proposed a model combining CNNs and RNNs for stock price prediction. The CNN component extracted local features from historical price data, while the RNN component captured the temporal dependencies. This hybrid model showed significant improvements in prediction performance, suggesting that integrating different neural network architectures could enhance financial forecasting. Also, attention mechanisms have been integrated into RNNs to improve financial forecasting.\\n\\n19 of 34\\n\\nInformation 2024, 15, 517\\n\\nLuo et al. [150] used an attention-based CNN-BiLSTM model that focused on relevant time steps in the input sequence, enhancing the model’s ability to capture important patterns in financial data. This approach allowed for more accurate predictions of stock prices and market trends by dynamically weighting the significance of past observations. Furthermore, Bao et al. [151] employed a novel deep learning framework combining LSTM with stacked autoencoders for financial time series forecasting. Their model utilized stacked autoencoders to reduce the dimensionality of input data and LSTM to model temporal dependencies. This method improved the model’s ability to predict future stock prices by effectively capturing both feature representations and sequential patterns.\\n\\nFeng et al. [152] explored the use of transfer learning for financial forecasting. They proposed a model that pre-trained an RNN on a large corpus of financial data and fine- tuned it on specific stock datasets. This approach employed the knowledge gained from broader market data to improve predictions on individual stocks, which demonstrates the potential of transfer learning in financial forecasting. Meanwhile, the application of reinforcement learning in conjunction with RNNs has gained attention in financial forecasting. Rundo [153] combined RL with LSTM to develop a trading strategy that maximizes returns. Their model learned optimal trading actions through interactions with the market environment, resulting in a robust and adaptive financial forecasting system.\\n\\nBeyond financial applications, RNNs have shown effectiveness in other domains, such as weather forecasting and renewable energy predictions, where modeling temporal dependencies is critical for accurate forecasts. In weather forecasting, Devi et al. [154] developed an RNN-based model specifically for weather prediction, which demonstrated superior performance in both short-term and long-term forecasting compared to traditional statistical methods. This model effectively captures the sequential dependencies in meteo- rological data, such as temperature, humidity, and atmospheric pressure, enabling more accurate and reliable forecasts. Additionally, Anshuka et al. [155] showed the effective- ness of using LSTM networks in predicting extreme weather events by learning complex temporal patterns in historical weather data. Furthermore, Lin et al. [100] proposed a model that integrates the attention mechanism with LSTM, which further improved the ability of RNNs, especially as the attention mechanism ensures the model focuses on critical features within the large dataset, thereby enhancing the accuracy of predictions in complex weather scenarios.\\n\\nIn the field of renewable energy, RNNs have been extensively applied to forecast energy generation from renewable sources such as wind and solar power. Marulanda et al. [156] utilized an LSTM-based model for short-term wind power forecasting, which showed signifi- cant improvements in prediction accuracy by capturing the non-linear and time-dependent characteristics of wind speed data. Similarly, Chen et al. [157] developed an advanced DL approach combining a bidirectional GRU with temporal convolutional networks (TCNs) for energy time series forecasting. This hybrid model was particularly effective in capturing both short-term fluctuations and long-term trends, leading to more reliable predictions. Moreover, RNNs have also been used to forecast energy demand in smart grids, where their ability to model temporal dependencies helps in optimizing the integration of renewable energy sources into the grid and improving overall energy management [158,159].\\n\\nFurthermore, RNNs have been applied to predict consumer demand patterns for goods and services, allowing businesses to optimize their supply chain management and reduce costs. For instance, Yildiz et al. [160] proposed a hybrid RNN model that combines LSTM with CNN to accurately predict electricity demand in urban areas, showing significant improvements over traditional forecasting techniques.\\n\\nMeanwhile, ESNs have also shown promise in weather forecasting and renewable energy predictions due to their ability to handle non-linear and chaotic time series data. For instance, Anshuka et al. [155] applied ESNs to model and predict extreme weather events, demonstrating the network’s ability to capture complex temporal patterns from historical weather data. Similarly, Marulanda et al. [156] used an ESN-based approach for short-term wind power forecasting, which effectively captured the non-linear dynamics of\\n\\n20 of 34\\n\\nInformation 2024, 15, 517\\n\\nApplication Domain\\n\\nSpeech recognition\\n\\nTime series forecasting\\n\\nwind speed and improved prediction accuracy compared to conventional methods. These studies highlight the versatility and robustness of ESNs in handling diverse time series forecasting tasks across different domains.\\n\\nAdditionally, Gao et al. [90] proposed a dynamic ensemble deep ESN specifically designed for wave height forecasting. This model adjusts reservoir weights dynamically, allowing it to model the complex and non-linear patterns often found in time series more effectively than traditional methods. Additionally, Bhambu et al. [161] introduced a re- current ensemble deep random vector functional link neural network for financial time series forecasting. This model integrates the strengths of both ESNs and recurrent net- works, providing superior performance in predicting financial market volatility and trends. Table 5 provides a summary of the RNN applications in both speech recognition and time series forecasting.\\n\\nAmong the various RNN architectures, LSTM networks stand out as the most effective for time series forecasting, especially in financial applications [162]. LSTM’s gating mechanisms allow it to maintain and utilize long-term dependencies, which are crucial for accurately predicting future trends based on historical data. The ability of LSTM to capture complex temporal patterns makes it particularly well-suited for financial markets, where long-range dependencies and intricate patterns in data are common. Meanwhile, when LSTM is combined with other techniques, such as CNNs for feature extraction or attention mechanisms for focusing on critical time steps, the models’ forecasting performance improves even further. This combination of adaptability, robustness, and precision demonstrates why LSTM is frequently considered the best RNN variant for time series-forecasting tasks.\\n\\nTable 5. Summary of RNNs in speech recognition and time series forecasting.\\n\\nReference\\n\\nYear Methods and Application\\n\\nHinton et al. [137]\\n\\n2012\\n\\nDeep neural networks, including RNNs, for speech-to-text systems\\n\\nHannun et al. [138]\\n\\n2014 DeepSpeech: LSTM-based speech recognition system\\n\\nAmodei et al. [139]\\n\\n2016\\n\\nDeepSpeech2: Enhanced LSTM-based speech recognition with bidirectional RNNs\\n\\nZhang et al. [141] Chiu et al. [140]\\n\\n2017 Convolutional RNN for robust speech recognition 2018 RNN-transducer models for end-to-end speech recognition\\n\\nDong et al. [142]\\n\\nBhaskar and Thasleema [143] Daouad et al. [144] Nasr et al. [146] Kumar et al. [147]\\n\\nDhanjal et al. [145]\\n\\nSpeech-Transformer: Leveraging self-attention for better pro- cessing of audio sequences LSTM for visual speech recognition using facial expressions\\n\\n2018\\n\\n2023 2023 Various RNN variants for automatic speech recognition 2023 2023\\n\\nEnd-to-end speech recognition using RNNs Performance evaluation of RNNs in speech recognition tasks Comprehensive study of different RNN models for speech recog- nition\\n\\n2024\\n\\nNelson et al. [149]\\n\\n2017 Hybrid CNN-RNN model for stock price prediction\\n\\nBao et al. [151]\\n\\nFischer and Krauss [148]\\n\\nFeng et al. [152]\\n\\nRundo [153]\\n\\nDevi et al. [154]\\n\\nAnshuka et al. [155]\\n\\n2017\\n\\n2018\\n\\n2019\\n\\n2019\\n\\n2024\\n\\n2022\\n\\nCombining LSTM with stacked autoencoders for financial time series forecasting Deep RNNs for predicting stock returns, outperforming tradi- tional ML models Transfer learning with RNNs for stock prediction Combining reinforcement learning with LSTM for trading strat- egy development RNN-based model for weather prediction and capturing se- quential dependencies in meteorological data LSTM networks for predicting extreme weather events by learn- ing complex temporal patterns\\n\\n21 of 34\\n\\nInformation 2024, 15, 517\\n\\nApplication Domain\\n\\nTable 5. Cont.\\n\\nReference\\n\\nYear Methods and Application\\n\\nLin et al. [100]\\n\\nMarulanda et al. [156]\\n\\nChen et al. [157]\\n\\nHasanat et al. [158]\\n\\nAsiri et al. [159]\\n\\nYildiz et al. [160]\\n\\n2022\\n\\n2023\\n\\n2024\\n\\n2024\\n\\n2024\\n\\n2024\\n\\nIntegrating attention mechanisms with LSTM for enhanced weather fore- casting accuracy LSTM model for short-term wind power forecasting and improving pre- diction accuracy Bidirectional GRU with TCNs for energy time series forecasting RNNs for forecasting energy demand in smart grids and optimizing renewable energy integration Short-term renewable energy predictions using RNN-based models Hybrid model of LSTM with CNN for accurate electricity demand predic- tion\\n\\nLuo et al. [150] Gao et al. [90]\\n\\n2024 Attention-based CNN-BiLSTM model for improved financial forecasting 2023 Dynamic ensemble deep ESN for wave height forecasting\\n\\nBhambu et al. [161]\\n\\n2024\\n\\nRecurrent ensemble deep random vector functional link neural network for financial time series forecasting\\n\\n7.4. Signal Processing\\n\\nRNNs, particularly ESNs, have seen significant applications in various signal-processing tasks due to their efficient training and robust performance in handling time-dependent data. One key area of application is physiological signal processing. Mastoi et al. [163] de- veloped an ESN-based approach for the real-time monitoring and prediction of heart rate variability. Their approach outperformed traditional methods in terms of accu- racy and computational efficiency, demonstrating ESNs’ potential in real-time health monitoring systems.\\n\\nESNs have also been extensively used in speech-processing tasks. Valin et al. [164] proposed an ESN architecture specifically designed for speech signal enhancement. This model demonstrated improved noise reduction and speech intelligibility in noisy envi- ronments, which is critical for applications such as hearing aids and speech recognition systems. The model’s ability to handle temporal dependencies in speech signals made it particularly effective in enhancing audio quality under challenging conditions.\\n\\nAdditionally, ESNs have been applied in the preprocessing and analysis of non- stationary and noisy time series data. Gao et al. [91] integrated the empirical wavelet transform (EWT) with ESNs to enhance performance in time series forecasting. This hybrid approach demonstrated that combining the EWT’s ability to decompose complex signals with the robust modeling capabilities of ESNs leads to superior performance, particularly in scenarios where data is noisy or exhibits non-stationary behavior. This integration demonstrates the adaptability of ESNs to a wide range of signal processing challenges, reinforcing their utility in domains requiring accurate and efficient time series analysis.\\n\\n7.5. Bioinformatics\\n\\nIn bioinformatics, RNNs have been used to analyze biological sequences such as DNA, RNA, and proteins. Li et al. [165] employed RNNs for gene prediction and protein structure prediction, demonstrating the ability of RNNs to capture dependencies within biological sequences and providing insights into genetic information and biological pro- cesses. Zhang et al. [166] used bidirectional LSTM in predicting DNA-binding protein sequences. Their model, called DeepSite, leveraged the sequential nature of biological data, achieving higher accuracy in identifying binding sites compared to traditional methods. This application demonstrated the potential of RNNs to enhance our understanding of protein-DNA interactions.\\n\\nIn the field of proteomics, RNNs have been used for protein structure prediction and function annotation. Xu et al. [167] developed an RNN-based model to predict protein secondary structures, showing that RNNs could effectively captures the sequential depen-\\n\\n22 of 34\\n\\nInformation 2024, 15, 517\\n\\ndencies in amino acid sequences. This application provided significant advancements in protein structure prediction, which is essential for drug discovery and disease research.\\n\\nMore recently, researchers have explored the integration of RNNs with other neural network architectures for bioinformatics applications. For example, Yadav et al. [168] combined BiLSTM with CNNs to analyze protein sequences. Their model extracted local features and captured long-range dependencies with BiLSTM, resulting in improved per- formance in protein classification tasks. Additionally, the use of ensemble deep learning has enhanced the performance of RNNs in bioinformatics. Aybey et al. [169] introduced an ensemble model for predicting protein–protein interactions using RNNs, GRUs, and CNNs. The model improves the accuracy of interaction predictions. This approach highlighted the potential of ensemble deep learning to enhance the interpretability and performance of RNNs in bioinformatics.\\n\\nIn bioinformatics, RNNs, specifically LSTM networks and GRUs, have established themselves as the best models for analyzing biological sequences due to their ability to process long sequences and maintain information over long distances, crucial for under- standing complex biological structures and functions. Bidirectional LSTM, used by Zhang et al. [166] in predicting DNA-binding protein sequences, is particularly effective, as it processes sequences in both forward and backward directions, providing a better context and significantly improving prediction accuracy over unidirectional approaches. This capability makes it preferable for tasks where understanding the full context of a sequence is essential, such as gene prediction, protein folding, and other complex bioinformatics applications involving sequential data.\\n\\n7.6. Autonomous Vehicles\\n\\nRNNs play an important role in autonomous vehicles by processing sequential data from sensors to make driving decisions. Li et al. [170] used RNNs for path planning, object detection, and trajectory prediction, enabling autonomous vehicles to navigate complex environments and make real-time decisions. Following this foundational work, researchers have continued to explore and enhance the use of RNNs in autonomous driving. For instance, Lee et al. [171] developed a deep learning framework that integrates LSTM with CNN for end-to-end driving. Their model utilized CNN to extract spatial features from camera images and LSTM to capture temporal dependencies, which improved the accuracy and robustness of driving decisions in dynamic environments.\\n\\nCodevilla et al. [172] introduced a conditional imitation learning approach that com- bined RNNs with imitation learning for autonomous driving. The model learned from human driving demonstrations and used RNNs to predict future actions based on past observations. This approach allowed the vehicle to adapt to various driving conditions and make safer decisions in complex scenarios. Additionally, researchers have explored the use of LSTM for trajectory prediction in autonomous vehicles. Altché and de La Fortelle [173] proposed an LSTM-based model that predicts the future trajectories of surrounding vehi- cles. This model leverages the sequential nature of traffic data to anticipate the movements of other road users, enabling more accurate and proactive path planning for autonomous vehicles. Meanwhile, attention mechanisms have been integrated into RNN models to enhance their performance in autonomous driving tasks. Li et al. [174] introduced an attention-based LSTM model that focuses on relevant parts of the data, improving the detection and tracking of video objects.\\n\\nResearchers have also explored the use of RNNs for behavior prediction in autonomous driving. Li et al. [175] proposed a model that combines RNNs with CNN to predict the intentions of other drivers. Their approach used sequential data to learn the behavioral patterns of surrounding vehicles, enabling the autonomous vehicle to anticipate potential hazards and respond accordingly. In addition, researchers have investigated the use of RNNs for decision-making in autonomous vehicles. Liu and Diao [176] introduced a deep reinforcement learning framework that incorporates GRU for decision-making in complex\\n\\n23 of 34\\n\\nInformation 2024, 15, 517\\n\\ntraffic scenarios. Their model used RNNs to process sequential observations and make real-time decisions, achieving state-of-the-art performance in various driving tasks.\\n\\nFurthermore, the integration of LSTM with CNNs seems to represent the best approach in autonomous vehicle applications, as demonstrated by Lee et al. [171]. This combination leverages LSTM’s ability to understand temporal dynamics and CNNs’ strength in spatial feature extraction, making it robust for real-time applications like driving, where both spatial and temporal understandings are crucial for decision-making. The hybrid nature of these models allows for a better understanding and processing of the vast amounts of data from various sensors, ensuring better performance in navigation and real-time decision making in dynamic environments.\\n\\n7.7. Anomaly Detection\\n\\nRNNs are used in anomaly detection across different fields, such as cybersecurity, industrial monitoring, and healthcare. Altindal et al. [177] demonstrated the use of LSTM networks for anomaly detection in time series data, showing that RNNs could effectively model normal behavior patterns and identify deviations indicative of anomalies. Sim- ilarly, Matar et al. [178] proposed a model for anomaly detection in multivariate time series. Their model utilized BiLSTM to learn temporal dependencies and to detect devia- tions from normal patterns. This approach was effective in industrial applications where monitoring the health of machinery and predicting failures is critical. In cybersecurity, Kumaresan et al. [179] applied RNNs to detect anomalies in network traffic. Their model analyzed sequential data to identify unusual patterns that could indicate security breaches or malicious activities. The use of RNNs allowed for real-time detection and response to potential threats, enhancing the security of network systems.\\n\\nFurthermore, Li et al. [180] explored the application of RNNs for anomaly detection in manufacturing processes. They developed a model combining RNNs with transfer learning to capture both temporal dependencies and feature representations. This method improved the detection of anomalies in complex industrial processes, contributing to the optimization of production efficiency and quality control. In healthcare, researchers have utilized RNNs for detecting anomalies in physiological signals. For instance, Mini et al. [181] employed RNNs to detect abnormal patterns in electrocardiogram (ECG) signals. Their model accurately identified deviations indicative of cardiac arrhythmias, demonstrating the potential of RNNs to assist in the early diagnosis and monitoring of heart conditions. Moreover, advances in unsupervised learning have further enhanced the capabilities of RNNs in anomaly detection. Zhou and Paffenroth [182] introduced a robust deep au- toencoder model that leverages RNNs for unsupervised anomaly detection. This approach effectively captured the underlying structure of the data, identifying anomalies without requiring labeled training data. Ren et al. [183] proposed an attention-based RNN model that focuses on relevant time steps in the data, improving the accuracy and interpretabil- ity of anomaly detection. This approach allowed for the more precise identification of anomalies by dynamically weighting the importance of different parts of the sequence. Ad- ditionally, hybrid models combining RNNs with other neural network architectures have also been employed in anomaly detection. Munir et al. [184] developed a hybrid model that integrates CNNs and RNNs to detect anomalies in multivariate time series data. The CNN component extracted local features, while the RNN component captured temporal dependencies, resulting in improved performance in various anomaly detection tasks.\\n\\nThe BiLSTM model stands out as the best RNN architecture in anomaly detection, especially in multivariate time series data, where understanding the influence of past and future input contexts is crucial. Matar et al. [178] demonstrated that BiLSTM effectively capture temporal dependencies in both directions, which is critical in anomaly detection scenarios where anomalies may be contextually linked to events in both the past and the future. This bidirectional processing capability allows for more robust detection of anomalies across various applications, from industrial monitoring to cybersecurity, making it the most suitable RNN model for these tasks.\\n\\n24 of 34\\n\\nInformation 2024, 15, 517\\n\\nA summary of RNN applications in bioinformatics, autonomous vehicles, and anomaly\\n\\ndetection is shown in Table 6.\\n\\nTable 6. Summary of RNNs in signal processing, bioinformatics, autonomous vehicles, and anomaly detection.\\n\\nApplication Domain\\n\\nReference\\n\\nYear Methods and Application\\n\\nSignal processing\\n\\nMastoi et al. [163] Valin et al. [164] Gao et al. [91]\\n\\n2019 2021 2021\\n\\nESNs for real-time heart rate variability monitoring ESNs for speech signal enhancement in noisy environments EWT integrated with ESNs for enhanced time series forecasting\\n\\nBioinformatics\\n\\nLi et al. [165]\\n\\n2019 RNNs for gene prediction and protein-structure prediction\\n\\nZhang et al. [166]\\n\\n2020\\n\\nBidirectional LSTM for predicting DNA-binding protein sequences\\n\\nXu et al. [167] Yadav et al. [168] Aybey et al. [169]\\n\\n2021 RNN-based model for predicting protein secondary structures 2019 Combining BiLSTM with CNNs for protein sequence analysis 2023\\n\\nEnsemble model for predicting protein–protein interactions\\n\\nAutonomous vehicles Altché and de La Fortelle [173]\\n\\nCodevilla et al. [172] Li et al. [170]\\n\\n2017 LSTM for predicting the future trajectories of vehicles 2018 RNNs with imitation learning for autonomous driving 2020 RNNs for path planning and object detection\\n\\nLee et al. [171]\\n\\n2020\\n\\nIntegrating LSTM with CNN for end-to-end autonomous driving\\n\\nLi et al. [174] Liu and Diao [176]\\n\\n2024 Attention-based LSTM for video object tracking 2024 GRU with deep reinforcement learning for decision-making\\n\\nAnomaly detection\\n\\nZhou and Paffenroth [182]\\n\\n2017\\n\\nRNNs autoencoders\\n\\nin unsupervised anomaly detection with deep\\n\\nMunir et al. [184] Ren et al. [183]\\n\\n2018 Hybrid CNN-RNN model for anomaly detection in time series 2019 Attention-based RNN model for anomaly detection\\n\\nLi et al. [180]\\n\\n2023\\n\\nRNNs with Transfer learning for anomaly detection in manufacturing\\n\\nMini et al. [181] Matar et al. [178] Kumaresan et al. [179] Altindal et al. [177]\\n\\n2023 RNNs for detecting anomalies in ECG signals 2023 2024 RNNs for detecting network traffic anomalies 2024\\n\\nBiLSTM for anomaly detection in multivariate time series\\n\\nLSTM for anomaly detection in time series data\\n\\n8. Challenges and Future Research Directions\\n\\nDespite significant advancements, several unresolved problems are encountered when applying RNNs. Addressing these issues is crucial for further improving the performance and usage of RNNs.\\n\\n8.1. Scalability and Efficiency\\n\\nTraining RNNs on large datasets with long sequences remains computationally inten- sive and time-consuming [185–187]. Although techniques like gradient checkpointing and hardware accelerators have provided improvements, the sequential nature of RNNs contin- ues to limit their scalability compared to parallelizable architectures like transformers [188]. Future research could focus on developing more efficient training algorithms and exploring asynchronous and parallel training methods to distribute the computational load more effectively. Additionally, hybrid architectures that combine RNNs with other models, such as integrating RNNs with attention mechanisms or convolutional layers, could provide new solutions. These hybrid models have the potential to reduce training times and improve scalability while maintaining the performance advantages of RNNs [104].\\n\\n8.2. Interpretability and Explainability\\n\\nRNNs are often perceived as “black-box” models due to their complex internal dy- namics, making it challenging to interpret their decisions [189,190]. Although attention mechanisms and post hoc explanation techniques like Local Interpretable Model-Agnostic Explanations (LIMEs) and Shapley Addictive Explanations (SHAPs) have been proposed\\n\\n25 of 34\\n\\nInformation 2024, 15, 517\\n\\nto improve interpretability, these methods can still be improved to further provide more comprehensive explanations [191]. Therefore, future research should aim to develop inher- ently interpretable RNN architectures and hierarchical models that offer structured insights into the model’s decision-making process. Additionally, integrating domain knowledge into RNN models can help align their behavior with human reasoning, enhancing both interpretability and performance in specialized applications.\\n\\n8.3. Bias and Fairness\\n\\nRNNs can inadvertently learn and propagate biases present in the training data, lead- ing to unfair predictions. While various bias detection and mitigation techniques have been developed, such as fairness-aware algorithms and adversarial training, these methods need further refinement to ensure fairness across diverse applications and datasets [192–194]. Research should continue to focus on developing robust bias detection techniques and fair training algorithms that explicitly incorporate fairness constraints. Additionally, trans- parency and accountability frameworks, including external audits and impact assessments, are essential for ensuring that RNNs are developed and deployed responsibly.\\n\\n8.4. Data Dependency and Quality\\n\\nRNNs require large amounts of high-quality, labeled sequential data for effective training [195]. In many real-world scenarios, such data may be scarce, noisy, or incomplete. Although data augmentation, transfer learning, and semi-supervised learning techniques have been explored, these methods require further refinement to handle diverse data challenges more effectively. Future research should focus on enhancing these techniques to improve the robustness of RNNs when trained on limited or imperfect data. Additionally, developing new methods for utilizing unlabeled data and integrating domain-specific knowledge can further improve the performance of RNNs in data-scarce environments.\\n\\n8.5. Overfitting and Generalization\\n\\nRNNs, particularly deep architectures, are prone to overfitting, especially when trained on small datasets [196]. Ensuring that RNN models generalize well to unseen data without overfitting remains a significant challenge. While regularization techniques like dropout and L2 regularization are commonly used, more robust methods for improving general- ization are needed. Future research can explore advanced regularization techniques, such as adversarial training and ensemble methods, to enhance the generalization capabilities of RNNs. Additionally, applying data augmentation and transfer learning can help RNN models learn more robust features, improving their ability to generalize to new data.\\n\\n9. Conclusions\\n\\nRNNs have demonstrated a remarkable ability to model sequential data, making them indispensable in numerous ML applications such as natural language processing, speech recognition, time series prediction, bioinformatics, and autonomous systems. This paper provided a comprehensive overview of RNNs and their variants, covering fundamental architectures like basic RNNs, LSTM networks, and GRUs, as well as advanced variants, including bidirectional RNNs, peephole LSTM, ESNs, and IndRNNs. This study has provided a detailed and comprehensive review of RNNs, as well as their architectures, applications, and challenges. The paper will be a valuable resource for researchers and practitioners in the field of machine learning, helping to guide future developments and applications of RNNs.\\n\\nAuthor Contributions: Conceptualization, I.D.M., T.G.S. and G.O.; methodology, I.D.M.; validation, I.D.M., T.G.S. and G.O.; investigation, I.D.M., T.G.S. and G.O.; resources, T.G.S.; writing—original draft preparation, I.D.M. and G.O.; writing—review and editing, I.D.M., T.G.S. and G.O.; visual- ization, I.D.M.; supervision, T.G.S. All authors have read and agreed to the published version of the manuscript.\\n\\n26 of 34\\n\\nInformation 2024, 15, 517\\n\\nFunding: This research received no external funding.\\n\\nInstitutional Review Board Statement: Not applicable.\\n\\nInformed Consent Statement: Not applicable.\\n\\nData Availability Statement: Not applicable.\\n\\nConflicts of Interest: The authors declare no conflicts of interest.\\n\\nAbbreviations\\n\\nThe following abbreviations are used in this manuscript:\\n\\nArtificial intelligence AI ANN Artificial neural network BiLSTM Bidirectional long short-term memory CNN DL GRU LSTM ML NAS NLP RNN RL SHAPs TPU VAE\\n\\nConvolutional neural network Deep learning Gated recurrent unit Long short-term memory Machine learning Neural architecture search Natural language processing Recurrent neural network Reinforcement learning Shapley Additive Explanations Tensor processing unit Variational autoencoder\\n\\nReferences\\n\\n1.\\n\\n2.\\n\\n3.\\n\\n4.\\n\\nO’Halloran, T.; Obaido, G.; Otegbade, B.; Mienye, I.D. A deep learning approach for Maize Lethal Necrosis and Maize Streak Virus disease detection. Mach. Learn. Appl. 2024, 16, 100556. [CrossRef] Peng, Y.; He, L.; Hu, D.; Liu, Y.; Yang, L.; Shang, S. Decoupling Deep Learning for Enhanced Image Recognition Interpretability. ACM Trans. Multimed. Comput. Commun. Appl. 2024. [CrossRef] Khan, W.; Daud, A.; Khan, K.; Muhammad, S.; Haq, R. Exploring the frontiers of deep learning and natural language processing: A comprehensive overview of key challenges and emerging trends. Nat. Lang. Process. J. 2023, 4, 100026. [CrossRef] Obaido, G.; Achilonu, O.; Ogbuokiri, B.; Amadi, C.S.; Habeebullahi, L.; Ohalloran, T.; Chukwu, C.W.; Mienye, E.; Aliyu, M.; Fasawe, O.; et al. An Improved Framework for Detecting Thyroid Disease Using Filter-Based Feature Selection and Stacking Ensemble. IEEE Access 2024, 12, 89098–89112. [CrossRef]\\n\\n5. Mienye, I.D.; Obaido, G.; Aruleba, K.; Dada, O.A. Enhanced Prediction of Chronic Kidney Disease using Feature Selection and Boosted Classifiers. In Proceedings of the International Conference on Intelligent Systems Design and Applications, Virtual, 13–15 December 2021; pp. 527–537. Al-Jumaili, A.H.A.; Muniyandi, R.C.; Hasan, M.K.; Paw, J.K.S.; Singh, M.J. Big data analytics using cloud computing based frameworks for power management systems: Status, constraints, and future recommendations. Sensors 2023, 23, 2952. [CrossRef] Gill, S.S.; Wu, H.; Patros, P.; Ottaviani, C.; Arora, P.; Pujol, V.C.; Haunschild, D.; Parlikad, A.K.; Cetinkaya, O.; Lutfiyya, H.; et al. Modern computing: Vision and challenges. Telemat. Inform. Rep. 2024, 13, 100116. [CrossRef]\\n\\n6.\\n\\n7.\\n\\n8. Mienye, I.D.; Jere, N. A Survey of Decision Trees: Concepts, Algorithms, and Applications. IEEE Access 2024, 12, 86716–86727. [CrossRef] Aruleba, R.T.; Adekiya, T.A.; Ayawei, N.; Obaido, G.; Aruleba, K.; Mienye, I.D.; Aruleba, I.; Ogbuokiri, B. COVID-19 diagnosis: A review of rapid antigen, RT-PCR and artificial intelligence methods. Bioengineering 2022, 9, 153. [CrossRef]\\n\\n8. Mienye, I.D.; Jere, N. A Survey of Decision Trees: Concepts, Algorithms, and Applications. IEEE Access 2024, 12, 86716–86727. [CrossRef] Aruleba, R.T.; Adekiya, T.A.; Ayawei, N.; Obaido, G.; Aruleba, K.; Mienye, I.D.; Aruleba, I.; Ogbuokiri, B. COVID-19 diagnosis: A review of rapid antigen, RT-PCR and artificial intelligence methods. Bioengineering 2022, 9, 153. [CrossRef]\\n\\n10. Alhajeri, M.S.; Ren, Y.M.; Ou, F.; Abdullah, F.; Christofides, P.D. Model predictive control of nonlinear processes using transfer learning-based recurrent neural networks. Chem. Eng. Res. Des. 2024, 205, 1–12. [CrossRef] Shahinzadeh, H.; Mahmoudi, A.; Asilian, A.; Sadrarhami, H.; Hemmati, M.; Saberi, Y. Deep Learning: A Overview of Theory and Architectures. In Proceedings of the 2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing (AISP), Babol, Iran, 21–22 February 2024; pp. 1–11.\\n\\n10. Alhajeri, M.S.; Ren, Y.M.; Ou, F.; Abdullah, F.; Christofides, P.D. Model predictive control of nonlinear processes using transfer learning-based recurrent neural networks. Chem. Eng. Res. Des. 2024, 205, 1–12. [CrossRef] Shahinzadeh, H.; Mahmoudi, A.; Asilian, A.; Sadrarhami, H.; Hemmati, M.; Saberi, Y. Deep Learning: A Overview of Theory and Architectures. In Proceedings of the 2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing (AISP), Babol, Iran, 21–22 February 2024; pp. 1–11.\\n\\n12. Baruah, R.D.; Organero, M.M. Explicit Context Integrated Recurrent Neural Network for applications in smart environments. Expert Syst. Appl. 2024, 255, 124752. [CrossRef]\\n\\n13. Werbos, P. Backpropagation through time: What it does and how to do it. Proc. IEEE 1990, 78, 1550–1560. [CrossRef] 14. Lalapura, V.S.; Amudha, J.; Satheesh, H.S. Recurrent neural networks for edge intelligence: A survey. ACM Comput. Surv. (CSUR) 2021, 54, 1–38. [CrossRef]\\n\\n27 of 34\\n\\nInformation 2024, 15, 517\\n\\n15. Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural Comput. 1997, 9, 1735–1780. [CrossRef] [PubMed] 16. Cho, K.; Van Merriënboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; Bengio, Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv 2014, arXiv:1406.1078.\\n\\n17. Liu, F.; Li, J.; Wang, L. PI-LSTM: Physics-informed long short-term memory network for structural response modeling. Eng. Struct. 2023, 292, 116500. [CrossRef]\\n\\n18. Ni, Q.; Ji, J.; Feng, K.; Zhang, Y.; Lin, D.; Zheng, J. Data-driven bearing health management using a novel multi-scale fused feature and gated recurrent unit. Reliab. Eng. Syst. Saf. 2024, 242, 109753. [CrossRef]\\n\\n19. Niu, Z.; Zhong, G.; Yue, G.; Wang, L.N.; Yu, H.; Ling, X.; Dong, J. Recurrent attention unit: A new gated recurrent unit for long-term memory of important parts in sequential data. Neurocomputing 2023, 517, 1–9. [CrossRef]\\n\\n20. Lipton, Z.C.; Berkowitz, J.; Elkan, C. A critical review of recurrent neural networks for sequence learning. arXiv:1506.00019.\\n\\n21. Yu, Y.; Si, X.; Hu, C.; Zhang, J. A review of recurrent neural networks: LSTM cells and network architectures. Neural Comput. 2019, 31, 1235–1270. [CrossRef]\\n\\n22. Tarwani, K.M.; Edem, S. Survey on recurrent neural network in natural language processing. Int. J. Eng. Trends Technol. 2017, 48, 301–304. [CrossRef]\\n\\n23. Tsoi, A.C.; Back, A.D. Locally recurrent globally feedforward networks: A critical review of architectures. IEEE Trans. Neural Netw. 1994, 5, 229–239. [CrossRef] [PubMed]\\n\\n24. Mastorocostas, P.A.; Theocharis, J.B. A stable learning algorithm for block-diagonal recurrent neural networks: Application to the analysis of lung sounds. IEEE Trans. Syst. Man. Cybern. Part B (Cybern.) 2006, 36, 242–254. [CrossRef] [PubMed]\\n\\n25. Dutta, K.K.; Poornima, S.; Sharma, R.; Nair, D.; Ploeger, P.G. Applications of Recurrent Neural Network: Overview and Case Studies. In Recurrent Neural Networks; CRC Press: Boca Raton, FL, USA, 2022; pp. 23–41.\\n\\n26. Quradaa, F.H.; Shahzad, S.; Almoqbily, R.S. A systematic literature review on the applications of recurrent neural networks in code clone research. PLoS ONE 2024, 19, e0296858. [CrossRef]\\n\\n27. Goodfellow, I.; Bengio, Y.; Courville, A. Deep Learning; MIT Press: Cambridge, MA, USA, 2016. 28. Greff, K.; Srivastava, R.K.; Koutník, J.; Steunebrink, B.R.; Schmidhuber, J. LSTM: A search space odyssey. IEEE Trans. Neural Netw. Learn. Syst. 2016, 28, 2222–2232. [CrossRef] [PubMed]\\n\\n29. Al-Selwi, S.M.; Hassan, M.F.; Abdulkadir, S.J.; Muneer, A.; Sumiea, E.H.; Alqushaibi, A.; Ragab, M.G. RNN-LSTM: From J. King Saud-Univ.-Comput. Inf. Sci. 2024, 36, 102068. applications to modeling techniques and beyond—Systematic review. [CrossRef]\\n\\n30. Zaremba, W.; Sutskever, I.; Vinyals, O. Recurrent neural network regularization. arXiv 2014, arXiv:1409.2329. 31. Bai, S.; Kolter, J.Z.; Koltun, V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv 2018, arXiv:1803.01271.\\n\\n32. Che, Z.; Purushotham, S.; Cho, K.; Sontag, D.; Liu, Y. Recurrent neural networks for multivariate time series with missing values. Sci. Rep. 2018, 8, 6085. [CrossRef]\\n\\n33. Chung, J.; Gulcehre, C.; Cho, K.; Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv 2014, arXiv:1412.3555.\\n\\n34. Badawy, M.; Ramadan, N.; Hefny, H.A. Healthcare predictive analytics using machine learning and deep learning techniques: A survey. J. Electr. Syst. Inf. Technol. 2023, 10, 40. [CrossRef] Ismaeel, A.G.; Janardhanan, K.; Sankar, M.; Natarajan, Y.; Mahmood, S.N.; Alani, S.; Shather, A.H. Traffic pattern classification in smart cities using deep recurrent neural network. Sustainability 2023, 15, 14522. [CrossRef]\\n\\n34. Badawy, M.; Ramadan, N.; Hefny, H.A. Healthcare predictive analytics using machine learning and deep learning techniques: A survey. J. Electr. Syst. Inf. Technol. 2023, 10, 40. [CrossRef] Ismaeel, A.G.; Janardhanan, K.; Sankar, M.; Natarajan, Y.; Mahmood, S.N.; Alani, S.; Shather, A.H. Traffic pattern classification in smart cities using deep recurrent neural network. Sustainability 2023, 15, 14522. [CrossRef]\\n\\n36. Mers, M.; Yang, Z.; Hsieh, Y.A.; Tsai, Y. Recurrent neural networks for pavement performance forecasting: Review and model performance comparison. Transp. Res. Rec. 2023, 2677, 610–624. [CrossRef]\\n\\n37. Chen, Y.; Cheng, Q.; Cheng, Y.; Yang, H.; Yu, H. Applications of recurrent neural networks in environmental factor forecasting: A review. Neural Comput. 2018, 30, 2855–2881. [CrossRef] [PubMed]\\n\\n38. Linardos, V.; Drakaki, M.; Tzionas, P.; Karnavas, Y.L. Machine learning in disaster management: Recent developments in methods and applications. Mach. Learn. Knowl. Extr. 2022, 4, 446–473. [CrossRef]\\n\\n39. Zhang, J.; Liu, H.; Chang, Q.; Wang, L.; Gao, R.X. Recurrent neural network for motion trajectory prediction in human-robot collaborative assembly. CIRP Ann. 2020, 69, 9–12. [CrossRef]\\n\\n40. Tsantekidis, A.; Passalis, N.; Tefas, A. Recurrent Neural Networks. In Deep Learning for Robot Perception and Cognition; Elsevier: Amsterdam, The Netherlands, 2022; pp. 101–115.\\n\\n41. Mienye, I.D.; Jere, N. Deep Learning for Credit Card Fraud Detection: A Review of Algorithms, Challenges, and Solutions. IEEE Access 2024, 12, 96893–96910. [CrossRef]\\n\\n42. Mienye, I.D.; Sun, Y. A machine learning method with hybrid feature selection for improved credit card fraud detection. Appl. Sci. 2023, 13, 7254. [CrossRef]\\n\\n43. Rezk, N.M.; Purnaprajna, M.; Nordström, T.; Ul-Abdin, Z. Recurrent neural networks: An embedded computing perspective. IEEE Access 2020, 8, 57967–57996. [CrossRef]\\n\\n44. Yu, Y.; Adu, K.; Tashi, N.; Anokye, P.; Wang, X.; Ayidzoe, M.A. Rmaf: Relu-memristor-like activation function for deep learning. IEEE Access 2020, 8, 72727–72741. [CrossRef]\\n\\n28 of 34\\n\\nInformation 2024, 15, 517\\n\\n45. Mienye, I.D.; Ainah, P.K.; Emmanuel, I.D.; Esenogho, E. Sparse Noise Minimization in Image Classification using Genetic Algorithm and DenseNet. In Proceedings of the 2021 Conference on Information Communications Technology and Society (ICTAS), Durban, South Africa, 10–11 March 2021; pp. 103–108.\\n\\n46. Ciaburro, G.; Venkateswaran, B. Neural Networks with R: SMART Models Using CNN, RNN, Deep Learning, and Artificial Intelligence Principles; Packt Publishing Ltd.: Birmingham, UK, 2017.\\n\\n47. Nwankpa, C.; Ijomah, W.; Gachagan, A.; Marshall, S. Activation functions: Comparison of trends in practice and research for deep learning. arXiv 2018, arXiv:1811.03378. Szandała, T. Review and comparison of commonly used activation functions for deep neural networks. Bio-Inspired Neurocomp. 2021, 203–224.\\n\\n47. Nwankpa, C.; Ijomah, W.; Gachagan, A.; Marshall, S. Activation functions: Comparison of trends in practice and research for deep learning. arXiv 2018, arXiv:1811.03378. Szandała, T. Review and comparison of commonly used activation functions for deep neural networks. Bio-Inspired Neurocomp. 2021, 203–224.\\n\\n49. Clevert, D.A.; Unterthiner, T.; Hochreiter, S. Fast and accurate deep network learning by exponential linear units (elus). arXiv 2015, arXiv:1511.07289.\\n\\n50. Dubey, S.R.; Singh, S.K.; Chaudhuri, B.B. Activation functions in deep learning: A comprehensive survey and benchmark. Neurocomputing 2022, 503, 92–108. [CrossRef]\\n\\n51. Obaido, G.; Mienye, I.D.; Egbelowo, O.F.; Emmanuel, I.D.; Ogunleye, A.; Ogbuokiri, B.; Mienye, P.; Aruleba, K. Supervised machine learning in drug discovery and development: Algorithms, applications, challenges, and prospects. Mach. Learn. Appl. 2024, 17, 100576. [CrossRef]\\n\\n52. Mienye, I.D.; Sun, Y. Effective Feature Selection for Improved Prediction of Heart Disease. In Proceedings of the Pan-African Artificial Intelligence and Smart Systems Conference, Durban, South Africa, 4–6 December 2021; pp. 94–107.\\n\\n53. Martins, A.; Astudillo, R. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. Proceedings of the International Conference on Machine Learning, New York, NY, USA, 20–22 June 2016; pp. 1614–1623. 54. Bianchi, F.M.; Maiorino, E.; Kampffmeyer, M.C.; Rizzi, A.; Jenssen, R.; Bianchi, F.M.; Maiorino, E.; Kampffmeyer, M.C.; Rizzi, A.; Jenssen, R. Properties and Training in Recurrent Neural Networks. In Recurrent Neural Networks for Short-Term Load Forecasting: An Overview and Comparative Analysis; Springer: Berlin/Heidelberg, Germany, 2017; pp. 9–21.\\n\\n55. Mohajerin, N.; Waslander, S.L. State Initialization for Recurrent Neural Network Modeling of Time-Series Data. In Proceedings of the 2017 International Joint Conference on Neural Networks (IJCNN), Anchorage, AK, USA, 14–19 May 2017; pp. 2330–2337. Forgione, M.; Muni, A.; Piga, D.; Gallieri, M. On the adaptation of recurrent neural networks for system identification. Automatica 2023, 155, 111092. [CrossRef] 56.\\n\\n57. Zhang, J.; He, T.; Sra, S.; Jadbabaie, A. Why gradient clipping accelerates training: A theoretical justification for adaptivity. arXiv 2019, arXiv:1905.11881.\\n\\n58. Qian, J.; Wu, Y.; Zhuang, B.; Wang, S.; Xiao, J. Understanding Gradient Clipping in Incremental Gradient Methods. In Proceedings of the International Conference on Artificial Intelligence and Statistics, Virtual, 13–15 April 2021; pp. 1504–1512. Fei, H.; Tan, F. Bidirectional grid long short-term memory (bigridlstm): A method to address context-sensitivity and vanishing gradient. Algorithms 2018, 11, 172. [CrossRef]\\n\\n58. Qian, J.; Wu, Y.; Zhuang, B.; Wang, S.; Xiao, J. Understanding Gradient Clipping in Incremental Gradient Methods. In Proceedings of the International Conference on Artificial Intelligence and Statistics, Virtual, 13–15 April 2021; pp. 1504–1512. Fei, H.; Tan, F. Bidirectional grid long short-term memory (bigridlstm): A method to address context-sensitivity and vanishing gradient. Algorithms 2018, 11, 172. [CrossRef]\\n\\n60. Dong, X.; Chowdhury, S.; Qian, L.; Li, X.; Guan, Y.; Yang, J.; Yu, Q. Deep learning for named entity recognition on Chinese electronic medical records: Combining deep transfer learning with multitask bi-directional LSTM RNN. PLoS ONE 2019, 14, e0216046. [CrossRef] [PubMed]\\n\\n61. Chorowski, J.K.; Bahdanau, D.; Serdyuk, D.; Cho, K.; Bengio, Y. Attention-based models for speech recognition. Adv. Neural Inf. Process. Syst. 2015, 28.\\n\\n62. Zhou, M.; Duan, N.; Liu, S.; Shum, H.Y. Progress in neural NLP: Modeling, learning, and reasoning. Engineering 2020, 6, 275–290. [CrossRef]\\n\\n63. Naseem, U.; Razzak, I.; Khan, S.K.; Prasad, M. A comprehensive survey on word representation models: From classical to state-of-the-art word representation language models. Trans. Asian Low-Resour. Lang. Inf. Process. 2021, 20, 1–35. [CrossRef] 64. Adil, M.; Wu, J.Z.; Chakrabortty, R.K.; Alahmadi, A.; Ansari, M.F.; Ryan, M.J. Attention-based STL-BiLSTM network to forecast tourist arrival. Processes 2021, 9, 1759. [CrossRef]\\n\\n65. Min, S.; Park, S.; Kim, S.; Choi, H.S.; Lee, B.; Yoon, S. Pre-training of deep bidirectional protein sequence representations with\\n\\nstructural information. IEEE Access 2021, 9, 123912–123926. [CrossRef] Jain, A.; Zamir, A.R.; Savarese, S.; Saxena, A. Structural-rnn: Deep Learning on Spatio-Temporal Graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June 2016; pp. 5308–5317. 67. Pascanu, R.; Gulcehre, C.; Cho, K.; Bengio, Y. How to construct deep recurrent neural networks. arXiv 2013, arXiv:1312.6026. 68.\\n\\n66.\\n\\nShi, H.; Xu, M.; Li, R. Deep learning for household load forecasting—A novel pooling deep RNN. IEEE Trans. Smart Grid 2017, 9, 5271–5280. [CrossRef]\\n\\n69. Gal, Y.; Ghahramani, Z. A theoretically grounded application of dropout in recurrent neural networks. Adv. Neural Inf. Process. Syst. 2016, 29.\\n\\n70. Moradi, R.; Berangi, R.; Minaei, B. A survey of regularization strategies for deep models. Artif. Intell. Rev. 2020, 53, 3947–3986. [CrossRef] Salehin, I.; Kang, D.K. A review on dropout regularization approaches for deep neural networks within the scholarly domain. Electronics 2023, 12, 3106. [CrossRef]\\n\\n70. Moradi, R.; Berangi, R.; Minaei, B. A survey of regularization strategies for deep models. Artif. Intell. Rev. 2020, 53, 3947–3986. [CrossRef] Salehin, I.; Kang, D.K. A review on dropout regularization approaches for deep neural networks within the scholarly domain. Electronics 2023, 12, 3106. [CrossRef]\\n\\n72. Cai, S.; Shu, Y.; Chen, G.; Ooi, B.C.; Wang, W.; Zhang, M. Effective and efficient dropout for deep convolutional neural networks. arXiv 2019, arXiv:1904.03392.\\n\\n29 of 34\\n\\nInformation 2024, 15, 517\\n\\n73. Garbin, C.; Zhu, X.; Marques, O. Dropout vs. batch normalization: An empirical study of their impact to deep learning. Multimed. Tools Appl. 2020, 79, 12777–12815. [CrossRef]\\n\\n74. Borawar, L.; Kaur, R. ResNet: Solving Vanishing Gradient in Deep Networks. In Proceedings of the International Conference on Recent Trends in Computing: ICRTC 2022, Delhi, India, 3–4 June 2022; Springer: Berlin/Heidelberg, Germany, 2023; pp. 235–247. IEEE Access 2023,\\n\\n75. Mienye, I.D.; Sun, Y. A deep learning ensemble with data resampling for credit card fraud detection. 11, 30628–30638. [CrossRef]\\n\\n76. Kiperwasser, E.; Goldberg, Y. Simple and accurate dependency parsing using bidirectional LSTM feature representations. Trans. Assoc. Comput. Linguist. 2016, 4, 313–327. [CrossRef]\\n\\n77. Zhang, W.; Li, H.; Tang, L.; Gu, X.; Wang, L.; Wang, L. Displacement prediction of Jiuxianping landslide using gated recurrent unit (GRU) networks. Acta Geotech. 2022, 17, 1367–1382. [CrossRef]\\n\\n78. Cahuantzi, R.; Chen, X.; Güttel, S. A Comparison of LSTM and GRU Networks for Learning Symbolic Sequences. In Proceedings of the Science and Information Conference, Nanchang, China, 2–4 June 2023; Springer: Berlin/Heidelberg, Germany, 2023; pp. 771–785. Shewalkar, A.; Nyavanandi, D.; Ludwig, S.A. Performance evaluation of deep neural networks applied to speech recognition: RNN, LSTM and GRU. J. Artif. Intell. Soft Comput. Res. 2019, 9, 235–245. [CrossRef]\\n\\n79.\\n\\n80. Vatanchi, S.M.; Etemadfard, H.; Maghrebi, M.F.; Shad, R. A comparative study on forecasting of long-term daily streamflow using ANN, ANFIS, BiLSTM and CNN-GRU-LSTM. Water Resour. Manag. 2023, 37, 4769–4785. [CrossRef]\\n\\n81. Mateus, B.C.; Mendes, M.; Farinha, J.T.; Assis, R.; Cardoso, A.M. Comparing LSTM and GRU models to predict the condition of a pulp paper press. Energies 2021, 14, 6958. [CrossRef]\\n\\n82. Gers, F.A.; Schmidhuber, J. Recurrent Nets That Time and Count. In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks, IJCNN 2000, Neural Computing: New Challenges and Perspectives for the New Millennium, Como, Italy, 24–27 July 2000; Volume 3, pp. 189–194.\\n\\n83. Gers, F.A.; Schraudolph, N.N.; Schmidhuber, J. Learning precise timing with LSTM recurrent networks. J. Mach. Learn. Res. 2002,\\n\\n84. 85.\\n\\n86.\\n\\n3, 115–143. Jaeger, H. Adaptive nonlinear system identification with echo state networks. Adv. Neural Inf. Process. Syst. 2002, 15, 593–600. Ishaq, M.; Kwon, S. A CNN-Assisted deep echo state network using multiple Time-Scale dynamic learning reservoirs for generating Short-Term solar energy forecasting. Sustain. Energy Technol. Assessments 2022, 52, 102275. Sun, C.; Song, M.; Cai, D.; Zhang, B.; Hong, S.; Li, H. A systematic review of echo state networks from design to application. IEEE Trans. Artif. Intell. 2022, 5, 23–37. [CrossRef]\\n\\n87. Gallicchio, C.; Micheli, A. Deep echo state network (deepesn): A brief survey. arXiv 2017, arXiv:1712.04323. 88. Gallicchio, C.; Micheli, A. Richness of Deep Echo State Network Dynamics. In Proceedings of the Advances in Computational Intelligence: 15th International Work-Conference on Artificial Neural Networks, IWANN 2019, Gran Canaria, Spain, 12–14 June 2019, Proceedings, Part I 15; Springer: Berlin/Heidelberg, Germany, 2019; pp. 480–491.\\n\\n89. Hu, R.; Tang, Z.R.; Song, X.; Luo, J.; Wu, E.Q.; Chang, S. Ensemble echo network with deep architecture for time-series modeling. Neural Comput. Appl. 2021, 33, 4997–5010. [CrossRef]\\n\\n90. Gao, R.; Li, R.; Hu, M.; Suganthan, P.N.; Yuen, K.F. Dynamic ensemble deep echo state network for significant wave height forecasting. Appl. Energy 2023, 329, 120261. [CrossRef]\\n\\n91. Gao, R.; Du, L.; Duru, O.; Yuen, K.F. Time series forecasting based on echo state network and empirical wavelet transformation. Appl. Soft Comput. 2021, 102, 107111. [CrossRef]\\n\\n92. Li, S.; Li, W.; Cook, C.; Zhu, C.; Gao, Y. Independently Recurrent Neural Network (indrnn): Building a Longer and Deeper rnn. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18–23 June 2018; pp. 5457–5466.\\n\\n93. Yang, J.; Qu, J.; Mi, Q.; Li, Q. A CNN-LSTM model for tailings dam risk prediction. IEEE Access 2020, 8, 206491–206502. [CrossRef] 94. Ren, P.; Xiao, Y.; Chang, X.; Huang, P.Y.; Li, Z.; Chen, X.; Wang, X. A comprehensive survey of neural architecture search: Challenges and solutions. ACM Comput. Surv. (CSUR) 2021, 54, 1–34. [CrossRef]\\n\\n95. Mellor, J.; Turner, J.; Storkey, A.; Crowley, E.J. Neural Architecture Search without Training. In Proceedings of the International Conference on Machine Learning, Virtual, 18–24 July 2021; pp. 7588–7598.\\n\\n96. Zoph, B.; Le, Q.V. Neural architecture search with reinforcement learning. arXiv 2016, arXiv:1611.01578. 97. Chen, X.; Wu, S.Z.; Hong, M. Understanding gradient clipping in private sgd: A geometric perspective. Adv. Neural Inf. Process. Syst. 2020, 33, 13773–13782.\\n\\n98. Zhang, Z. Improved Adam Optimizer for Deep Neural Networks. In Proceedings of the 2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS), Banff, AB, Canada, 4–6 June 2018; pp. 1–2.\\n\\n99. De Santana Correia, A.; Colombini, E.L. Attention, please! A survey of neural attention models in deep learning. Artif. Intell. Rev. 2022, 55, 6037–6124. [CrossRef]\\n\\n100. Lin, J.; Ma, J.; Zhu, J.; Cui, Y. Short-term load forecasting based on LSTM networks considering attention mechanism. Int. J. Electr. Power Energy Syst. 2022, 137, 107818. [CrossRef]\\n\\n101. Chaudhari, S.; Mithal, V.; Polatkan, G.; Ramanath, R. An attentive survey of attention models. ACM Trans. Intell. Syst. Technol. (TIST) 2021, 12, 1–32. [CrossRef]\\n\\n102. Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv 2014, arXiv:1409.0473.\\n\\n30 of 34\\n\\nInformation 2024, 15, 517\\n\\n103. Luong, M.T.; Pham, H.; Manning, C.D. Effective approaches to attention-based neural machine translation. arXiv:1508.04025.\\n\\n104. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, Ł.; Polosukhin, I. Attention is all you need. Adv. Neural Inf. Process. Syst. 2017, 30.\\n\\n105. Marcus, M.P.; Marcinkiewicz, M.A.; Santorini, B. Building a large annotated corpus of English: The Penn Treebank. Comput. Linguist. 1993, 19, 313–330.\\n\\n106. Maas, A.L.; Daly, R.E.; Pham, P.T.; Huang, D.; Ng, A.Y.; Potts, C. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Portland, OR, USA, 19–24 June 2011; pp. 142–150.\\n\\n107. LeCun, Y.; Bottou, L.; Bengio, Y.; Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 1998, 86, 2278–2324. [CrossRef]\\n\\n108. Garofolo, J.S.; Lamel, L.F.; Fisher, W.M.; Fiscus, J.G.; Pallett, D.S. TIMIT acoustic-phonetic continuous speech corpus. Linguist. Data Consort. 1993, 93, 27403.\\n\\n109. Lewis, D. Reuters-21578 Text Categorization Test Collection; Distribution 1.0; AT&T Labs-Research: Atlanta, GA, USA, 1997. 110. Dua, D.; Graff, C. UCI Machine Learning Repository; School of Information and Computer Science, University of California: Irvine, CA, USA, 2017.\\n\\n111. Lomonaco, V.; Maltoni, D. Core50: A New Dataset and Benchmark for Continuous Object Recognition. In Proceedings of the Conference on Robot Learning. PMLR, Mountain View, CA, USA, 13–15 November 2017; pp. 17–26.\\n\\n112. Souri, A.; El Maazouzi, Z.; Al Achhab, M.; El Mohajir, B.E. Arabic Text Generation using Recurrent Neural Networks. In Proceedings of the Big Data, Cloud and Applications: Third International Conference, BDCA 2018, Kenitra, Morocco, 4–5 April 2018; Revised Selected Papers 3; Springer: Berlin/Heidelberg, Germany, 2018; pp. 523–533.\\n\\n113. Islam, M.S.; Mousumi, S.S.S.; Abujar, S.; Hossain, S.A. Sequence-to-sequence Bangla sentence generation with LSTM recurrent neural networks. Procedia Comput. Sci. 2019, 152, 51–58. [CrossRef]\\n\\n114. Gajendran, S.; Manjula, D.; Sugumaran, V. Character level and word level embedding with bidirectional LSTM–Dynamic recurrent neural network for biomedical named entity recognition from literature. J. Biomed. Inform. 2020, 112, 103609. [CrossRef] 115. Hu, H.; Liao, M.; Mao, W.; Liu, W.; Zhang, C.; Jing, Y. Variational Auto-Encoder for Text Generation. In Proceedings of the 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC), Chongqing, China, 12–14 June 2020; pp. 595–598.\\n\\n116. Holtzman, A.; Buys, J.; Du, L.; Forbes, M.; Choi, Y. The curious case of neural text degeneration. arXiv 2019, arXiv:1904.09751. 117. Yin, W.; Schütze, H. Attentive convolution: Equipping cnns with rnn-style attention mechanisms. Trans. Assoc. Comput. Linguist. 2018, 6, 687–702. [CrossRef]\\n\\n118. Hussein, M.A.H.; Sava¸s, S. LSTM-Based Text Generation: A Study on Historical Datasets. arXiv 2024, arXiv:2403.07087. 119. Baskaran, S.; Alagarsamy, S.; S, S.; Shivam, S. Text Generation using Long Short-Term Memory. In Proceedings of the 2024 Third International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS), Krishnankoil, India, 14–16 March 2024; pp. 1–6. [CrossRef]\\n\\n120. Keskar, N.S.; McCann, B.; Varshney, L.R.; Xiong, C.; Socher, R. Ctrl: A conditional transformer language model for controllable generation. arXiv 2019, arXiv:1909.05858.\\n\\n121. Guo, H. Generating text with deep reinforcement learning. arXiv 2015, arXiv:1510.09202. 122. Yadav, V.; Verma, P.; Katiyar, V. Long short term memory (LSTM) model for sentiment analysis in social data for e-commerce products reviews in Hindi languages. Int. J. Inf. Technol. 2023, 15, 759–772. [CrossRef]\\n\\n123. Abimbola, B.; de La Cal Marin, E.; Tan, Q. Enhancing Legal Sentiment Analysis: A Convolutional Neural Network–Long Short-Term Memory Document-Level Model. Mach. Learn. Knowl. Extr. 2024, 6, 877–897. [CrossRef]\\n\\n124. Zulqarnain, M.; Ghazali, R.; Aamir, M.; Hassim, Y.M.M. An efficient two-state GRU based on feature attention mechanism for sentiment analysis. Multimed. Tools Appl. 2024, 83, 3085–3110. [CrossRef]\\n\\n125. Pujari, P.; Padalia, A.; Shah, T.; Devadkar, K. Hybrid CNN and RNN for Twitter Sentiment Analysis. International Conference on Smart Computing and Communication; Springer: Berlin/Heidelberg, Germany, 2024; pp. 297–310. 126. Wankhade, M.; Annavarapu, C.S.R.; Abraham, A. CBMAFM: CNN-BiLSTM multi-attention fusion mechanism for sentiment\\n\\nclassification. Multimed. Tools Appl. 2024, 83, 51755–51786. [CrossRef]\\n\\n127. Sangeetha, J.; Kumaran, U. A hybrid optimization algorithm using BiLSTM structure for sentiment analysis. Meas. Sensors 2023, 25, 100619. [CrossRef]\\n\\n128. He, R.; McAuley, J. Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering. In Proceedings of the 25th International Conference on World Wide Web, Montreal, QC, Canada, 11–15 April 2016; pp. 507–517.\\n\\n129. Samir, A.; Elkaffas, S.M.; Madbouly, M.M. Twitter Sentiment Analysis using BERT. In Proceedings of the 2021 31st International Conference on Computer Theory and Applications (ICCTA), Kochi, Kerala, India, 17–19 August 2021; pp. 182–186.\\n\\n130. Prottasha, N.J.; Sami, A.A.; Kowsher, M.; Murad, S.A.; Bairagi, A.K.; Masud, M.; Baz, M. Transfer learning for sentiment analysis using BERT based supervised fine-tuning. Sensors 2022, 22, 4157. [CrossRef]\\n\\n131. Mujahid, M.; Rustam, F.; Shafique, R.; Chunduri, V.; Villar, M.G.; Ballester, J.B.; Diez, I.d.l.T.; Ashraf, I. Analyzing sentiments regarding ChatGPT using novel BERT: A machine learning approach. Information 2023, 14, 474. [CrossRef]\\n\\n31 of 34\\n\\nInformation 2024, 15, 517\\n\\n132. Wu, Y.; Schuster, M.; Chen, Z.; Le, Q.V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.; et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv 2016, arXiv:1609.08144.\\n\\n133. Sennrich, R.; Haddow, B.; Birch, A. Neural machine translation of rare words with subword units. arXiv 2015, arXiv:1508.07909. 134. Kang, L.; He, S.; Wang, M.; Long, F.; Su, J. Bilingual attention based neural machine translation. Appl. Intell. 2023, 53, 4302–4315. [CrossRef]\\n\\n135. Yang, Z.; Dai, Z.; Salakhutdinov, R.; Cohen, W.W. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv 2017, arXiv:1711.03953.\\n\\n136. Song, K.; Tan, X.; Qin, T.; Lu, J.; Liu, T.Y. Mass: Masked sequence to sequence pre-training for language generation. arXiv 2019, arXiv:1905.02450.\\n\\n137. Hinton, G.; Deng, L.; Yu, D.; Dahl, G.E.; Mohamed, A.r.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T.N.; et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Process. Mag. 2012, 29, 82–97. [CrossRef]\\n\\n138. Hannun, A.; Case, C.; Casper, J.; Catanzaro, B.; Diamos, G.; Elsen, E.; Prenger, R.; Satheesh, S.; Sengupta, S.; Coates, A.; et al. Deep speech: Scaling up end-to-end speech recognition. arXiv 2014, arXiv:1412.5567.\\n\\n139. Amodei, D.; Ananthanarayanan, S.; Anubhai, R.; Bai, J.; Battenberg, E.; Case, C.; Casper, J.; Catanzaro, B.; Cheng, Q.; Chen, G.; et al. Deep Speech 2: End-to-End Speech Recognition in English and Mandarin. In Proceedings of the International Conference on Machine Learning, New York, NY, USA, 20–22 June 2016; pp. 173–182.\\n\\n140. Chiu, C.C.; Sainath, T.N.; Wu, Y.; Prabhavalkar, R.; Nguyen, P.; Chen, Z.; Kannan, A.; Weiss, R.J.; Rao, K.; Gonina, E.; et al. State- of-the-Art Speech Recognition with Sequence-to-Sequence Models. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Calgary, Canada, 15–20 April 2018; pp. 4774–4778.\\n\\n141. Zhang, Y.; Chan, W.; Jaitly, N. Very Deep Convolutional Networks for End-to-End Speech Recognition. In Proceedings of the 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), New Orleans, LA, USA, 5–9 March 2017; pp. 4845–4849.\\n\\n142. Dong, L.; Xu, S.; Xu, B. Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Calgary, AB, Canada, 15–20 April 2018; pp. 5884–5888.\\n\\n143. Bhaskar, S.; Thasleema, T. LSTM model for visual speech recognition through facial expressions. Multimed. Tools Appl. 2023, 82, 5455–5472. [CrossRef]\\n\\n144. Daouad, M.; Allah, F.A.; Dadi, E.W. An automatic speech recognition system for isolated Amazigh word using 1D & 2D CNN-LSTM architecture. Int. J. Speech Technol. 2023, 26, 775–787.\\n\\n145. Dhanjal, A.S.; Singh, W. A comprehensive survey on automatic speech recognition using neural networks. Multimed. Tools Appl. 2024, 83, 23367–23412. [CrossRef]\\n\\n146. Nasr, S.; Duwairi, R.; Quwaider, M. End-to-end speech recognition for arabic dialects. Arab. J. Sci. Eng. 2023, 48, 10617–10633. [CrossRef]\\n\\n147. Kumar, D.; Aziz, S. Performance Evaluation of Recurrent Neural Networks-LSTM and GRU for Automatic Speech Recognition. In Proceedings of the 2023 International Conference on Computer, Electronics & Electrical Engineering & Their Applications (IC2E3), Srinagar Garhwal, India, 8–9 June 2023; pp. 1–6.\\n\\n148. Fischer, T.; Krauss, C. Deep learning with long short-term memory networks for financial market predictions. Eur. J. Oper. Res. 2018, 270, 654–669. [CrossRef]\\n\\n149. Nelson, D.M.; Pereira, A.C.; De Oliveira, R.A. Stock Market’s Price Movement Prediction with LSTM Neural Networks. In Proceedings of the 2017 International Joint Conference on Neural Networks (IJCNN), Anchorage, AK, USA, 14–19 May 2017; pp. 1419–1426.\\n\\n150. Luo, A.; Zhong, L.; Wang, J.; Wang, Y.; Li, S.; Tai, W. Short-term stock correlation forecasting based on CNN-BiLSTM enhanced by attention mechanism. IEEE Access 2024, 12, 29617–29632. [CrossRef]\\n\\n151. Bao, W.; Yue, J.; Rao, Y. A deep learning framework for financial time series using stacked autoencoders and long-short term memory. PLoS ONE 2017, 12, e0180944. [CrossRef] [PubMed]\\n\\n152. Feng, F.; Chen, H.; He, X.; Ding, J.; Sun, M.; Chua, T.S. Enhancing Stock Movement Prediction with Adversarial Training. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19), Macao, China, 10–16 August 2019; Volume 19, pp. 5843–5849.\\n\\n153. Rundo, F. Deep LSTM with reinforcement learning layer for financial trend prediction in FX high frequency trading systems. Appl. Sci. 2019, 9, 4460. [CrossRef]\\n\\n154. Devi, T.; Deepa, N.; Gayathri, N.; Rakesh Kumar, S. AI-Based Weather Forecasting System for Smart Agriculture System Using a Recurrent Neural Networks (RNN) Algorithm. Sustain. Manag. Electron. Waste 2024, 97–112.\\n\\n155. Anshuka, A.; Chandra, R.; Buzacott, A.J.; Sanderson, D.; van Ogtrop, F.F. Spatio temporal hydrological extreme forecasting framework using LSTM deep learning model. Stoch. Environ. Res. Risk Assess. 2022, 36, 3467–3485. [CrossRef]\\n\\n156. Marulanda, G.; Cifuentes, J.; Bello, A.; Reneses, J. A hybrid model based on LSTM neural networks with attention mechanism for short-term wind power forecasting. Wind. Eng. 2023, 0309524X231191163. [CrossRef]\\n\\n157. Chen, W.; An, N.; Jiang, M.; Jia, L. An improved deep temporal convolutional network for new energy stock index prediction. Inf. Sci. 2024, 682, 121244. [CrossRef]\\n\\n32 of 34\\n\\nInformation 2024, 15, 517\\n\\n158. Hasanat, S.M.; Younis, R.; Alahmari, S.; Ejaz, M.T.; Haris, M.; Yousaf, H.; Watara, S.; Ullah, K.; Ullah, Z. Enhancing Load Forecasting Accuracy in Smart Grids: A Novel Parallel Multichannel Network Approach Using 1D CNN and Bi-LSTM Models. Int. J. Energy Res. 2024, 2024, 2403847. [CrossRef]\\n\\n159. Asiri, M.M.; Aldehim, G.; Alotaibi, F.; Alnfiai, M.M.; Assiri, M.; Mahmud, A. Short-term load forecasting in smart grids using hybrid deep learning. IEEE Access 2024, 12, 23504–23513. [CrossRef]\\n\\n160. Yıldız Do ˘gan, G.; Aksoy, A.; Öztürk, N. A Hybrid Deep Learning Model to Estimate the Future Electricity Demand of Sustainable Cities. Sustainability 2024, 16, 6503. [CrossRef]\\n\\n161. Bhambu, A.; Gao, R.; Suganthan, P.N. Recurrent ensemble random vector functional link neural network for financial time series forecasting. Appl. Soft Comput. 2024, 161, 111759. [CrossRef]\\n\\n162. Mienye, E.; Jere, N.; Obaido, G.; Mienye, I.D.; Aruleba, K. Deep Learning in Finance: A Survey of Applications and Techniques. Preprints 2024. [CrossRef]\\n\\n163. Mastoi, Q.U.A.; Wah, T.Y.; Gopal Raj, R. Reservoir computing based echo state networks for ventricular heart beat classification. Appl. Sci. 2019, 9, 702. [CrossRef]\\n\\n164. Valin, J.M.; Tenneti, S.; Helwani, K.; Isik, U.; Krishnaswamy, A. Low-Complexity, Real-Time Joint Neural Echo Control and Speech Enhancement Based on Percepnet. In Proceedings of the ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Toronto, ON, Canada, 6–11 June 2021; pp. 7133–7137.\\n\\n165. Li, Y.; Huang, C.; Ding, L.; Li, Z.; Pan, Y.; Gao, X. Deep learning in bioinformatics: Introduction, application, and perspective in the big data era. Methods 2019, 166, 4–21. [CrossRef]\\n\\n166. Zhang, Y.; Qiao, S.; Ji, S.; Li, Y. DeepSite: Bidirectional LSTM and CNN models for predicting DNA–protein binding. Int. J. Mach. Learn. Cybern. 2020, 11, 841–851. [CrossRef]\\n\\n167. Xu, J.; Mcpartlon, M.; Li, J. Improved protein structure prediction by deep learning irrespective of co-evolution information. Nat. Mach. Intell. 2021, 3, 601–609. [CrossRef]\\n\\n168. Yadav, S.; Ekbal, A.; Saha, S.; Kumar, A.; Bhattacharyya, P. Feature assisted stacked attentive shortest dependency path based Bi-LSTM model for protein–protein interaction. Knowl.-Based Syst. 2019, 166, 18–29. [CrossRef]\\n\\n169. Aybey, E.; Gümü¸s, Ö. SENSDeep: An ensemble deep learning method for protein–protein interaction sites prediction. Interdiscip. Sci. Comput. Life Sci. 2023, 15, 55–87. [CrossRef] [PubMed]\\n\\n170. Li, Z.; Du, X.; Cao, Y. DAT-RNN: Trajectory Prediction with Diverse Attention. In Proceedings of the 2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA), Miami, FL, USA, 14–17 December 2020; pp. 1512–1518.\\n\\n171. Lee, M.j.; Ha, Y.g. Autonomous Driving Control Using End-to-End Deep Learning. In Proceedings of the 2020 IEEE International Conference on Big Data and Smart Computing (BigComp), Busan, Republic of Korea, 19–22 February 2020; pp. 470–473. [CrossRef]\\n\\n172. Codevilla, F.; Müller, M.; López, A.; Koltun, V.; Dosovitskiy, A. End-to-End Driving via Conditional Imitation Learning. In Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA), Brisbane, Australia, 21–25 May 2018; pp. 4693–4700.\\n\\n173. Altché, F.; de La Fortelle, A. An LSTM Network for Highway Trajectory Prediction.\\n\\nIn Proceedings of the 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC), Abu Dhabi, United Arab Emirates, 25–28 October 2017; pp. 353–359.\\n\\n174. Li, P.; Zhang, Y.; Yuan, L.; Xiao, H.; Lin, B.; Xu, X. Efficient long-short temporal attention network for unsupervised video object segmentation. Pattern Recognit. 2024, 146, 110078. [CrossRef]\\n\\n175. Li, R.; Shu, X.; Li, C. Driving Behavior Prediction Based on Combined Neural Network Model. IEEE Trans. Comput. Soc. Syst. 2024, 11, 4488–4496. [CrossRef]\\n\\n176. Liu, Y.; Diao, S. An automatic driving trajectory planning approach in complex traffic scenarios based on integrated driver style inference and deep reinforcement learning. PLoS ONE 2024, 19, e0297192. [CrossRef]\\n\\n177. Altindal, M.C.; Nivlet, P.; Tabib, M.; Rasheed, A.; Kristiansen, T.G.; Khosravanian, R. Anomaly detection in multivariate time series of drilling data. Geoenergy Sci. Eng. 2024, 237, 212778. [CrossRef]\\n\\n178. Matar, M.; Xia, T.; Huguenard, K.; Huston, D.; Wshah, S. Multi-Head Attention Based bi-lstm for Anomaly Detection in Multivariate Time-Series of wsn. In Proceedings of the 2023 IEEE 5th International Conference on Artificial Intelligence Circuits and Systems (AICAS), Hangzhou, China, 11–13 June 2023; pp. 1–5.\\n\\n179. Kumaresan, S.J.; Senthilkumar, C.; Kongkham, D.; Beenarani, B.; Nirmala, P. Investigating the Effectiveness of Recurrent Neural Networks for Network Anomaly Detection. In Proceedings of the 2024 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE), Bangalore, India, 24–25 January 2024; pp. 1–5.\\n\\n180. Li, E.; Bedi, S.; Melek, W. Anomaly detection in three-axis CNC machines using LSTM networks and transfer learning. Int. J. Adv. Manuf. Technol. 2023, 127, 5185–5198. [CrossRef]\\n\\n181. Minic, A.; Jovanovic, L.; Bacanin, N.; Stoean, C.; Zivkovic, M.; Spalevic, P.; Petrovic, A.; Dobrojevic, M.; Stoean, R. Applying recurrent neural networks for anomaly detection in electrocardiogram sensor data. Sensors 2023, 23, 9878. [CrossRef]\\n\\n182. Zhou, C.; Paffenroth, R.C. Anomaly Detection with Robust Deep Autoencoders. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, 13–17 August 2017; pp. 665–674.\\n\\n33 of 34\\n\\nInformation 2024, 15, 517\\n\\n183. Ren, H.; Xu, B.; Wang, Y.; Yi, C.; Huang, C.; Kou, X.; Xing, T.; Yang, M.; Tong, J.; Zhang, Q. Time-Series Anomaly Detection Service at Microsoft. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, Anchorage, AK, USA, 4–8 August 2019; pp. 3009–3017.\\n\\n184. Munir, M.; Siddiqui, S.A.; Dengel, A.; Ahmed, S. DeepAnT: A deep learning approach for unsupervised anomaly detection in time series. IEEE Access 2018, 7, 1991–2005. [CrossRef]\\n\\n185. Hewamalage, H.; Bergmeir, C.; Bandara, K. Recurrent neural networks for time series forecasting: Current status and future directions. Int. J. Forecast. 2021, 37, 388–427. [CrossRef]\\n\\n186. Ahmed, S.F.; Alam, M.S.B.; Hassan, M.; Rozbu, M.R.; Ishtiak, T.; Rafa, N.; Mofijur, M.; Shawkat Ali, A.; Gandomi, A.H. Deep learning modelling techniques: Current progress, applications, advantages, and challenges. Artif. Intell. Rev. 2023, 56, 13521–13617. [CrossRef]\\n\\n187. Li, X.; Qin, T.; Yang, J.; Liu, T.Y. LightRNN: Memory and computation-efficient recurrent neural networks. Adv. Neural Inf. Process. Syst. 2016, 29.\\n\\n188. Katharopoulos, A.; Vyas, A.; Pappas, N.; Fleuret, F. Transformers Are rnns: Fast Autoregressive Transformers with Linear Attention. In Proceedings of the International Conference on Machine Learning, Virtual, 12–18 July 2020; pp. 5156–5165. 189. Shao, W.; Li, B.; Yu, W.; Xu, J.; Wang, H. When Is It Likely to Fail? Performance Monitor for Black-Box Trajectory Prediction Model. IEEE Trans. Autom. Sci. Eng. 2024, 4, 765–772. [CrossRef]\\n\\n190. Jacobs, W.R.; Kadirkamanathan, V.; Anderson, S.R. Interpretable deep learning for nonlinear system identification using frequency response functions with ensemble uncertainty quantification. IEEE Access 2024, 12, 11052–11065. [CrossRef]\\n\\n191. Mamalakis, M.; Mamalakis, A.; Agartz, I.; Mørch-Johnsen, L.E.; Murray, G.; Suckling, J.; Lio, P. Solving the enigma: Deriving optimal explanations of deep networks. arXiv 2024, arXiv:2405.10008.\\n\\n192. Shah, M.; Sureja, N. A Comprehensive Review of Bias in Deep Learning Models: Methods, Impacts, and Future Directions. Arch. Comput. Methods Eng. 2024, 1–13. [CrossRef]\\n\\n193. Goethals, S.; Calders, T.; Martens, D. Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics. arXiv 2024, arXiv:2401.13391.\\n\\n194. Weerts, H.; Pfisterer, F.; Feurer, M.; Eggensperger, K.; Bergman, E.; Awad, N.; Vanschoren, J.; Pechenizkiy, M.; Bischl, B.; Hutter, F. Can fairness be automated? Guidelines and opportunities for fairness-aware AutoML. J. Artif. Intell. Res. 2024, 79, 639–677. [CrossRef]\\n\\n195. Bai, Y.; Geng, X.; Mangalam, K.; Bar, A.; Yuille, A.L.; Darrell, T.; Malik, J.; Efros, A.A. Sequential Modeling Enables Scalable Learning for Large Vision Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle WA, USA, 17–21 June 2024; pp. 22861–22872.\\n\\n196. Taye, M.M. Understanding of machine learning with deep learning: Architectures, workflow, applications and future directions. Computers 2023, 12, 91. [CrossRef]\\n\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.\\n\\n34 of 34'),\n",
       " Document(metadata={'source': 'research_ppr/GRU.pdf'}, page_content=\"Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks Rahul Dey and Fathi M. Salem Circuits, Systems, and Neural Networks (CSANN) LAB Department of Electrical and Computer Engineering Michigan State University East Lansing, MI 48824-1226, USA deyrahul@msu.edu || salemf@msu.edu\\n\\nAbstract – The paper evaluates three variants of the Gated Recurrent Unit (GRU) in recurrent neural networks (RNN) by reducing parameters in the update and reset gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and show that these GRU-RNN variant models perform as well as the the original GRU RNN model while reducing computational expense.\\n\\nperformance sentiment classification from a given review paragraph.\\n\\nII. BACKGROUND: RNN, LSTM AND GRU\\n\\nIn principal, RNN are more suitable relationships among sequential data types. The so-called simple RNN has a recurrent hidden state as in\\n\\nfor capturing\\n\\nI. INTRODUCTION\\n\\nGated Recurrent Neural Network (RNN) have shown success in several applications involving sequential or temporal data [1-13]. For example, they have been applied extensively in speech recognition, natural language processing, machine translation, etc. [2, 5]. Long Short-Term Memory (LSTM) RNN and the recently introduced Gated Recurrent Unit (GRU) RNN have been successfully shown to perform well with long sequence applications [2-5, 8-12].\\n\\nTheir success is primarily due to the gating network signals that control how the present input and previous memory are used to update the current activation and produce the current state. These gates have their own sets of weights that are adaptively updated in the learning phase (i.e., the training and evaluation process). While these models empower successful in learning parameterization through their gate networks. Consequently, there is an added computational expense vis-à-vis the simple RNN model [2, 5, 6]. It is noted that the LSTM RNN employs 3 distinct gate networks while the GRU RNN reduce the gate networks to two. In [14], it is proposed to reduce the external gates to the minimum of one with preliminary evaluation of sustained performance.\\n\\nin RNN,\\n\\nthey\\n\\nintroduce\\n\\nan\\n\\nincrease\\n\\nIn this paper, we focus on the GRU RNN and explore three new gate-variants with reduced parameterization. We comparatively evaluate the performance of the original and the variant GRU RNN on two public datasets. Using the MNIST dataset, one generates two sequences [2, 5, 6, 14]. One sequence is obtained from each 28x28 image sample as pixel- wise long sequence of length 28x28=784 (basically, scanning from the upper left to the bottom right of the image). Also, one generates a row-wise short sequence of length 28, with each element being a vector of dimension 28 [14, 15]. The third sequence type employs the IMDB movie review dataset where one defines the length of the sequence in order to achieve high\\n\\nℎ(cid:2) = (cid:4)(cid:5)(cid:6)(cid:7)(cid:2) + (cid:9)ℎ(cid:2)−1 + (cid:12)(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)1(cid:13)\\n\\nwhere (cid:7)(cid:15) is the (external) m-dimensional input vector at time (cid:2) , ℎ(cid:15) the n-dimensional hidden state, g is the (point-wise) activation function, such as the logistic function, the hyperbolic tangent function, or the rectified Linear Unit (ReLU) [2, 6], and (cid:6), (cid:9)\\t(cid:17)(cid:18)(cid:19)\\t(cid:12) are the appropriately sized parameters (two weights and bias). Specifically, in this case, (cid:6) is an (cid:18) × (cid:21) matrix, (cid:9) is an (cid:18) × (cid:18) matrix, and (cid:12) is an (cid:18) × 1 matrix (or vector).\\n\\nBengio at al. [1] showed that it is difficult to capture long-term dependencies using such simple RNN because the (stochastic) gradients tend to either vanish or explode with long sequences. Two particular models, the Long Short-Term Memory (LSTM) unit RNN [3, 4] and Gated Recurrent Unit (GRU) RNN [2] have been proposed to solve the “vanishing” or “exploding” gradient problems. We will present these two models in sufficient details for our purposes below.\\n\\nA. Long Short-Term Memory (LSTM) RNN The LSTM RNN architecture uses the computation of the simple RNN of Eqn (1) as an intermediate candidate for the internal memory cell (state), say (cid:22)̃(cid:15), and add it in a (element- wise) weighted-sum to the previous value of the internal memory state, say (cid:22)(cid:15)(cid:24)(cid:25), to produce the current value of the memory cell (state) (cid:22)(cid:15). This is expressed succinctly in the following discrete dynamic equations:\\n\\n(cid:22)(cid:15) = (cid:27)(cid:15)⨀(cid:22)(cid:15)(cid:24)(cid:25) + (cid:29)(cid:15)\\t⨀\\t(cid:22)̃(cid:15)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)2(cid:13) (cid:22)̃(cid:15) = (cid:4)(cid:5)(cid:6)(cid:31)(cid:7)(cid:15) + (cid:9)(cid:31)ℎ(cid:15)(cid:24)(cid:25) + (cid:12)(cid:31)(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)3(cid:13) ℎ(cid:15) = !(cid:15)⨀g(cid:5)(cid:22)(cid:15)(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)4(cid:13)\\n\\nIn Eqns (3) and (4), the activation nonlinearity (cid:4) is typically the hyperbolic tangent function but more recently may be implemented as a rectified Linear Unit (reLU). The weighted\\n\\nsum is implemented in Eqn (2) via element-wise (Hadamard) multiplication denoted by ⨀ to gating signals. The gating (control) signals (cid:29)(cid:15), (cid:27)(cid:15)\\t(cid:17)(cid:18)(cid:19)\\t!(cid:15) denote, respectively, the input, forget, and output gating signals at time (cid:2) . These control gating signals are in fact replica of the basic equation (3), with their own parameters and replacing (cid:4) by the logistic function. The logistic function limits the gating signals to within 0 and 1. The specific mathematical form of the gating signals are thus expressed as the vector equations:\\n\\n(cid:29)(cid:15) = $(cid:5)(cid:6)%(cid:7)(cid:15) + (cid:9)%ℎ(cid:15)(cid:24)(cid:25) + (cid:12)%(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t (cid:27)(cid:15) = $&(cid:6)'(cid:7)(cid:15) + (cid:9)'ℎ(cid:15)(cid:24)(cid:25) + (cid:12)'(\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t !(cid:15) = $(cid:5)(cid:6))(cid:7)(cid:15) + (cid:9))ℎ(cid:15)(cid:24)(cid:25) + (cid:12))(cid:13)\\n\\nwhere $ is the logistic nonlinearity and the parameters for each gate consist of two matrices and a bias vector. Thus, the total number of parameters (represented as matrices and bias vectors) for the 3 gates and the memory cell structure are, respectively, (cid:6)%, (cid:9)%, (cid:12)%, (cid:6)', (cid:9)', (cid:12)', (cid:6)), (cid:9)), (cid:12)), (cid:6)(cid:31), (cid:9)(cid:31)\\t(cid:17)(cid:18)(cid:19)\\t(cid:12)(cid:31). These parameters are all updated at each training step and stored. It is immediately noted that the number of parameters in the LSTM model is increased 4-folds from the simple RNN model in Eqn (1). Assume that the cell state is n-dimensional. (Note that the activation and all the gates have the same dimensions). Assume also that the input signal is m- dimensional. Then, the total parameters in the LSTM RNN are equal to 4×(n2 + nm +n).\\n\\nB. Gated Recurrent Unit (GRU) RNN The GRU RNN reduce the gating signals to two from the LSTM RNN model. The two gates are called an update gate *(cid:15) and a reset gate +(cid:15). The GRU RNN model is presented in the form:\\n\\nℎ(cid:15) = (cid:5)1 − *(cid:15)(cid:13)⨀ℎ(cid:15)(cid:24)(cid:25) + *(cid:15)⨀ℎ,(cid:15)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)5(cid:13) ℎ,(cid:15) = (cid:4)(cid:5)(cid:6).(cid:7)(cid:15) + (cid:9).(cid:5)+(cid:15)⨀ℎ(cid:15)(cid:24)(cid:25)(cid:13) + (cid:12).(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)6(cid:13)\\n\\nwith the two gates presented as:\\n\\n(cid:15) = $(cid:5)(cid:6)0(cid:7)(cid:15) + (cid:9)0ℎ(cid:15)(cid:24)(cid:25) + (cid:12)0(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)7(cid:13)\\t +(cid:15) = $(cid:5)(cid:6)2(cid:7)(cid:15) + (cid:9)2ℎ(cid:15)(cid:24)(cid:25) + (cid:12)2(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)8(cid:13)\\n\\nOne observes that the GRU RNN [Eqns (5)-(6)] is similar to the LSTM RNN [Eqns (2)-(3)], however with less external gating signal in the interpolation Eqn (5). This saves one gating signal and the associated parameters. We defer further information to reference [2], and the references therein. In essence, the GRU RNN has 3-folds increase in parameters in comparison to the simple RNN of Eqn (1). Specifically, the total number of parameters in the GRU RNN equals 3×(n2 + nm +n).\\n\\nIn various studies, e.g., in [2] and the references therein, it has been noted that GRU RNN is comparable to, or even outperforms, the LSTM in most cases. Moreover, there are other reduced gated RNNs, e.g. the Minimal Gated Unit (MGU) RNN, where only one gate equation is used and it is reported that this (MGU) RNN performance is comparable to the LSTM RNN and the GRU RNN, see [14] for details.\\n\\nIn this paper, we focus on the GRU RNN model and evaluate new variants. Specifically, we retain the architecture of Eqns (5)-(6) unchanged, and focus on variation in the structure of the gating signals in Eqns (7) and (8). We apply the variations identically to the two gates for uniformity and simplicity.\\n\\nIII. THE VARIANT GRU ARCHITECTURES\\n\\nThe gating mechanism in the GRU (and LSTM) RNN is a replica of the simple RNN in terms of parametrization. The weights corresponding to these gates are also updated using the backpropagation through time (BTT) stochastic gradient descent as it seeks to minimize a loss/cost function [3, 4]. Thus, each parameter update will information pertaining to the state of the overall network. Thus, all information regarding the current input and the previous hidden states are reflected in the latest state variable. There is a redundancy in the signals driving the gating signals. The key driving signal should be the internal state of the network. Moreover, involve components of the internal state of the system [16, 17]. In this study, we consider three distinct variants of the gating equations applied uniformly to both gates:\\n\\ninvolve\\n\\nthe adaptive parameter updates all\\n\\nVariant 1: called GRU1, where each gate is computed using only the previous hidden state and the bias.\\n\\n(cid:15) = $(cid:5)(cid:9)0ℎ(cid:15)(cid:24)(cid:25) + (cid:12)0(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)9 − (cid:17)(cid:13) +(cid:15) = $(cid:5)(cid:9)2ℎ(cid:15)(cid:24)(cid:25) + (cid:12)2(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)9 − (cid:12)(cid:13)\\n\\nThus, the total number of parameters is now reduced in comparison to the GRU RNN by 2× nm.\\n\\nVariant 2: called GRU2, where each gate is computed using only the previous hidden state.\\n\\n(cid:15) = $(cid:5)(cid:9)0ℎ(cid:15)(cid:24)(cid:25)(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)10 − (cid:17)(cid:13) \\t\\t+(cid:15) = $(cid:5)(cid:9)2ℎ(cid:15)(cid:24)(cid:25)(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)10 − (cid:12)(cid:13)\\n\\nThus, the total number of parameters is reduced in comparison to the GRU RNN by 2× (nm+n).\\n\\nVariant 3: called GRU3, where each gate is computed using only the bias.\\n\\n(cid:15) = $(cid:5)(cid:12)0(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)11 − (cid:17)(cid:13) \\t\\t\\t\\t+(cid:15) = $(cid:5)(cid:12)2(cid:13)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)11 − (cid:12)(cid:13)\\n\\nThus the total number of parameters is reduced in comparison to the GRU RNN by 2× (nm+n2).\\n\\nWe have performed an empirical study of the performance of each of these variants as compared to the GRU RNN on, first, sequences generated from the MNIST dataset and then on the IMDB movie review dataset. In the subsequent figures and tables, we refer to the base GRU RNN model as GRU0 and the three variants as GRU1, GRU2, and GRU3 respectively.\\n\\nOur architecture consists of a single layer of one of the variants of GRU units driven by the input sequence and the activation function (cid:4) set as ReLU. (Initial experiments using\\n\\n(cid:4) = (cid:2)(cid:17)(cid:18)ℎ\\t have produced similar results). For the MNIST dataset, we generate the row-wise sequences as in [15]. The networks have been generated in Python using the Keras library [15] with Theano as a backend library. As Keras has a GRU layer class, we modified this class to classes for GRU1, GRU2, and GRU3. All of these classes used the ReLU activation function. The RNN layer of units is followed by a softmax layer in the case of the MNIST dataset or a traditional logistic activation layer in the case of the IMDB dataset to predict the output category. The Root Mean Square Propagation (RMSprop) is used as the choice of optimizer that is known to adapt the learning rate for each of the parameters. To speed up training, we also decay the learning rate exponentially with the cost in each epoch\\n\\nthe pixel-wise and\\n\\n6 = 678(cid:31))9(cid:15)\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(cid:5)12(cid:13)\\n\\nwhere 67 represents a base constant learning rate and (cid:22)!:(cid:2) is the cost computed in the previous epoch. The details of our models are delineated in Table I.\\n\\nTable I: Network model characteristics\\n\\nModel\\n\\nHidden Units Gate Activation Activation Cost\\n\\nEpochs Optimizer Dropout Batch Size\\n\\nMNIST Pixel-wise 100 Sigmoid ReLU Categorical Cross-entropy 100 RMSProp 20% 32\\n\\nMNIST Row-wise 100 Sigmoid ReLU Categorical Cross-entropy 50 RMSProp 20% 32\\n\\nIMDB\\n\\n128 sigmoid ReLU Binary Cross- entropy 100 RMSProp 20% 32\\n\\nIV. RESULTS AND DISCUSSION\\n\\nA. Application to MNIST Dataset – pixel-wise sequences\\n\\nThe MNIST dataset [15] consists of 60000 training images and 10000 test images, each of size 28x28 of handwritten digits. We evaluated our three variants against the original GRU model on the MNIST dataset by generating the sequential input in one case (pixel-wise, one pixel at a time) and in the second case (row-wise, one row at a time). The pixel-wise sequence generated from each image are 1-element signal of length 784, while the 28-element row-wise produces a sequence of length 28. For each case, we performed different iterations by varying the constant base learning rate 67. The results of our experiments are depicted in Fig. 1, 2, and 3, with summary in Table II below.\\n\\nTable II: MNIST pixel-wise sequences: performance summary of different architectures using 4 constant base learning rates 67, in 100 epochs.\\n\\nArchitecture GRU0 GRU1 GRU2 GRU3\\n\\nLr = 1e-3 Train Test 99.19 98.59 98.59 98.04 98.88 98.37 98.52 98.90 98.10 98.61 10.44 -\\n\\nLr = 5e-4 Train Test\\n\\n- -\\n\\n\\n\\n1e-4 5e-5 Train Test 30600 - 30400 - - 30200 60.97 59.60 10400\\n\\n# Params\\n\\n- -\\n\\nFig. 1 Training (left) and Testing (right) Accuracy of GRU0, GRU1 and GRU2 on MNIST pixel-wise generated sequences at eta=0.001\\n\\nFig. 2 Training Accuracy of GRU0, GRU1, GRU2 and GRU3 on MNIST generated sequences at eta=5e-4\\n\\nFig. 3 Performance of GRU3 on MNIST generated sequences for 3 constant base learning rates 67.\\n\\nFrom Table II and Fig. 1 and 2, GRU1 and GRU2 perform almost as well as GRU0 on MNIST pixel-wise generated sequence inputs. While GRU3 does not perform as well for this (constant base) learning rate. Figure 3 shows that reducing the (constant base) learning rate to (0.0001) and below has enabled GRU3 to increase its (test) accuracy performance to 59.6% after 100 epochs, and with a positive slope indicating that it would increase further after more epochs. Note that in\\n\\nthis experiment, GRU3 has about 33% of the number of (adaptively computed) parameters compared to GRU0. Thus, there exists a potential trade-off between the higher accuracy performance and the decrease in the number of parameters. In our experiments, using 100 epochs, the GRU3 architecture never attains saturation. Further experiments using more epochs and/or more units would shed more light on the comparative evaluation of this trade-off between performance and parameter-reduction.\\n\\nB. Application to MNIST Dataset – row-wise sequences\\n\\nWhile pixel-wise sequences represent relatively long sequences, row-wise generated sequences can test short sequences (of length 28) with vector elements. The accuracy profile performance vs. epochs of the MNIST dataset with row-wise input of all four GRU RNN variants are depicted in Fig. 4, Fig. 5, and Fig. 6, using several constant base learning rates. Accuracy performance results are then summarized in Table III below.\\n\\nTable III: MNIST row-wise generated sequences: Accuracy (%) performance of different variants using several constant base learning rates over 50 epochs\\n\\nArchitecture GRU0 GRU1 GRU2 GRU3\\n\\nLr = 1e-2 Train Test 96.99 98.49 98.14 98.85 93.02 96.66 38700 97.24 98.55 97.46 98.93 91.54 96.58 33100 96.95 98.71 97.33 98.93 91.20 96.23 32900 97.19 98.85 97.04 97.39 80.33 87.96 13100\\n\\nLr = 1e-3 Train Test\\n\\nLr = 1e-4 Train Test\\n\\n# Params\\n\\nFig. 4 Training and testing accuracy on MNIST row-wise generated sequences at a constant base learning rate of 1e-2\\n\\nFig. 5 Training and testing accuracy on MNIST row-wise generated sequences at a constant base learning rate of 1e-3\\n\\nFig. 6 Training and testing accuracy on MNIST row-wise generated sequences at a constant base learning rate of 1e-4\\n\\nFrom Table III and Fig. 4, Fig.5, and Fig. 6, all the four variants GRU0, GRU1, GRU2, and GRU3 appear to exhibit comparable accuracy performance over three constant base learning rates. GRU3 exhibits lower performance at the base learning rate of 1e-4 where, after 50 epochs, is still lagging. From Fig. 6, however, it appears that the profile has not yet levelled off and has a positive slope. More epochs are likely to increase performance to comparable levels with the other variants. It is noted that in this experiment, GRU3 can achieve comparable performance with roughly one third of the number of (adaptively computed) parameters. Computational expense savings may play a role in favoring one variant over the others in targeted applications and/or available resources.\\n\\nC. Application to the IMDB Dataset– natural sequence\\n\\nThe IMDB dataset is composed of 25000 test data and 25000 training data consisting of movie reviews and their binary sentiment classification. Each review is represented by a maximum of 80 (most frequently occurring) words in a vocabulary of 20000 words [7]. We have trained the dataset on all 4 GRU variants using the two constant base learning rates of 1e-3 and 1e-4 over 100 epochs. In the training, we employ 128-dimensional GRU RNN variants and have adopted a batch size of 32. We have observed that, using the constant base learning rate of 1e-3, performance fluctuates visibly (see Fig. 7), whereas performance is uniformly progressing over profile-curves as shown in Fig. 8. Table IV summarizes the results of accuracy performance which show comparable performance among GRU0, GRU1, GRU2, and GRU3. Table IV also lists the number of parameters in each.\\n\\nFig. 7 Test and validation accuracy on IMDB dataset using a base learning rate of 1e-3\\n\\nFig. 8 Training and testing accuracy on IMDB dataset using a base learning rate of 1e-4\\n\\nTable IV: IMDB dataset: Accuracy (%) performance of different architectures using two base learning rates over 100 epochs\\n\\nArchitecture GRU0 GRU1 GRU2 GRU3\\n\\nTrain 95.3 94.5 94.5 92.3\\n\\nLr = 1e-3 Test 83.7 84.1 84.2 83.2\\n\\nTrain 87.4 87.0 86.9 86.8\\n\\nLr = 1e-4 Test 84.8 84.8 84.6 84.5\\n\\n# Params\\n\\n98688 65920 65664 33152\\n\\nThe IMDB data experiments provide the most striking results. It can be clearly seen that all the 3 GRU variants perform comparably to the GRU RNN while using less number of parameters. The learning pace of GRU3 was also similar to those of the other variants at the constant base learning rate of 1e-4. From Table IV, it is noted that more saving in computational load is achieved by all variant GRU RNN as the input is represented as a large 128-dimensional vector.\\n\\nV. CONCLUSION\\n\\nThe experiments on the variants GRU1, GRU2, and GRU3 verse the GRU RNN have demonstrated that their accuracy performance is comparable on three example sequence lengths. Two sequences generated from the MNIST dataset and one from the IMDB dataset. The main driving signal of the gates appear to be the (recurrent) state as it contains essential information about other signals. Moreover, the use of the stochastic gradient descent implicitly carries information about the network state. This may explain the relative success in using the bias alone in the gate signals as its adaptive update carries information about the state of the network. The GRU variants reduce their performance has been comparable to the original GRU RNN. While GRU1 and GRU2 have indistinguishable performance from the GRU RNN, GRU3 frequently lags in performance, especially for relatively long sequences and may require more execution time to achieve comparable performance,\\n\\nthis redundancy and\\n\\nthus\\n\\nWe remark that the goal of this work to comparatively evaluate the performance of GRU1, GRU2 and GRU3, which possess less gate parameters, and thus less computational expense, than the original GRU RNN. By performing more experimental evaluations using constant or varying learning rates, and training for longer number of epochs, one can validate the performance on broader domain. We remark that\\n\\nthe three GRU RNN variants need to be further comparatively evaluated on diverse datasets for a broader empirical performance evidence.\\n\\nREFERENCES [1] Bengio, Y., Simard, P., and Frasconi, P. Learning Longterm Dependencies with Gradient Descent is Difficult. IEEE Trans.Neural Networks, 5(2):157–166, 1994. H. Simpson, Dumb Robots, 3rd ed., Springfield: UOS Press, 2004, pp.6-9.\\n\\n[2] Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. arXiv preprint arXiv:1412.3555, 2014. Gers, F. A., Schraudolph, N. N., and Schmidhuber, J. Learning Precise Timing with LSTM Recurrent Networks. Journal of Machine Learning Research, 3:115–143, 2002. J.- G. Lu, “Title of paper with only the first word capitalized,” J. Name Stand. Abbrev., in press.\\n\\n[3] Hochreiter, S. and Schmidhuber, J. Long Short-Term Memory. Neural\\n\\n[4]\\n\\nComputation, 9(8):1735–1780, 1997 Jozefowicz, R., Zaremba, W., and Sutskever, I. An Empirical Exploration of Recurrent Network Architectures. In Proc., Int’l Conf. on Machine Learning, pp. 2342–2350, 2015.\\n\\n[5] Le, Q. V., Jaitly, N., and Hinton, G. E. A Simple Way to Initialize Recurrent Networks of Rectified Linear Units. arXiv preprint arXiv:1504.00941, 2015.\\n\\n[6] Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning Word Vectors for Sentiment Analysis. In Proc. 49th Annual Meeting of the ACL, pp. 142–150, 2011.\\n\\n[7] Mikolov, T., Joulin, A., Chopra, S., Mathieu, M., and Ranzato, M. Learning Longer Memory in Recurrent Neural Networks. In Int’l Conf Learning Represenations, 2015.\\n\\n[8] Zaremba, W., Sutskever, I., and Vinyals, O. Recurrent Neural Network\\n\\nRegularization. arXiv preprint arXiv:1409.2329, 2014.\\n\\n[9] Boulanger-Lewandowski, Nicolas, Bengio, Yoshua, and Vincent, Pascal. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. arXiv preprint arXiv:1206.6392, 2012.\\n\\n[10] Gers, Felix A, Schmidhuber, J¨urgen, and Cummins, Fred. Learning to lstm. Neural computation,\\n\\nforget: Continual prediction with 12(10):2451–2471, 2000.\\n\\n[11] Mikolov, Tomas, Joulin, Armand, Chopra, Sumit, Mathieu, Michael, and Ranzato, Marc’Aurelio. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753, 2014.\\n\\n[12] Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the training recurrent neural networks. arXiv preprint\\n\\ndifficulty of arXiv:1211.5063, 2012.\\n\\n[13] Zhou G. B., Wu J., Zhang C. L., and Zhou Z. H. Minimal Gated Unit for Recurrent Neural Networks. arXiv preprint arXiv:1603.09420v1 [cs.NE] 31 Mar 2016\\n\\n[14] https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py. [15] F. M. Salem, `“Reduced Parameterization in Gated Recurrent Neural\\n\\nNetworks,” Memorandum 7.11.2016, MSU, Nov 2016.\\n\\n[16] F. M. Salem, “A Basic Recurrent Neural Network Model,” arXiv Preprint\\n\\narXiv: 1612.09022, Dec. 2016.\"),\n",
       " Document(metadata={'source': 'research_ppr/A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Review Article\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nFarhad Mortezapour Shiria Raihani Mohameda\\n\\n, Thinagaran Perumala\\n\\n, Norwati Mustaphaa, and\\n\\naFaculty of Computer Science and Information Technology, University Putra Malaysia (UPM), Serdang, 43400, Malaysia\\n\\nKEYWORDS\\n\\nABSTRACT\\n\\nDeep Learning\\n\\nConvolutional Neural Network (CNN)\\n\\nLong Short-Term Memory (LSTM)\\n\\nGated Recurrent Unit (GRU)\\n\\nTemporal Convolutional Network (TCN)\\n\\nTransformer\\n\\nKolmogorov-Arnold networks (KAN)\\n\\nDeep Reinforcement Learning (DRL)\\n\\nDeep Transfer Learning (DTL)\\n\\nAutoencoder\\n\\nGenerative Adversarial Network (GAN)\\n\\nDeep Belief Network (DBN)\\n\\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Temporal Convolutional Networks (TCN), Transformer, Kolmogorov-Arnold networks (KAN), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available datasets: IMDB, ARAS, and Fruit-360. We compared the performance of six renowned deep learning models: CNN, RNN, Long Short-Term Memory (LSTM), Bidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU alongside two newer models, TCN and Transformer, using the IMDB and ARAS datasets. Additionally, we evaluated the performance of eight CNN-based models, including VGG (Visual Geometry Group), Inception, ResNet (Residual Network), InceptionResNet, Xception (Extreme Inception), MobileNet, DenseNet (Dense Convolutional Network), and NASNet (Neural Architecture Search Network), for image classification tasks using the Fruit-360 dataset.\\n\\n1 Introduction\\n\\nArtificial intelligence (AI) aims to emulate human-level intelligence in machines. In computer science, AI refers to the study of \"intelligent agents,\" which are objects capable of perceiving their environment and taking actions to maximize their chances of achieving specific goals [1]. Machine learning (ML) is a field that focuses on the development and application of methods capable of learning from datasets [2]. ML finds extensive use in various domains, such as speech recognition,\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\ncomputer vision, text analysis, video games, medical sciences, and cybersecurity.\\n\\nIn recent years, deep learning (DL) techniques, a subset of machine learning (ML), have outperformed traditional ML approaches across numerous tasks, driven by several critical advancements [3]. The proliferation of large datasets has been pivotal in enabling models to learn intricate patterns and relationships, thereby significantly enhancing their performance [4]. Concurrently, advancements in hardware acceleration technologies, notably Graphics Processing Units (GPUs) and Field-Programmable Gate Arrays (FPGAs) [5] have markedly reduced model training times by facilitating rapid computations and parallel processing capabilities. These advancements have substantially accelerated the training process.2 Moreover, enhancements in algorithmic techniques for optimization and training have further augmented the speed and efficiency of deep learning models, leading to quicker convergence and superior generalization capabilities [4]. Deep learning techniques have demonstrated remarkable success across a wide range of applications, including computer vision (CV), natural language processing (NLP), and speech recognition. These applications underscore the transformative impact of DL in various domains, where it continues to set new performance benchmarks [6, 7].\\n\\nDeep learning models draw inspiration from the structure and functionality of the human nervous system and brain. These models employ input, hidden, and output layers to organize processing units. Within each layer, the nodes or units are interconnected with those in the layer below, and each connection is assigned to a weight value. The units sum the inputs after multiplying them by their corresponding weights [8]. Fig. 1 illustrates the relationship between AI, ML, and DL, highlighting that machine learning and deep learning are subfields of artificial intelligence.\\n\\nThe objective of this research is to provide a comprehensive overview of various deep learning models and compare their performance across different applications. In Section 2, we introduce a fundamental definition of deep learning. Section 3 covers supervised deep learning models, including Multi-Layer Perceptron (MLP), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Temporal Convolutional Networks (TCN), and Kolmogorov-Arnold Networks (KAN). Section 4 reviews generative models such as Autoencoders, Generative Adversarial Networks (GANs), and Deep Belief Networks (DBNs). Section 5 presents a comprehensive survey of Transformer architecture. Deep Reinforcement Learning (DRL) is discussed in Section 6, while Section 7 addresses Deep Transfer Learning (DTL). The principles of hybrid deep learning are explored in Section 8, followed by a discussion of deep learning applications in Section 9. Section 10 surveys the challenges in deep learning and potential alternative solutions. In Section 11, we conduct experiments and analyze the performance of different deep learning models using three datasets. Research directions and future aspects are covered in Section 12. Finally, Section 13 concludes the paper.\\n\\nArtificial\\n\\nMachine\\n\\nDeep\\n\\nIntelligent\\n\\nLearning\\n\\nLearning\\n\\nFigure 1. Relationship between artificial intelligence (AI), machine learning (ML), and deep\\n\\nlearning (DL).\\n\\n2\\n\\nF. M. Shiri et al.\\n\\n2 Deep Learning\\n\\nDeep learning (DL) involves the process of learning hierarchical representations of data by utilizing architectures with multiple hidden layers. With the advancement of high-performance computing facilities, deep learning techniques using deep neural networks have gained increasing popularity [9]. In a deep learning algorithm, data is passed through multiple layers, with each layer progressively extracting features and transmitting information to the subsequent layer. The initial layers extract low-level characteristics, which are then combined by later layers to form a comprehensive representation [6].\\n\\nIn traditional machine learning techniques, the classification task typically involves a sequential process that includes pre-processing, feature extraction, meticulous feature selection, learning, and classification. The effectiveness of machine learning methods heavily relies on accurate feature selection, as biased feature selection can lead to incorrect class classification. In contrast, deep learning models enable simultaneous learning and classification, eliminating the need for separate steps. This capability makes deep learning particularly advantageous for automating feature learning across diverse tasks [10]. Fig. 2 visually illustrates the distinction between deep learning and traditional machine learning in terms of feature extraction and learning. In the era of deep learning, a wide array of methods and architectures have been developed. These models can be broadly categorized into two main groups: discriminative (supervised) and generative (unsupervised) approaches. Among the discriminative models, two prominent groups are Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). Additionally, generative approaches encompass various models such as Generative Adversarial Networks (GANs) and Auto-Encoders (AEs) [11]. In the following sections, we provide a comprehensive survey of different types of deep learning models.\\n\\nFigure 2. Visual illustration of the distinction between deep learning and traditional machine\\n\\nlearning in terms of feature extraction and learning [10].\\n\\n3 Supervised Deep Learning Models\\n\\nIn supervised learning and classification tasks, this family of deep learning algorithms is used to perform discriminative functions. These supervised deep architectures typically model the posterior distributions of classes based on observable data, enabling effective pattern classification. Common supervised models include Multi-Layer Perceptron (MLP), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Temporal Convolutional Networks (TCN), Kolmogorov-Arnold Networks (KAN), and their variations. A brief overview of these methods follows.\\n\\n3.1 Multi Layers Perceptron (MLP)\\n\\nThe Multi-Layer Perceptron (MLP) model is a type of feedforward artificial neural network\\n\\n3\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n(ANN) that serves as a foundation architecture for deep learning or deep neural networks (DNNs) [11]. It operates as a supervised learning approach. The MLP consists of three layers: the input layer, the output layer, and one or more hidden layers [12]. It is a fully connected network, meaning each neuron in one layer is connected to all neurons in the subsequent layer.\\n\\nIn an MLP, the input layer receives the input data and performs feature normalization. The hidden layers, which can vary in number, process the input signals. The output layer makes decisions or predictions based on the processed information [13]. Fig. 3 (a) depicts a single-neuron perceptron model, where the activation function φ (Eq. (1)) is a non-linear function used to map the summation function (𝑥𝑤 + 𝑏) to the output value 𝑦. 𝑦 = 𝜑(𝑥𝑤 + 𝑏) (1) In Eq. (1), the terms 𝑥, 𝑤, 𝑏, and 𝑦 represent the input vector, weighting vector, bias, and output value, respectively [14]. Fig. 3 (b) illustrates the structure of the multilayer perceptron (MLP) model.\\n\\n(a)\\n\\n(b)\\n\\nFigure 3. (a) Single-neuron perceptron model. (b) Structure of the MLP [14].\\n\\n3.2 Convolutional Neural Networks (CNN)\\n\\nConvolutional Neural Networks (CNNs) are a powerful class of deep learning models widely applied in various tasks, including object detection, speech recognition, computer vision, image classification, and bioinformatics [15]. They have also demonstrated success in time series prediction tasks [16]. CNNs are feedforward neural networks that leverage convolutional structures to extract features from data [17]. CNN has a two-stage architecture that combines a classifier and a feature extractor to provide automatic feature extraction and end-to-end training with the least amount of pre-processing necessary [18]. Unlike traditional methods, CNNs automatically learn and recognize features from the data without the need for manual feature extraction by humans [19]. The design of CNNs is inspired by visual perception [17]. The major components of CNNs include the convolutional layer, pooling layer, fully connected layer, and activation function [20, 21]. Fig. 4 presents the pipeline of the convolutional neural network, highlighting how each layer contributes to the efficient processing and successful progression of input data through the network.\\n\\nconvolution layer\\n\\nPooling layer\\n\\nFully connected layer\\n\\nClass 1\\n\\nInput\\n\\nData\\n\\nClass N\\n\\nFigure 4. The pipeline of a Convolutional Neural Network.\\n\\n4\\n\\nF. M. Shiri et al.\\n\\nFigure 5. Schematic diagram of the convolution process [22].\\n\\nConvolutional Layer: The convolutional layer is a pivotal component of CNN. Through multiple convolutional layers, the convolution operation extracts distinct features from the input. In image classification, lower layers tend to capture basic features such as texture, lines, and edges, while higher layers extract more abstract features. The convolutional layer comprises learnable convolution kernels, which are weight matrices typically of equal length, width, and an odd number (e.g., 3x3, 5x5, or 7x7). These kernels are convolved with the input feature maps, sliding over the regions of the feature map and executing convolution operations [22]. Fig. 5 illustrates the schematic diagram of the convolution process.\\n\\nPooling Layer: Typically following the convolutional layer, the pooling layer reduces the number of connections in the network by performing down-sampling and dimensionality reduction on the input data [23]. Its primary purpose is to alleviate the computational burden and address overfitting issues [24]. Moreover, the pooling layer enables CNN to recognize objects even when their shapes are distorted or viewed from different angles, by incorporating various dimensions of an image through pooling [25]. The pooling operation produces output feature maps that are more robust against distortion and errors in individual neurons [26]. There are various pooling methods, including Max Pooling, Average Pooling, Spatial Pyramid Pooling, Mixed Pooling, Multi-Scale Order-Less, and Stochastic Pooling [27-30]. Fig. 6 depicts an example of Max Pooling, where a window slides across the input, and the contents of the window are processed by a pooling function [31].\\n\\nFigure 6. Computing the output values of a 3 × 3 max pooling operation on a 5 × 5 input.\\n\\n5\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nX1\\n\\nw1\\n\\nX2\\n\\nw2\\n\\nOutput\\n\\n. . .\\n\\nWN\\n\\n𝒛 = σ 𝒘𝒊 + 𝒙𝒊 𝒊\\n\\n+ 𝒃 f (z)\\n\\nXN\\n\\nFigure 7. The general structure of activation functions.\\n\\nFully Connected (FC) Layer: The FC layer is typically located at the end of a CNN architecture. In this layer, every neuron is connected to all neurons in the preceding layer, adhering to the principles of a conventional multi-layer perceptron neural network. The FC layer receives input from the last pooling or convolutional layer, which is a vector created by flattening the feature maps. The FC layer serves as the classifier in the CNN, enabling the network to make predictions [10].\\n\\nActivation Functions: Activation functions are fundamental components in convolutional neural networks (CNNs), indispensable for introducing non-linearity into the network. This non- linearity is crucial for CNN’s ability to model complex patterns and relationships within the data, allowing it to perform tasks beyond simple linear classification or regression. Without non-linear activation functions, a CNN would be limited to linear operations, significantly constraining its capacity to accurately represent the intricate, non-linear behaviors typical of many real-world phenomena [32].\\n\\nFig. 7 typically illustrates how these activation functions modulate input signals to produce output, highlighting the non-linear transformations applied to the input data across different regions of the function curve. In this figure, 𝑥𝑖 represents the input feature, while 𝑤𝑖𝑗 denotes the weight associated with the connection between the input feature 𝑥𝑖 and neuron 𝑗. The figure shows that neuron 𝑗 receives 𝑛 features simultaneously. The output from neuron 𝑗 is labeled by 𝑦𝑗, and its internal state, or bias, is indicated by 𝑏𝑗. The activation function, depicted as 𝑓(. ), could be any one of several types such as the Rectified Linear Unit (ReLU), hyperbolic tangent (Tanh), Sigmoid function, or others [33, 34].\\n\\nThese various activation functions are shown in Fig. 8, with emphasis on their distinct characteristics and profiles. These activation functions are essential for convolutional neural networks (CNNs) to be more effective in a variety of applications by allowing them to recognize intricate patterns and provide accurate predictions. Sigmoid and Tanh functions are frequently referred to as saturating nonlinearities due to the way they act when inputs are very large or small. As per the reference, the Sigmoid function approaches values of 0 or 1, whereas the Tanh function leans towards -1 or 1[17]. Different alternative nonlinearities have been suggested for reducing problems associated with these saturating effects, including Rectified Linear Unit (ReLU) [35], Leaky ReLU [36], Parametric Rectified Linear Units (PReLU) [37], Randomized Leaky ReLU (RReLU) [38], S-shaped ReLU (SReLU) [39], and Exponential Linear Units (ELUs) [40], Gaussian Error Linear Units (GELUs) [41].\\n\\n6\\n\\nF. M. Shiri et al.\\n\\nSigmoid\\n\\nHyperbolic Tangent\\n\\nReLU\\n\\nLeaky ReLU\\n\\nELU\\n\\nGELU\\n\\nFigure 8. Diagram of different activation functions.\\n\\nReLU (Rectified Linear Unit) is one of the most often used activation functions in modern CNNs because of how well it solves the vanishing gradient issue during training. The definition of ReLU in mathematics is as Eq. (2), where the input to the neuron is represented by 𝑥 [34].\\n\\n𝑓(𝑥) = max(0, 𝑥) = {\\n\\n𝑥𝑖, 𝑖𝑓 𝑥𝑖 ≥ 0 0, 𝑖𝑓 𝑥𝑖 < 0\\n\\n(2)\\n\\nThis feature helps CNN learn complicated features more efficiently by effectively \"turning off\" any negative input values while maintaining positive values. It also keeps neurons from being saturated during training.\\n\\nAs an alternative, the definition of the Sigmoid function is represented by Eq. (3), where 𝑥\\n\\nstands for the input of the neuron.\\n\\n1 𝑒−𝑥 (3) Although the sigmoid distinctive S-shape and capacity to condense real numbers into a range between 0 and 1 make it useful for binary classification, its propensity to saturate can hinder training by causing the vanishing gradient problem in deep neural networks.\\n\\n𝑓(𝑥) =\\n\\nConvolutional Neural Networks (CNNs) are extensively used in various fields, including natural language processing, image segmentation, image analysis, video analysis, and more. Several CNN variations have been developed, such as AlexNet [42], VGG (Visual Geometry Group) [43], Inception [44, 45], ResNet (Residual Networks) [46, 47], WideResNet [48], FractalNet [49], SqueezeNet [50], InceptionResNet [51], Xception (Extreme Inception) [52], MobileNet [53, 54], DenseNet (Dense Convolutional Network) [55], SENet (Squeeze-and-Excitation Network) [56], Efficientnet [57, 58] among others. These variants are applied in different application areas based on their learning capabilities and performance.\\n\\n7\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n3.3 Recurrent Neural Networks (RNN)\\n\\nRecurrent Neural Networks (RNNs) are a class of deep learning models that possess internal memory, enabling them to capture sequential dependencies. Unlike traditional neural networks that treat inputs as independent entities, RNNs consider the temporal order of inputs, making them suitable for tasks involving sequential information [59]. By employing a loop, RNNs apply the same operation to each element in a series, with the current computation depending on both the current input and the previous computations [60].\\n\\nThe ability of RNNs to utilize contextual information is particularly valuable in tasks such as natural language processing, video classification, and speech recognition. For example, in language modeling, understanding the preceding words in a sentence is crucial for predicting the next word. RNNs excel at capturing such dependencies due to their recurrent nature[61-63].\\n\\nHowever, a limitation of simple RNN is their short-term memory, which restricts their ability to retain information over long sequences [64]. To overcome this, more advanced RNN variants have been developed, including Long Short-Term Memory (LSTM) [65], bidirectional LSTM [66], Gated Recurrent Unit (GRU) [67], bidirectional GRU [68], Bayesian RNN [69], and others.\\n\\nFigure 9. Simple RNN internal operation [70].\\n\\nFig. 9 depicts a simple recurrent neural network, where the internal memory (ℎ𝑡) is computed\\n\\nusing Eq. (4) [70]:\\n\\nℎ𝑡 = 𝑔(𝑊𝑥𝑡 + 𝑈ℎ𝑡 + 𝑏) (4) In this equation, 𝑔() represents the activation function (typically Tanh), 𝑈 and 𝑊 are adjustable weight matrices for the hidden state (ℎ), 𝑏 is the bias, and 𝑥 denotes the input vector. RNNs have proven to be powerful models for processing sequential data, leveraging their ability to capture dependencies over time. Various types of RNN models, such as LSTM, bidirectional LSTM, GRU, and bidirectional GRU, have been developed to address specific challenges in different applications.\\n\\n3.3.1 Long Short-Term Memory (LSTM)\\n\\nLong Short-Term Memory (LSTM) is an advanced variant of Recurrent Neural Networks (RNN) that addresses the issue of capturing long-term dependencies. LSTM was initially introduced by [65] in 1997 and further improved by [71] in 2013, gaining significant popularity in the deep learning community. Compared to standard RNN, LSTM models have proven to be more effective at retaining and utilizing information over longer sequences.\\n\\nIn an LSTM network, the current input at a specific time step and the output from the previous time step are fed into the LSTM unit, which then generates an output that is passed to the next time step. The final hidden layer of the last time step, sometimes along with all hidden layers, is commonly employed for classification purposes [72]. The overall architecture of an LSTM network is depicted in Fig. 10 (a). LSTM consists of three gates: input gate, forget gate, and output gate. Each gate performs a specific function in controlling the flow of information. The input gate decides how to update the internal state based on the current input and the previous internal state. The forget gate determines how much of the previous internal state should be forgotten. Finally, the output gate regulates the influence of the internal state on the system [60, 73].\\n\\n8\\n\\nF. M. Shiri et al.\\n\\n(a)\\n\\n(b)\\n\\nFigure 10. (a) The high-level architecture of LSTM. (b) The inner structure of LSTM unit [60].\\n\\nFig. 10 (b) illustrates the update mechanism within the inner structure of an LSTM. The update\\n\\nfor the LSTM unit is expressed by Eq. (5): ℎ(𝑡) = 𝑔𝑜\\n\\n(𝑡)𝑓ℎ(𝑠(𝑡)) (𝑡)𝑓𝑠(𝑤ℎ(𝑡−1)) + 𝑢𝑋(𝑡) + 𝑏 𝑠(𝑡−1) = 𝑔𝑓 (𝑡) = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (𝑤𝑖ℎ(𝑡−1) + 𝑢𝑖𝑋(𝑡) + 𝑏𝑖) (5) 𝑔𝑖 (𝑡) = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (𝑤𝑓ℎ(𝑡−1) + 𝑢𝑓𝑋(𝑡) + 𝑏𝑓) 𝑔𝑓 (𝑡) = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (𝑤𝑜ℎ(𝑡−1) + 𝑢𝑜𝑋(𝑡) + 𝑏𝑜) 𝑔𝑜 where 𝑓ℎ and 𝑓𝑠 represent the activation functions of the system state and internal state, typically utilizing the hyperbolic tangent function. The gating operation, denoted as g, is a feedforward neural network with a sigmoid activation function, ensuring output values within the range of [0, 1], which are interpreted as a set of weights. The subscripts 𝑖, 𝑜, and 𝑓 correspond to the input gate, output gate, and forget gate, respectively.\\n\\n(𝑡)𝑠(𝑡−1) + 𝑔𝑖\\n\\n{\\n\\nWhile standard LSTM has demonstrated promising performance in various tasks, it may struggle to comprehend input structures that are more complex than a sequential format. To address this limitation, a tree-structured LSTM network, known as S-LSTM, was proposed by [74]. S- LSTM consists of memory blocks comprising an input gate, two forget gates, a cell gate, and an output gate. While S-LSTM exhibits superior performance in challenging sequential modeling problems, it comes with higher computational complexity compared to standard LSTM [75].\\n\\n3.3.2 Bidirectional LSTM\\n\\nBidirectional Long Short-Term Memory (Bi-LSTM) is an extension of the LSTM architecture that addresses the limitation of standard LSTM models by considering both past and future context in sequence modeling tasks. While traditional LSTM models process input data only in the forward direction, Bi-LSTM overcomes this limitation by training the model in two directions: forward and backward [76].\\n\\nA Bi-LSTM consists of two parallel LSTM layers: one processes the input sequence in the forward direction, while the other processes it in the backward direction. The forward LSTM layer reads the input data from left to right, as indicated by the green arrow in Fig. 11. Simultaneously, the backward LSTM layer reads the input data from right to left, as represented by the red arrow [77]. This bidirectional processing enables the model to capture information from both past and future contexts, allowing for a more comprehensive understanding of temporal dependencies within the sequence.\\n\\n9\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nFigure 11. The architecture of a Bidirectional LSTM model [76].\\n\\nDuring the training phase, the forward and backward LSTM layers independently extract features and update their internal states based on the input sequence. The output of each LSTM layer at each time step is a prediction score. These prediction scores are then combined using a weighted sum to generate the final output result [77]. By incorporating information from both directions, Bi-LSTM models can capture a broader context and improve the model\\'s ability to model temporal dependencies in sequential data.\\n\\nBi-LSTM has been widely applied in various sequence modeling tasks such as natural language processing, speech recognition, and sentiment analysis. It has shown promising results in capturing complex patterns and dependencies in sequential data, making it a popular choice for tasks that require an understanding of both past and future context.\\n\\n3.3.3 Gated Recurrent Unit (GRU)\\n\\nThe Gated Recurrent Unit (GRU) is another variant of the RNN architecture that addresses the short-term memory issue and offers a simpler structure compared to LSTM [59]. GRU combines the input gate and forget gate of LSTM into a single update gate, resulting in a more streamlined design. Unlike LSTM, GRU does not include a separate cell state. A GRU unit consists of three main components: an update gate, a reset gate, and the current memory content. These gates enable the GRU to selectively update and utilize information from previous time steps, allowing it to capture long-term dependencies in sequences [78]. Fig. 12 illustrates the structure of a GRU unit [79].\\n\\nThe update gate (Eq. (6)) determines how much of the past information should be retained and combined with the current input at a specific time step. It is computed based on the concatenation of the previous hidden state ℎ𝑡−1 and the current input 𝑥𝑡, followed by a linear transformation and a sigmoid activation function. 𝑧𝑡 = 𝜎(𝑊𝑧[ℎ𝑡−1, 𝑥𝑡] + 𝑏𝑧) (6) The reset gate (Eq. (7)) decides how much of the past information should be forgotten. It is computed in a similar manner to the update gate using the concatenation of the previous hidden state and the current input. 𝑟𝑡 = 𝜎(𝑊𝑟[ℎ𝑡−1, 𝑥𝑡] + 𝑏𝑟) (7) The current memory content (Eq. (8)) is calculated based on the reset gate and the concatenation of the transformed previous hidden state and the current input. The result is passed through a hyperbolic tangent activation function to produce the candidate activation. ℎ̃𝑡 = 𝑡𝑎𝑛ℎ(𝑊ℎ[𝑟𝑡ℎ𝑡−1, 𝑥𝑡]) (8)\\n\\n10\\n\\nF. M. Shiri et al.\\n\\nFigure 12. The structure of a GRU unit [79].\\n\\nFinally, the final memory state ℎ𝑡 is determined by a combination of the previous hidden state and the candidate activation (Eq. (9)). The update gate determines the balance between the previous hidden state and the candidate activation. Additionally, an output gate 𝑜𝑡 can be introduced to control the information flow from the current memory content to the output (Eq. (10)). The output gate is computed using the current memory state ℎ𝑡 and is typically followed by an activation function, such as the sigmoid function. ℎ𝑡 = (1 − 𝑧𝑡)ℎ𝑡−1 + 𝑧𝑡ℎ̃𝑡 (9) 𝑜𝑡 = 𝜎𝑜(𝑊𝑜ℎ𝑡 + 𝑏𝑜) (10) where the weight matrix of the output layer is 𝑊𝑜 and the bias vector of the output layer is\\n\\n𝑏𝑜.\\n\\nGRU offers a simpler alternative to LSTM with fewer tensor operations, allowing for faster training. However, the choice between GRU and LSTM depends on the specific use case and problem at hand. Both architectures have their advantages and disadvantages, and their performance may vary depending on the nature of the task [59].\\n\\n3.3.4 Bidirectional GRU\\n\\nThe Bidirectional Gated Recurrent Unit (Bi-GRU) [80] improves upon the conventional GRU architecture through the integration of contexts from the past and future in sequential modeling tasks. In contrast to the conventional GRU, which exclusively processes input sequences forward, the Bi-GRU manages sequences in both forward and backward directions. In order to do this, two parallel GRU layers are used, one of which processes the input data forward and the other in reverse. Fig. 13 shows the Bi-GRU\\'s structural layout.\\n\\nFigure 13. The structure of a Bi-GRU model [81].\\n\\n11\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n3.4 Temporal Convolutional Networks (TCN)\\n\\nTemporal Convolutional Networks (TCN) represent a significant advancement in neural network architectures, specifically tailored for handling sequential data, particularly time series. Originating as an extension of the one-dimensional Convolutional Neural Network (CNN), TCN was first introduced by [82] in 2017 for the task of action segmentation in video data, and its application was further generalized to other types of sequential data by [83] in 2018. TCN retains the powerful feature extraction capabilities inherent to CNN while being highly efficient in processing and analyzing time series data.\\n\\nThe purpose of training TCN is to forecast the next 𝑙 values of the input time series. Assume that we have a sequence of inputs 𝑥0, 𝑥1, … . , 𝑥𝑙. We would like to predict, at each time step, some corresponding output 𝑦0, 𝑦1, … . , 𝑦𝑙, whose values are equal to the input shifted forward 𝑙 time steps. The primary limitation is that it can only use the inputs that have already been observed: 𝑥0, 𝑥1, … . , 𝑥𝑡, when forecasting the output 𝑦𝑡 for a given time step 𝑡 [84]. TCN is characterized by two fundamental properties: (1) The convolutions within the network are causal, ensuring that the output at any given time step depends solely on the current and past inputs, without any influence from future inputs. (2) Similar to Recurrent Neural Networks (RNNs), TCN can process sequences of arbitrary length and produce output sequences of identical length. The three primary components of a typical TCN are residual connections, dilated convolution, and causal convolution [83, 85, 86]. Fig. 14 illustrates the schematic architecture of a TCN model.\\n\\nFigure 14. Schematic diagram of the TCN model architecture [87].\\n\\nCausal Convolution:\\n\\nTCN architecture is built upon two foundational principles. To adhere to the first principle, the initial layer of a TCN is a one-dimensional fully convolutional network, wherein each hidden layer maintains the same length as the input layer, achieved through zero-padding. This padding ensures that each successive layer remains the same length as the preceding one. To satisfy the second principle, TCN employs causal convolutions. A causal convolution is a specialized one- dimensional convolutional network where only elements from time 𝑡 and earlier are convolved to produce the output at time 𝑡. Fig. 15 demonstrates the structure of a causal convolutional network.\\n\\nDilated Convolution:\\n\\nTCN aims to effectively capture long-range dependencies in sequential data. A simple causal convolution can only consider a history that scales linearly with the depth of the network. This limitation would necessitate the use of large filters or an exceptionally deep network structure, which could hinder performance, particularly for tasks requiring a longer history.\\n\\n12\\n\\nF. M. Shiri et al.\\n\\nFigure 15. The structure of the causal convolutional network [85].\\n\\nThe depth of the network could lead to issues such as vanishing gradients, ultimately degrading network performance or causing it to plateau. To address these challenges, TCN employs dilated convolutions [88], which exponentially expand the receptive field, allowing the network to process large time series efficiently without a proportional increase in computational complexity. The architecture of a dilated convolutional network is depicted in Fig. 16.\\n\\nBy inserting gaps between the weights of the convolutional kernel, dilated convolutions effectively increase the network\\'s receptive field while maintaining computational efficiency. The mathematical formulation of a dilated convolution is given by Eq. (11).\\n\\n𝑘−1\\n\\n𝐹(𝑠) = (𝑥 ∗𝑑 𝑓)(𝑠) = ∑ 𝑓(𝑖) ∙ 𝑥𝑠−𝑑∙𝑖 (11)\\n\\n𝑖=0\\n\\nwhere 𝑑 is the dilation rate, 𝑘 is the size of the filter, and 𝑠 − 𝑑 ∙ 𝑖 accounts for the direction of the past. Dilation is the same as adding a fixed step in between each pair of neighboring filter taps. When 𝑑 = 1, dilated convolution becomes a regular convolution. As 𝑑 increases, the output at the higher layers reflects a broader range of inputs, improving performance on long-range dependencies in time series.\\n\\nResidual Connections:\\n\\nTo construct a more expressive TCN model, it is essential to use small filter sizes and stack multiple layers. However, stacking dilated and causal convolutional layers increases the depth of the network, potentially leading to problems such as gradient decay or vanishing gradients during training. To mitigate these issues, TCN incorporates residual connections into the output layer. Residual connections facilitate the flow of data across layers by adding a shortcut path, allowing the network to learn residual functions, which are modifications to the identity mapping, rather than learning a full transformation. This approach has been shown to be highly effective in very deep networks.\\n\\nFigure 16. Dilated convolutional structure [85].\\n\\n13\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nA residual block [46] has a branch that lead to a set of transformations F, whose outputs are\\n\\nappended to block\\'s input x, as shown in Eq. (12). 𝑜 = 𝐴𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 (𝑥 + 𝐹(𝑥)) (12) This method enables the network to focus on learning residual functions rather than the entire mapping. The TCN residual block typically consists of two layers of dilated causal convolutions followed by a non-linear activation function, such as Rectified Linear Unit (ReLU). The convolutional filters within the TCN are normalized using weight normalization [89], and dropout [90] is applied to each dilated convolution layer for regularization, where an entire channel is zeroed out at each training step. In contrast to a conventional ResNet, where the input is directly added to the output of the residual function, TCN adjusts for differing input-output widths by performing an additional 1 × 1 convolution to ensure that the element-wise addition ⊕ operates on tensors of matching dimensions.\\n\\n3.6 Kolmogorov-Arnold Network (KAN)\\n\\nKolmogorov-Arnold Networks (KANs) represent a promising alternative to traditional Multi- Layer Perceptrons (MLPs) by leveraging the Kolmogorov-Arnold theorem, a sophisticated mathematical framework that enhances the capacity of neural networks to process complex data structures. KANs were first introduced in 2024 by [91], with the goal of incorporating advanced mathematical theories into deep learning architectures to improve their performance on intricate tasks. While MLPs are inspired by the universal approximation theorem, KANs are motivated by the Kolmogorov-Arnold representation theorem [92], which states that any multivariate continuous function 𝑓 over a bounded domain can be expressed as a finite composition of simpler one- dimensional continuous functions:\\n\\n2𝑛+1\\n\\n𝑛\\n\\n𝑓(𝑥1, … . , 𝑥𝑛) = ∑ Φ𝑞\\n\\n(∑ 𝜙𝑞,𝑝(𝑥𝑝)\\n\\n) (13)\\n\\n𝑞=1\\n\\n𝑝=1\\n\\nwhere 𝜙𝑞,𝑝 is a mapping [0,1] → ℝ and Φ𝑞 is a mapping ℝ → ℝ. KAN maintains a fully connected structure like MLP, but with a key distinction: while MLP assign fixed activation functions to nodes (neurons), KAN assigns learnable activation functions to edges (weights). Consequently, KAN does not employ traditional linear weight matrices; instead, each weight parameter is replaced by a learnable one-dimensional function parameterized as a spline. Unlike MLP, which apply non-linear activation functions at each node, KAN nodes only sum the incoming data, relying on the rich, learnable spline functions to introduce non-linearity. Although this approach might initially seem computationally expensive, KAN often result in significantly smaller computation graphs compared to MLP. Fig. 17 illustrates the structure of a KAN.\\n\\nFigure 17. The structure of Kolmogorov-Arnold Network (KAN) [91].\\n\\n14\\n\\nF. M. Shiri et al.\\n\\nThe Kolmogorov-Arnold Network (KAN) can be expressed specifically as follows:\\n\\n𝐾𝐴𝑁(𝑥) = (Φ𝐿−1 ◦ Φ𝐿−2 ◦ · · · ◦ Φ1 ◦ Φ1)(𝑥) (14) The transformation of each layer, Φ𝑙 , operates on the input 𝑥𝑙 to generate 𝑥𝑙+1, the input\\n\\nfor the following layer, as follows:\\n\\n𝑥𝑙+1 = Φ𝑙(𝑥𝑙) =\\n\\n(\\n\\n𝜙𝑙,1,1(∙) 𝜙𝑙,1,2(∙) 𝜙𝑙,2,1(∙) 𝜙𝑙,2,2(∙)\\n\\n⋮\\n\\n⋮\\n\\n𝜙𝑙,𝑛𝑙+1,1(∙) 𝜙𝑙,𝑛𝑙+1,2(∙)\\n\\n… 𝜙𝑙,1,𝑛𝑙 … 𝜙𝑙,2,𝑛𝑙 ⋮ ⋱ … 𝜙𝑙,𝑛𝑙+1,𝑛𝑙\\n\\n(∙) (∙)\\n\\n(∙)\\n\\n)\\n\\n𝑥𝑙 (15)\\n\\nWhere each activation function ∅𝑙,𝑗,𝑖 is a spline, offering a rich, flexible response surface to\\n\\ninputs from the model:\\n\\n𝑠𝑝𝑙𝑖𝑛𝑒(𝑥) = ∑ 𝑐𝑖𝐵𝑖(𝑥), 𝑐𝑖 𝑎𝑟𝑒 𝑡𝑟𝑎𝑖𝑛𝑎𝑏𝑙𝑒 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡𝑠 (16)\\n\\n𝑖\\n\\nSeveral variants of KANs have emerged to tackle specific challenges in various applications: ➢ Convolutional KAN (CKAN) [93]: CKAN is a pioneering alternative to standard CNN, which have significantly advanced the field of computer vision. Convolutional KAN integrate the non-linear activation functions of KAN into the convolutional layers, leading to a substantial reduction in the number of parameters and offering a novel approach to optimizing neural network architectures.\\n\\n➢ Temporal KAN (TKAN) [94]: Temporal Kolmogorov-Arnold Networks combines the principles of KAN and Long Short-Term Memory (LSTM) networks to create an advanced architecture for time series analysis. Comprising layers of Recurrent Kolmogorov-Arnold Networks (RKANs) with embedded memory management, TKAN excels in multi-step time series forecasting. The TKAN architecture offers tremendous promise for improvement in domains needing one-step-ahead forecasting by solving the shortcomings of existing models in handling complicated sequential patterns [95, 96].\\n\\n➢ Multivariate Time Series KAN (MT-KAN) [97]: MT-KAN is specifically designed to handle multivariate time series data. The primary objective of MT-KAN is to enhance forecasting accuracy by modeling the intricate interactions between multiple variables. MT-KAN utilizes spline-parametrized univariate functions to capture temporal relationships while incorporating methods to model cross-variable interactions.\\n\\n➢ Fractional KAN (fKAN) [98]: fKAN is an enhancement of the KAN architecture that integrates the unique properties of fractional-orthogonal Jacobi functions into the network\\'s basis functions. This method guarantees effective learning and improved accuracy by utilizing the special mathematical characteristics of fractional Jacobi functions, such as straightforward derivative equations, non-polynomial behavior, and activity for positive and negative input values.\\n\\n➢ Wavelet KAN (Wav-KAN) [99]: The purpose of this innovative neural network design is to improve interpretability and performance by incorporating wavelet functions into the Kolmogorov-Arnold Networks (KAN) framework. Wav-KAN is an excellent way to capture complicated data patterns by utilizing wavelets\\' multiresolution analysis capabilities. It offers a reliable solution to the drawbacks of both recently suggested KANs and classic multilayer perceptrons (MLPs).\\n\\n➢ Graph KAN [100]: This innovative model applies KAN principles to graph-structured data, replacing the MLP and activation functions typically used in Graph Neural Networks (GNNs) with KAN. This substitution enables more effective feature extraction from graph-like data structures.\\n\\n15\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n4 Generative (Unsupervised) Deep Learning Models\\n\\nSupervised machine learning is widely used in artificial intelligence (AI), while unsupervised learning remains an active area of research with numerous unresolved questions. However, recent advancements in deep learning and generative modeling have injected new possibilities into unsupervised learning. A rapidly evolving domain within computer vision research is generative models (GMs). These models leverage training data originating from an unknown data-generating distribution to produce novel samples that adhere to the same distribution. The ultimate goal of generative models is to generate data samples that closely resemble real data distribution [101].\\n\\nVarious generative models have been developed and applied in different contexts, such as Auto-Encoder [102], Generative Adversarial Network (GAN) [103], Restricted Boltzmann Machine (RBM) [104], and Deep Belief Network (DBN) [105].\\n\\n4.1 Autoencoder\\n\\nThe concept of an autoencoder originated as a neural network designed to reconstruct its input data. Its fundamental objective is to learn a meaningful representation of the data in an unsupervised manner, which can have various applications, including clustering [102].\\n\\nAn autoencoder is a neural network that aims to replicate its input at its output. It consists of an internal hidden layer that defines a code representing the input data. The autoencoder network is comprised of two main components: an encoder function, denoted as 𝑧 = 𝑓(𝑥), and a decoder function that generates a reconstruction, denoted as 𝑟 = 𝑔(𝑧) [106]. The function 𝑓(𝑥) transforms a data point 𝑥 from the data space to the feature space, while the function 𝑔(𝑧) transforms 𝑧 from the feature space back to the data space to reconstruct the original data point 𝑥 . In modern autoencoders, these functions 𝑧 = 𝑓(𝑥) and 𝑟 = 𝑔(𝑧) are considered as stochastic functions, represented as 𝑝𝑒𝑛𝑐𝑜𝑑𝑒𝑟 (𝑧|𝑥) and 𝑝𝑑𝑒𝑛𝑐𝑜𝑑𝑒𝑟 (𝑟|𝑧), respectively, where 𝑟 denotes the reconstruction of 𝑥 [107]. Fig. 18 illustrates an autoencoder model.\\n\\nAutoencoder models find utility in various unsupervised learning tasks, such as generative modeling [108], dimensionality reduction [109], feature extraction [110], anomaly or outlier detection [111], and denoising [112].\\n\\nReconstructed\\n\\nOriginal data\\n\\ndata\\n\\nCompressed Representation\\n\\nFigure 18. The structure of autoencoders.\\n\\nIn general, autoencoder models can be categorized into two major groups: Regularized Autoencoders, which are valuable for learning representations for subsequent classification tasks, and Variational Autoencoders [113], which can function as generative models. Examples of include Sparse Autoencoder (SAE) [114], Contractive regularized autoencoder models Autoencoder (CAE) [115], and Denoising Autoencoder (DAE) [116].\\n\\nVariational Autoencoder (VAE) is a generative model that employs probabilistic distributions, such as the mean and variance of a Gaussian distribution, for data generation [102]. VAE provide a principled framework for learning deep latent-variable models and their associated inference models. The VAE consists of two coupled but independently parameterized models: the encoder or recognition model and the decoder or generative model. During \"expectation maximization\"\\n\\n16\\n\\nF. M. Shiri et al.\\n\\nlearning iterations, the generative model receives an approximate posterior estimation of its latent random variables from the recognition model, which it uses to update its parameters. Conversely, the generative model acts as a scaffold for the recognition model, enabling it to learn meaningful representations of the data, such as potential class labels. In terms of Bayes\\' rule, the recognition model is roughly the inverse of the generative model [117].\\n\\n4.2 Generative Adversarial Network (GAN)\\n\\nA notable neural network architecture for generative modeling, capable of producing realistic and novel samples on demand, is the Generative Adversarial Network (GAN), initially proposed by Ian Goodfellow in 2014 [103]. A GAN consists of two key components: a generative model and a discriminative model. The generative model aims to generate data that resemble real ones, while the discriminative model aims to differentiate between real and synthetic data. Both models are typically implemented using multilayer perceptrons [118]. Fig. 19 depicts the framework of a GAN, where a two-player adversarial game is played between a generator (G) and a discriminator (D). The generator\\'s updating gradients are determined by the discriminator through an adaptive objective [119].\\n\\nFigure 19. The framework of a GAN.\\n\\nAs previously mentioned, GANs operate based on principles derived from neural networks, utilizing a training set as input to generate new data that resembles the training set. In the case of GANs trained on image data, they can generate new images exhibiting human-like characteristics.\\n\\nThe following outlines the step-by-step operation of a GAN [120]:\\n\\n1. The generator, created by a discriminative network, generates content based on the real data distribution.\\n\\n2. The system undergoes training to increase the discriminator\\'s ability to distinguish between synthesized and real candidates, allowing the generator to better fool the discriminator.\\n\\n3. The discriminator initially trains using a dataset as the training data. 4. Training sample datasets are repeatedly presented until the desired accuracy is achieved. 5. The generator is trained to process random input and generate candidates that deceive the discriminator.\\n\\n17\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n6. Backpropagation is employed to update both the discriminator and the generator, with the former improving its ability to identify real images and the latter becoming more adept at producing realistic synthetic images.\\n\\n7. Convolutional Neural Networks (CNNs) are commonly used as discriminators, while deconvolutional neural networks are utilized as generative networks. Generative Adversarial Networks (GANs) have introduced numerous applications across various domains, including image blending [121], 3D object generation [122], face aging [123], medicine [124, 125], steganography [126], image manipulation [127], text transfer [128], language and speech synthesis [129], traffic control [130], and video generation [131].\\n\\nFurthermore, several models have been developed based on the Generative Adversarial Network (GAN) framework to address specific tasks. These models include Laplacian GAN (Lap- GAN) [132], Coupled GAN (Co-GAN) [118], Markovian GAN [133], Unrolled GAN [134], Wasserstein GAN (WGAN) [135], and Boundary Equilibrium GAN (BEGAN) [136], CycleGAN [137], DiscoGAN [138], Relativistic GAN [139], StyleGAN [140], Evolutionary GAN (E-GAN) [119], Bayesian Conditional GAN [141], Graph Embedding GAN (GE-GAN) [130].\\n\\n4.3 Deep Belief Network (DBN)\\n\\nThe Deep Belief Network (DBN) is a type of deep generative model utilized primarily in unsupervised learning to uncover patterns within large datasets. Consisting of multiple layers of hidden units, DBNs are adept at identifying intricate patterns and extracting features from data. Unlike discriminative models, DBNs exhibit a higher resistance to overfitting, making them well- suited for feature extraction from unlabeled data [142].\\n\\nThe stack of Restricted Boltzmann Machines (RBMs), which operate in an unsupervised learning framework, is a fundamental part of DBN. Every RBM in a DBN is made up of a hidden layer that contains latent representations and a visible layer that represents observable data features [143]. RBMs are trained layer by layer: first, each RBM is trained independently, and then all of the RBMs are fine-tuned together as a whole within the DBN.\\n\\nDuring the forward pass, the activations represent the probability of an output given a weighted input. In the backward pass, the activations estimate the probability of inputs given the weighted outputs. Through iterative training of RBMs within a DBN, these processes converge to form joint probability distributions of activations and inputs, allowing the network to effectively capture the underlying data structure [144, 145]. Fig. 20 illustrates the schematic structure of a Deep Belief Network (DBN).\\n\\nFigure 20. structure of a DBN model [143].\\n\\n18\\n\\nF. M. Shiri et al.\\n\\n5 Transformer Architecture\\n\\nThe Transformer architecture was originally introduced by Vaswani et al. [146] in 2017 for machine translation and has since become a foundational model in deep learning, especially for natural language processing (NLP). The transformer functions as a self-attention encoder-decoder structure. The encoder consists of a stack of identical layers, and each layer consists of two sublayers. A multi-head self-attention mechanism is the first layer, while the other layer is a position-wise fully connected feed-forward network. Also, A normalizing layer [147] and residual connections [46] connect the multi-headed self-attention module\\'s inputs and output. After that, a decoder uses the representation that the encoder produced to create an output sequence. A stack of identical layers makes up the decoder as well. The decoder adds a third sub-layer to each encoder layer in addition to the primary two, and this sub-layer handles multi-head attention over the encoder stack\\'s output. Like the encoder, residual connections and a normalizing layer are used surrounding each of the sub-layers. The encoder and decoder\\'s overall Transformer design is depicted in Fig. 21, left and right halves respectively [148, 149].\\n\\nTraditional RNN-based Seq2Seq models could be replaced with attention layers. Using various projection matrices, the query, key, and value vectors in the self-attention layer are all produced from the same sequence [150]. RNN training takes a very long period because it is sequential and iterative. Transformer training, on the other hand, is parallel and enables all features to be learned concurrently, significantly improving computational efficiency and cutting down on the amount of time needed for model training [151].\\n\\nMulti-Head Attention: In the Transformer model, a multi-headed self-attention mechanism is employed to enhance the model\\'s ability to capture dependencies between elements in a sequence. The core principle of the attention mechanism is that every token in the sequence can aggregate information from other tokens, allowing the model to understand contextual relationships more effectively. This is achieved by mapping a query, a set of key-value pairs, and an output (each represented as vectors) to form an attention function. The output is computed as a weighted sum of the values, where the weights are determined by the compatibility function between the query and its corresponding key [146].\\n\\nFigure 21. The architecture of the Transformer model [146].\\n\\n19\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nMulti-head attention is equivalent to the blended of 𝑛 distinct scaled dot-product attention (self-attention). It can effectively process the three vectors Q, K, and V, in parallel to obtain the final result by combining and calculating. The formula is visible in Eq. (17). 𝑀𝑢𝑙𝑡𝑖𝐻𝑒𝑎𝑑 (𝑄, 𝐾, 𝑉) = 𝐶𝑜𝑛𝑐𝑎𝑡(ℎ𝑒𝑎𝑑1, … , ℎ𝑒𝑎𝑑2)𝑊𝑂\\n\\n{\\n\\n𝑄, 𝐾𝑊𝑖\\n\\n𝐾, 𝑉𝑊𝑖 Where the projections are parameter matrices 𝑊𝑖 ℝ𝑑𝑚𝑜𝑑𝑒𝑙×𝑑𝑉, 𝑎𝑛𝑑 𝑊𝑂 ∈ ℝℎ𝑑𝑣×𝑑𝑚𝑜𝑑𝑒𝑙.\\n\\n𝑤ℎ𝑒𝑟𝑒 ℎ𝑒𝑎𝑑𝑖 = 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 (𝑄𝑊𝑖\\n\\n(17)\\n\\n𝑉) 𝑄 ∈ ℝ𝑑𝑚𝑜𝑑𝑒𝑙×𝑑𝑘, 𝑊𝑖\\n\\n𝐾 ∈ ℝ𝑑𝑚𝑜𝑑𝑒𝑙×𝑑𝑘, 𝑊𝑖\\n\\n𝑉 ∈\\n\\nThe main component of the transformer, scaled dot-product attention (self-attention), uses the\\n\\nweight of each sensor event in the input vector, which is represented by\\n\\n𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 (𝑄, 𝐾, 𝑉) = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 (\\n\\n𝑄𝐾𝑇\\n\\n√𝑑𝑘\\n\\n) 𝑉 (18)\\n\\nThe initial step in scaled dot-product attention is to convert the input data into an embedding vector and the three vectors of query vector (Q), key vector (K), and value vector (V) are then extracted from the embedding vectors. Next, a score is determined for every vector: score is equal to 𝑄 · 𝐾. Score normalization (dividing by √𝑑𝑘) is used for gradient stability. Next, the score is processed using the softmax activation function. The weighted score 𝑣 for every input vector is obtained by taking the softmax dot product value 𝑣. The final result is produced after summing. Scaled dot-product attention and multi-head attention are displayed in Fig. 22 [152]. Position-wise Feed-Forward Networks: Each encoder and decoder layer have a fully connected feed-forward network in addition to attention sub-layers. This feed-forward network is applied to each position independently and in the same way. This is made up of two linear transformations connected by a ReLU activation. 𝐹𝐹𝑁(𝑥) = 𝑚𝑎𝑥(0, 𝑥𝑊1 + 𝑏1)𝑊2 + 𝑏2 (19) Positional Encoding: Since the Transformer model does not rely on recurrence or convolution, it requires a way to capture the relative or absolute positions of tokens within a sequence to effectively utilize the sequence\\'s order. To address this, positional encoding is introduced at the input level of both the encoder and decoder stacks. These positional encodings are added to the input embeddings, as they share the same dimensionality, 𝑑𝑚𝑜𝑑𝑒𝑙. This combination enables the model to incorporate positional information, allowing it to better understand the sequential nature of the data [146].\\n\\n(a)\\n\\n(b)\\n\\nFigure 22. (a) Scaled Dot-Product Attention, (b) Multi-Head Attention.\\n\\n20\\n\\nF. M. Shiri et al.\\n\\nPositional encodings in transformer architecture were achieved by using sine and cosine\\n\\nfunctions of various frequencies:\\n\\n{\\n\\n𝑃𝐸(𝑝𝑜𝑠,2𝑖) = 𝑠𝑖𝑛(𝑝𝑜𝑠/100002𝑖/𝑑𝑚𝑜𝑑𝑒𝑙) 𝑃𝐸(𝑝𝑜𝑠,2𝑖+1) = 𝑐𝑜𝑠(𝑝𝑜𝑠/100002𝑖/𝑑𝑚𝑜𝑑𝑒𝑙)\\n\\n(20)\\n\\nwhere 𝑝𝑜𝑠 is the position and 𝑖 is the dimension. Every dimension of the positional encoding has a sinusoidal relationship. The wavelengths range from 2𝜋 𝑡𝑜 10000 · 2𝜋 in a geometric development. This function was selected because it would make it simple for the model to learn how to attend to relative positions, since for any fixed offset 𝑘, 𝑃𝐸𝑝𝑜𝑠+𝑘 can be expressed as a linear function of 𝑃𝐸𝑝𝑜𝑠.\\n\\n5.1 Transformer Variants\\n\\nThe Transformer architecture has proven to be highly versatile, with numerous variants developed to address specific challenges across different domains. Typically, Transformers are pre- trained on large datasets using unsupervised methods to learn general representations, which are then fine-tuned on specific tasks using supervised learning. This hybrid approach leverages the strengths of both learning paradigms. Some notable Transformer variants include: ➢ Bidirectional Encoder Representations from Transformers (BERT) [153]: A multi-layer bidirectional Transformer encoder for unsupervised pre-training in natural language understanding (NLU) tasks.\\n\\n➢ Generative pre-training Transformer (GPT) [154, 155]: A type of Transformer model developed by OpenAI that excels in natural language processing (NLP) tasks through unsupervised pre-training followed by supervised fine-tuning.\\n\\n➢ Transformer-XL [156]: It is proposed for language modeling to permit learning reliance beyond a set temporal coherence. Transformer-XL (Transformer-Extra Long) comprises a unique relative positional encoding method and a segment-level recurrence mechanism. This approach not only makes it possible to record longer-term dependencies, but also fixes the issue of context fragmentation.\\n\\nlength without compromising\\n\\n➢ XLNet [157]: It is a generalized autoregressive (AR) pretraining technique that combines the benefits of autoencoding (AE) and autoregressive (AR) techniques with a permutation language modeling aim. XLNet\\'s neural architecture, which integrates Transformer-XL and the two-stream attention mechanism, is built to function effortlessly with the autoregressive (AR) objective.\\n\\n➢ Fast Transformer [158]: It introduces multi-query attention as an alternative to multi-head attention. This approach reduces memory bandwidth requirements, leading to increased processing speed.\\n\\n➢ Multimodal Transformer (MulT) [159]: It is designed for analyzing human multimodal language. At the heart of MulT is the crossmodal attention mechanism, which provides a latent crossmodal adaptation that fuses multimodal information by directly attending to low-level features in other modalities.\\n\\n➢ Vision Transformer (ViT) [160]: An innovative approach based on Transformer structure for\\n\\nvisual tasks like image classification.\\n\\n➢ Pyramid Vision Transformer (PVT) [161]: An Transformer framework for complex\\n\\nprediction tasks like semantic segmentation and object recognition.\\n\\n➢ Swin Transformer [162]: A hierarchical transformer that uses shifted windows to construct its representation. A wide variety of vision tasks, including semantic segmentation, object detection, and image classification, may be performed with Swin Transformer.\\n\\n➢ Tokens-to-Token Vision Transformer (T2T-ViT) [163]: A vision transformer that can be\\n\\n21\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\ntrained from scratch on ImageNet. T2T-ViT overcomes ViT\\'s drawbacks by accurately modeling the structural information of images and enhancing feature richness.\\n\\n➢ Transformer in Transformer (TNT) [164]: A vision transformer for visual recognition. Both local and global representations are extracted by the TNT architecture through the use of an inner transformer and an outer transformer.\\n\\n➢ PyramidTNT [165]: A improved TNT model which used pyramid architecture, and\\n\\nconvolutional stem in order to greatly enhance the original TNT model.\\n\\n➢ Switch Transformers [166]: It is suggested as a straightforward and computationally effective\\n\\nmethod of increasing a Transformer model\\'s parameter count.\\n\\n➢ ConvNeXt [167]: A redesigned transformer architecture that makes use of the transformer attention mechanism and incorporates convolutional layers into the encoder and decoder modules to extract spatially localized data.\\n\\n➢ Evolutionary Algorithm Transformer (EATFormer) [168]: An\\n\\nimproved vision\\n\\ntransformer influenced by an evolutionary algorithm.\\n\\n6 Deep Reinforcement Learning\\n\\nReinforcement learning (RL) is a machine learning approach that deals with sequential decision-making, aiming to map situations to actions in a way that maximizes the associated reward. Unlike supervised learning, where explicit instructions are given after each system action, in the RL framework, the learner, known as an agent, is not provided with explicit guidance on which actions to take at each timestep 𝑡. The RL agent must explore through trial and error to determine which actions yield the highest rewards [169]. Furthermore, unlike supervised learning, where the correct output is obtained and the model is updated based on the loss or error, RL uses gradients without a differentiable loss function to teach a model to explore randomly and learn to make optimal decisions [170]. Fig. 23 depicts the agent-environment interaction in reinforcement learning (RL). The standard theoretical framework for RL is based on a Markov Decision Process (MDP), which extends the concept of a Markov process and is used to model decision-making based on states, actions, and rewards [171].\\n\\nDeep reinforcement learning combines the decision-making capabilities of reinforcement learning with the perception function of deep learning. It is considered a form of \"real AI\" as it aligns more closely with human thinking. Fig. 24 illustrates the basic structure of deep reinforcement learning, where deep learning processes sensory inputs from the environment and provides the current state data. The reinforcement learning process then links the current state to the appropriate action and evaluates values based on anticipated rewards [172, 173].\\n\\n22\\n\\nF. M. Shiri et al.\\n\\nFigure 23. Agent-Environment interaction in RL.\\n\\nFigure 24. Basic structure of Deep Reinforcement Learning (DRL) [172].\\n\\nOne of the most renowned deep reinforcement learning models is the Deep Q-learning Network (DQN) [174], which directly learns policies from high-dimensional inputs using Convolutional Neural Network (CNN). Other common models in deep reinforcement learning include Double DQN [175], Dueling DQN [176], and Monte Carlo Tree Search (MCTS) [177].\\n\\nDeep reinforcement learning (DRL) models find applications in various domains, such as video game playing [178, 179], robotic manipulation [180, 181], image segmentation [182, 183], video analysis [184, 185], energy management [186, 187], and more.\\n\\n7 Deep Transfer Learning\\n\\nDeep neural networks have significantly improved performance across various machine learning tasks and applications. However, achieving these remarkable performance gains often requires large amounts of labeled data for supervised learning, as it relies on capturing the latent patterns within the data [188]. Unfortunately, in certain specialized domains, the availability of sufficient training data is a major challenge. Constructing a large-scale, high-quality annotated dataset is costly and time-consuming [189].\\n\\nTo address the issue of limited training data, transfer learning (TL) has emerged as a crucial tool in machine learning. The concept of transfer learning finds its roots in educational psychology, where the theory of generalization suggests that transferring knowledge from one context to another is facilitated by generalizing experiences. To achieve successful transfer, there needs to be a connection between the two learning tasks. For example, someone who has learned to play the violin is likely to learn the piano more quickly due to the shared characteristics between musical instruments [190]. Fig. 25 depicts the learning process of transfer learning. Deep transfer learning (DTL) makes use of the learning experience to reduce the time and effort needed to train large networks as well as the time and effort needed to create the weights for an entire network from scratch [191].\\n\\nWith the growing popularity of deep neural networks in various fields, numerous deep transfer learning techniques have been proposed. Deep transfer learning can be categorized into four main types based on the techniques employed [189]: instances-based deep transfer learning, mapping- based (feature-based) deep transfer learning, network-based (model-based) deep transfer learning,\\n\\n23\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nand adversarial-based deep transfer learning.\\n\\nFigure 25. Learning process of transfer learning.\\n\\nInstances-based deep transfer learning involves selecting a subset of instances from the source domain and assigning appropriate weight values to these selected instances to supplement the training set in the target domain. Algorithms such as TaskTrAdaBoost [192] and TrAdaBoost.R2 [193] are well-known approaches based on this strategy.\\n\\nMapping-based deep transfer learning focuses on mapping instances from both the source and target domains into a new data space, where instances from the two domains exhibit similarity and are suitable for training a unified deep neural network. Successful methods based on this approach include Extend MMD (Maximum Mean Discrepancy) [194], and MK-MMD (Multiple Kernel variant of MMD) [195].\\n\\nNetwork-based (model-based) deep transfer learning involves reusing a segment of a pre- trained network from the source domain, including its architecture and connection parameters, and applying it to a deep neural network in the target domain. These model-based approaches are highly effective for domain adaptation between source and target data by adjusting the network (model), making them the most widely adopted strategies in deep transfer learning (DTL). Remarkably, these methods can even adapt target data that is significantly different from the source data [196]. Network-based (model-based) approaches in deep transfer learning typically involve pre- training, freezing, fine-tuning, and adding new layers. Pre-trained models consist of layers from a deep learning network (DL model) that have been trained using source data. Two key methods for training a model with target data are freezing and fine-tuning. These methods involve using some or all layers of a pre-defined model. When layers are frozen, they retain fixed parameters/weights from the pre-trained model. In contrast, fine-tuning involves initializing parameters and weights with pre-trained values instead of starting with random values, either for the entire network or specific layers [196].\\n\\nA recent advancement in model-based deep transfer learning is Progressive Neural Networks (PNNs). This strategy involves the freezing of a pre-trained model and integrating new layers specifically for training on target data [197]. The concept behind progressive learning is grounded in the idea that acquiring a new skill necessitates leveraging existing knowledge. This mirrors the way humans learn new abilities. For instance, a child learns to run by employing all the skills acquired during crawling and walking. PNN constructs a new model for each task it encounters.\\n\\n24\\n\\nF. M. Shiri et al.\\n\\nEach freshly generated model is interconnected with all others, aiming to learn a new task by applying the knowledge accumulated from preceding models.\\n\\nAdversarial-based methods focus on gathering transferable features from both the source and target data by leveraging logical relationships or rules acquired in the source domain. Alternatively, they may utilize techniques inspired by generative adversarial networks (GANs) [198].\\n\\nThese deep transfer learning techniques have proven to be effective in overcoming the challenge of limited training data, enabling knowledge transfer across domains, and facilitating improved performance in various applications such as image classification [199, 200], speech recognition [201, 202], video analysis [203, 204], signal processing [205, 206], and other.\\n\\nIn transfer learning, several popular pre-trained deep learning models are frequently used, including Xception [52], MobileNet [53], DenseNet [55], EfficientNet [57], NasNet [207], and among others. These models are initially trained on large-scale datasets like ImageNet, and their learned weights are then transferred to a target domain. The architectures of these networks reflect a broader trend in deep learning design, transitioning from manually crafted by human experts to automatically optimized patterns. This evolution focuses on striking a balance between model accuracy and computational complexity [208].\\n\\n8 Hybrid Deep Learning Models\\n\\nHybrid deep learning architectures, which integrate elements from various deep learning models, demonstrate significant potential in enhancing performance. By combining different fundamental generative or discriminative models, the following three categories of hybrid deep learning models can be particularly effective for addressing real-world problems: • Combination of various supervised models to extract more relevant and robust features, such as CNN+LSTM or CNN+GRU. By leveraging the strengths of different architectures, these hybrid models effectively capture both spatial and temporal dependencies within the data. Integrating various types of generative models, such as combining Autoencoders (AE) with Generative Adversarial Networks (GANs), to harness their strengths and enhance performance across a range of tasks. Integrating the capabilities of generative models with supervised models to leverage the strengths of both approaches can significantly enhance performance on various tasks. This hybrid strategy improves feature learning, data augmentation, and model robustness. Examples of such combinations include DBN+MLP, GAN+CNN, AE+CNN, and so on.\\n\\n\\n\\n\\n\\n9 Application of Deep Learning\\n\\nIn recent years, deep learning has demonstrated remarkable effectiveness across a wide range of applications, tackling various challenges in fields including healthcare, computer vision, speech recognition, natural language processing (NLP), e-learning, smart environments, and more. Fig. 26 highlights several potential real-world application areas of deep learning.\\n\\nFive useful categories have been established for these applications: classification, detection, localization, segmentation, and regression [10]. A concept called classification divides a collection of facts into classes. Detection typically involves recognizing objects and their boundaries within images, videos, or other data types. Localization refers to the process of identifying and determining the position of specific objects or features within an image or other types of data. Segmentation involves dividing an image or dataset into distinct regions or segments, with each segment representing a particular object or feature of interest. Regression is used to model and analyze the relationships between a dependent variable and one or more independent variables. It predicts continuous outcomes based on input features.\\n\\n25\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nFigure 26. Numerous possible domains for deep learning applications in the real world.\\n\\nHowever, each real-world application area has its own specific goals and requires particular tasks and deep learning techniques. Table 1 provides a summary of various deep learning tasks and methods applied across multiple real-world application domains.\\n\\nTable 1: A summary of the practical applications of deep learning models in real-world domains. Reference Application Setting Tasks Smart Homes & [209] Smart Cities [210] [211] [212] [213] [214] [82]\\n\\nModels CNN+LSTM Reinforcement learning GRU based CNN based Stacked GRU+LSTM\\n\\nHuman Activity Recognition Smart Energy Management Traffic Management Waste Management Smart Parking System Student Engagement Detection DenseNet self-attention Student Affective States Recognition Automatic Attendance System Automated Exam Control Medical Image Analysis Early Disease Detection\\n\\nEducation\\n\\nConvNeXt + GRU\\n\\nCNN+LSTM CNN based (VGG) Vision transformer InceptionV3\\n\\nHealthcare\\n\\n[215] [216] [217] [218]\\n\\n26\\n\\nRemote Patient Monitoring Analyze Genomic Data Question Answering Systems Sentiment Analysis Text Summarization\\n\\nCNN based Transfer learning based BERT based Transformer based Attentional LSTM LSTM+CNN Deep transfer learning ViT +CNN GRU+CNN Autoencoders, GAN CNN CNN+RNN Attention GRU LSTM based Extended ViT CNN based Transformer based CNN+ Bi-LSTM LSTM based LSTM+CNN Deep Autoencoder CNN+LSTM RNN based\\n\\nNatural Language Processing (NLP)\\n\\nSpeech Recognition Speech Emotion Recognition Automatic Speech Translation Plant Disease Detection Precision Agriculture Smart Irrigation System Soil Quality Prediction Earthquake Prediction Flood Forecasting Tsunami Prediction Land Cover Classification Investigation Wildfire Area Deforestation Detection Intrusion Detection Malware Detection Phishing Detection Credit Card Fraud Detection Biometric Authentication Context-Aware Recommendation Sequential Recommendation Multimodal Recommendation Purchase Behavior Prediction Loan Default Prediction Stock Trend Prediction Object Detection Pedestrian Detection Localization And Mapping Lane Detection & Path Planning CNN based Defect Detection Predictive Maintenance Process Optimization Supply Chain Optimization Robotic Grasping\\n\\nAgriculture\\n\\nNatural Disaster Management\\n\\nRemote Sensing\\n\\nCybersecurity\\n\\nRecommender Systems\\n\\nLSTM based CNN based RNN based CNN based Bi-LSTM Swin transformer +CNN Deep CNN CNN-GRU\\n\\nBusiness\\n\\nAutonomous Vehicles\\n\\nManufacturing\\n\\nTransformer based LSTM, GRU, CNN Reinforcement learning LSTM Reinforcement learning\\n\\nRobotics\\n\\nTracking And Motion Planning Reinforcement learning\\n\\nHuman-Robot Interaction\\n\\nRNN based\\n\\nF. M. Shiri et al.\\n\\n[219] [220] [221] [222] [223] [224] [201] [225] [226] [227] [228] [229] [230] [231] [232] [233] [234] [235] [236] [237] [238] [239] [240]\\n\\n[241] [242] [243] [244] [245] [246] [247] [248] [249] [250] [251] [252] [253] [254]\\n\\n[255]\\n\\n[256]\\n\\n27\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n10 Deep Learning Challenges\\n\\nWhile deep learning models have achieved remarkable success across various domains, they also come with significant challenges. Below are some of the most critical challenges, followed by potential solutions to address them.\\n\\n10.1 Insufficient Data\\n\\nDeep learning models require large amounts of data to perform well. The performance of these models typically improves as the volume of data increases. However, in many cases, sufficient data may not be available, making it difficult to train deep learning models effectively [10].\\n\\nThree possible approaches may be used to appropriately handle the insufficient data problem. The first method is Transfer Learning (TL), which is used to DL models by reusing pre-trained model pieces in new models. We thoroughly reviewed the transfer learning strategy in section 7. Data augmentation is the second method of gathering additional data. The goal of data augmentation is to improve the trained models\\' capacity for generalization. Generalization is necessary for networks to overcome small datasets or datasets with unequal class distributions, and it is especially crucial for real-world data [257]. There are several strategies for augmenting data, and each one is contingent upon the characteristics of the datasets [258]. A few of these techniques are geometric transformations [259], Mixup augmentation [260], Random oversampling [261], Feature space augmentation [262], generative data augmentation [263], and many more.\\n\\nThe third approach considers using simulated data to increase the training set\\'s volume. If you have a good understanding of the physical process, you can sometimes build simulators from it. Consequently, the outcome will include simulating as much data as necessary [10, 264].\\n\\n10.2 Imbalanced Data\\n\\nIn real-world situations, particularly in those that deep learning models address, the issue of class imbalance is common. If the majority of instances in the data set belong to one class and the remaining instances belong to the other class, then there is a class imbalance in a binary classification scenario. In multi-class, multi-label, multi-instance learning as well as in regression difficulties and other situations, class imbalances are present and are actually reinforced [265].\\n\\nIt has been determined that there are three main approaches to addressing imbalanced data: data-level techniques, algorithm-level techniques, and hybrid techniques. The focus of data-level techniques is to add or remove samples from training sets in order to balance the data distributions. These techniques balance the data distributions by adding new samples to the minority class (oversampling) or removing samples from the majority class (undersampling) [266, 267]. A variety of oversampling techniques, including Synthetic Minority Over-sampling Technique (SMOTE) [268], Borderline-SMOTE [269], Adaptive Synthetic (ADASYN) [270], SVM (Support Vector Machine)-SMOTE [271], Majority Weighted Minority Oversampling Technique (MWMOTE) [272], Sampling With the Majority (SWIM) [273], Reverse-SMOTE (R-SMOTE) [274], Constrained Oversampling (CO) [275], SMOTE Based on Furthest Neighbor Algorithm (SOMTEFUNA) [276], and many more can be used to solve imbalanced data problems. Also, there are several techniques for undersampling, including EasyEnsemble [277], BalanceCascade [277], Inverse Random Undersampling [278], MLP-based Undersampling Technique (MLPUS) [279], and others.\\n\\nAlgorithm-level approaches modify existing learning algorithms to mitigate the bias towards the majority class. These techniques require specialized knowledge of both the application domain and the learning algorithm to diagnose why a classifier fails under imbalanced class distributions [266]. Two of the most commonly used methods in this context are Cost-Sensitive Learning [280, 281] and One-Class Learning [282].\\n\\n28\\n\\nF. M. Shiri et al.\\n\\nThe third approach consists of hybrid methods, which combine algorithm-level techniques with data-level methods in the appropriate way. Hybridization is required to address issues with algorithm and data-level approaches and improve classification accuracy [283].\\n\\n10.3 Overfitting\\n\\nOverfitting occurs when a deep learning model learns the systematic and noise components of the training data to the point that it adversely affects the model\\'s performance on new data. In fact, overfitting occurs as a result of noise, the small size of the training set, and the complexity of the classifiers. Overfitted models tend to memorize all the data, including the inevitable noise in the training set, rather than understanding the underlying patterns in the data [24]. Overfitting is addressed with methods including dropout [90], weight decay [284], batch normalization [285, 286], regularization [287], data augmentation, and others, although determining the ideal balance is still difficult.\\n\\n10.4 Vanishing and Exploding Gradient\\n\\nIn deep neural networks, the computation of gradients is propagated layer by layer, leading to a phenomenon known as the vanishing or exploding gradient problem. As gradients are backpropagated through the network, they can exponentially diminish or grow, respectively, causing significant issues in training. When gradients vanish, the weights of the network are adjusted so minimally that the model\\'s learning process becomes exceedingly slow, potentially stalling altogether. Conversely, exploding gradients can cause weights to be updated excessively, leading to instability and divergence during training. This problem is particularly pronounced with non-linear activation functions such as sigmoid and tanh, which compress the output into a narrow range, further exacerbating the issue by limiting the gradient\\'s magnitude. Consequently, the model struggles to learn effectively, especially in deep networks where gradients must pass through many layers [8].\\n\\nTo mitigate the vanishing and exploding gradient problem, several strategies have been developed. One effective approach is to use the Rectified Linear Unit (ReLU) activation function, which does not saturate and therefore helps to maintain the gradient flow throughout the network [288]. Proper weight initialization techniques, such as Xavier initialization [289] can also reduce the likelihood of gradient issues by ensuring that initial weights are set in a way that prevents gradients from becoming too small or too large [290]. Another solution is batch normalization, which normalizes the inputs of each layer to maintain a stable distribution of activations throughout training. By doing so, batch normalization helps to alleviate the vanishing gradient problem and can accelerate convergence by reducing internal covariate shifts. Overall, addressing the vanishing and exploding gradient problem is crucial for training deep neural networks effectively, enabling them to learn complex patterns without succumbing to instability or inefficiency [286].\\n\\n10.5 Catastrophic Forgetting\\n\\nCatastrophic forgetting is a critical challenge in the pursuit of artificial general intelligence within neural networks. It occurs when a model, after being trained on a new task, loses its ability to perform previously learned tasks. This phenomenon is particularly problematic in scenarios where a model is expected to learn sequentially across multiple tasks without forgetting earlier ones, such as in lifelong learning or continual learning applications. The root cause of catastrophic forgetting lies like neural networks, which update their weights based on new training data. When trained on a new task, the model adjusts its parameters to optimize performance on that task, often at the expense of previously acquired knowledge. As a result, the model may exhibit excellent performance on the most recent task but perform poorly on earlier ones, effectively \"forgetting\"\\n\\n29\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nthem [291].\\n\\nSeveral strategies have been proposed to address catastrophic forgetting. One such approach is Elastic Weight Consolidation (EWC) [292], which penalizes changes to the weights that are important for previous tasks, thereby preserving learned knowledge while allowing the model to adapt to new tasks. Incremental Moment Matching (IMM) ) [293] is another technique that merges models trained on different tasks into a single model, balancing the performance across all tasks. The iCaRL (incremental Classifier and Representation Learning) [294] method combines classification with representation learning, enabling the model to learn new classes without forgetting previously learned ones. Additionally, the Hard Attention to the Task (HAT) [291] approach employs task-specific masks that prevent interference between tasks, reducing the likelihood of forgetting.\\n\\n10.6 Underspecifcation\\n\\nUnderspecification is an emerging challenge in the deployment of machine learning (ML) models, particularly deep learning (DL) models, in real-world applications. It refers to the phenomenon where an ML pipeline can produce a multitude of models that all perform well on the validation set but exhibit unpredictable behavior in deployment. This issue arises because the pipeline\\'s design does not fully specify which model characteristics are critical for generalization in real-world scenarios. The underspecification problem is often linked to the high degrees of freedom inherent in ML pipelines. Factors such as random seed initialization, hyperparameter selection, and the stochastic nature of training can lead to the creation of models with similar validation performance but divergent behaviors in production. These differences can manifest as inconsistent predictions when the model is exposed to new data or deployed in environments different from the training conditions [295].\\n\\nAddressing underspecification requires rigorous testing and validation beyond standard metrics. Stress tests, as proposed by D’Amour et al. [295], are designed to evaluate a model\\'s robustness under various real-world conditions, identifying potential failure points that may not be apparent during standard validation. These tests simulate different deployment scenarios, such as varying input distributions or environmental changes, to assess how the model\\'s predictions might vary. Moreover, some researches have been conducted to analyze and mitigate underspecification across different ML tasks [296, 297].\\n\\n11 Analysis of Deep Learning Models\\n\\nThis section details the methodology used in this study, which focuses on applying and evaluating various deep learning models for classification tasks across three distinct datasets. For our experimental analysis, we utilized three publicly available datasets: IMDB [298], ARAS [299], and Fruit-360 [300]. The objective is to conduct a comparative analysis of the performance of these deep learning models.\\n\\nThe IMDB dataset, which stands for Internet Movie Database, provides a collection of movie reviews categorized as positive or negative sentiments. ARAS is a dataset comprising annotated sensor events for human activity recognition tasks. Fruit-360 is a dataset consisting of images of various fruit types for classification purposes.\\n\\nWe began by evaluating eight different models: CNN, RNN, LSTM, Bidirectional LSTM, GRU, Bidirectional GRU, TCN, and Transformer on the IMDB and ARAS datasets. Our analysis aimed to compare the performance of these deep learning models across diverse datasets. The CNN model (Convolutional Neural Network) is particularly effective in capturing spatial dependencies, making it suitable for tasks involving structured data. RNN (Recurrent Neural Network) is well- suited for sequential data analysis, while LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) models are designed to capture long-term dependencies in sequential data. The\\n\\n30\\n\\nF. M. Shiri et al.\\n\\nBidirectional LSTM and Bidirectional GRU models provide an additional advantage by processing information in both forward and backward directions.\\n\\nAdditionally, we evaluated eight different CNN-based models: VGG, Inception, ResNet, InceptionResNet, Xception, MobileNet, DenseNet, and NASNet for the classification of fruit images using the Fruit-360 dataset. Given that image data is not sequential or time-dependent, recurrent models were not suitable for this task. CNN-based models are particularly effective for image analysis because of their ability to capture spatial dependencies. Moreover, the faster training time of CNN models is due to their parallel processing capabilities, which allow for efficient computation on GPU (Graphics Processing Unit), thereby accelerating the training process.\\n\\nTo evaluate the performance of these models, we employed assessment metrics such as accuracy, precision, recall, and F1-measure. Accuracy measures the overall correctness of the model\\'s predictions, while precision evaluates the proportion of correctly predicted positive instances. Recall assesses the model\\'s ability to correctly identify positive instances, and F1- measure provides a balanced measure of precision and recall [301].\\n\\n𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =\\n\\n𝑇𝑝 + 𝑇𝑛 𝑇𝑝 + 𝑇𝑛 + 𝐹𝑝 + 𝐹𝑛\\n\\n(21)\\n\\n𝑇𝑝 𝑇𝑝 + 𝐹𝑝 𝑇𝑝 𝑇𝑝 + 𝐹𝑛\\n\\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =\\n\\n(22)\\n\\n𝑅𝑒𝑐𝑎𝑙𝑙 =\\n\\n(23)\\n\\n𝐹1 − 𝑆𝑐𝑜𝑟𝑒 = 2 ×\\n\\n𝑅𝑒𝑐𝑎𝑙𝑙 × 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑅𝑒𝑐𝑎𝑙𝑙 + 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛\\n\\n(24)\\n\\nWhere 𝑇𝑝 = True Positive, 𝑇𝑛 = True Negative, 𝐹𝑝 = False Positive, and 𝐹𝑛 = False\\n\\nNegative.\\n\\nBy conducting a comprehensive analysis using these metrics, we can gain insights into the strengths and weaknesses of each deep learning model. This comparative evaluation enables us to identify the most effective model for specific datasets and applications, ultimately advancing the field of deep learning and its practical applications.\\n\\nAll experiments were conducted on a GeForce RTX 3050 GPU (Graphics Processing Unit)\\n\\nwith 4 Gigabyte of RAM (Random Access Memory).\\n\\n11.1 Methodology and Experiments on IMDB Dataset\\n\\nThe IMDB dataset is a widely used dataset for sentiment analysis tasks. It consists of movie reviews along with their corresponding binary sentiment polarity labels. The dataset contains a total of 50,000 reviews, evenly split into 25,000 training samples and 25,000 testing samples. There is an equal distribution of positive and negative labels, with 25,000 instances of each sentiment. To reduce the correlation between reviews for a given movie, only 30 reviews are included in the dataset [298]. Positive reviews often contain words like \"great,\" \"well,\" and \"love,\" while negative reviews frequently use words like \"bad\" and \"can\\'t.\" However, certain words such as \"one,\" \"character,\" and \"well\" appear frequently in both positive and negative reviews, although their usage may differ in terms of frequency between the two sentiment classes [72].\\n\\nIn our analysis, we employed eight different deep learning models including CNN, RNN, LSTM, Bidirectional LSTM, GRU, Bidirectional GRU, TCN, and Transformer for sentiment classification using the IMDB dataset. Fig. 27 presents a structural overview of the deep learning model intended for analyzing the performance of eight different models on the IMDB dataset.\\n\\n31\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nFigure 27. The structural for analysis of different deep learning models on IMDB dataset\\n\\nIn this architecture, text data is first passed through an embedding layer, which transforms the high-dimensional, sparse input into dense, lower-dimensional vectors of real numbers. This allows the model to capture semantic relationships within the data. In the second layer, one of eight models: CNN, RNN, LSTM, Bi-LSTM, GRU, Bi-GRU, TCN, or Transformer is employed for feature extraction and data training. This layer is crucial for capturing patterns and dependencies in the data. Following this, a dropout layer is included to address the issue of overfitting by randomly deactivating a portion of the neurons during training, which helps improve the model\\'s generalization. Subsequently, the multi-dimensional vector turns into a one-dimensional vector using a flatten layer, enabling it to work with fully connected layers. Finally, the output is passed through a fully connected (Dense) layer, which uses a Softmax function for classification, converting the model\\'s predictions into probabilities for each class.\\n\\nBuilding a neural network with high accuracy necessitates careful attention to hyperparameter selection, as these adjustments significantly influence the network\\'s performance. For example, setting the number of training iterations too high can lead to overfitting, where the model performs well on the training data but poorly on unseen data. Another critical hyperparameter is the learning rate, which affects the rate of convergence during training. If the learning rate is too high, the network may converge too quickly, potentially overshooting the global minimum of the loss function. Conversely, if the learning rate is too low, the convergence process may become excessively slow, prolonging training. Therefore, finding the optimal balance of hyperparameters is essential for maximizing the network\\'s performance and ensuring effective learning.\\n\\nIn the experiment phase, consistent parameters were applied across all models to ensure a standardized comparison. The parameters were set as follows: epochs = 30, batch size = 64, dropout = 0.2, with the loss function set to \"Binary Crossentropy,\" and the optimizer function set to Stochastic Gradient Descent (SGD) with a learning rate of 0.2. For the CNN model, 100 filters were used with a kernel size of 3, along with the Rectified Linear Unit (ReLU) activation function. The RNN, LSTM, Bi-LSTM, GRU, and Bi-GRU models each employed 64 units. The TCN model was configured with 16 filters, a kernel size of 5, and dilation rates of [1, 2, 4, 8]. The Transformer\\n\\n32\\n\\nF. M. Shiri et al.\\n\\nmodel was set up with 2 attention heads, a hidden layer size of 64 in the feed-forward network, and the ReLU activation function. These parameter settings and architectural choices were designed to allow for a standardized comparison of the deep learning models on the IMDB dataset. This standardization facilitates an accurate analysis of each model\\'s performance, enabling a comparison of their accuracy and loss values.\\n\\nTable 2 shows the result of different deep learning models on IMDB review dataset based on\\n\\nvarious metrics including Accuracy, Precision, Recall, F1-Score, and Time of training.\\n\\nTable 2: Result of different deep learning models on the IMDB dataset\\n\\nmodel\\n\\nAccuracy % Precision % Recall % F1-Score % Time (h:m:s)\\n\\nCNN\\n\\n85.90\\n\\n85.89\\n\\n85.88\\n\\n85.89\\n\\n0:02:57\\n\\nRNN\\n\\n59.03\\n\\n59.03\\n\\n59.02\\n\\n59.03\\n\\n0:12:23\\n\\nLSTM\\n\\n87.53\\n\\n87.53\\n\\n87.54\\n\\n87.54\\n\\n0:09:09\\n\\nBi-LSTM\\n\\n87.45\\n\\n87.46\\n\\n87.47\\n\\n87.46\\n\\n0:10:43\\n\\nGRU\\n\\n87.55\\n\\n87.56\\n\\n87.57\\n\\n87.56\\n\\n0:05:10\\n\\nBI-GRU\\n\\n87.97\\n\\n87.92\\n\\n87.99\\n\\n87.95\\n\\n0:09:54\\n\\nTCN\\n\\n84.42\\n\\n84.40\\n\\n84.42\\n\\n84.41\\n\\n0:07:38\\n\\nTransformer 88.03\\n\\n88.04\\n\\n88.01\\n\\n88.03\\n\\n0:03:44\\n\\nTo compare the performance of these models, we utilized accuracy, validation-accuracy, loss, and validation-loss diagrams. These diagrams provide insights into how well the models are learning from the data and help in evaluating their effectiveness for sentiment classification tasks. Fig. 28 shows the accuracy and validation-accuracy diagrams where the accuracy provides a visual representation of how the different deep learning models perform in terms of accuracy during the training process and validation-accuracy shows the trend of accuracy values on the testing set across multiple epochs for each model.\\n\\n(a) Accuracy Diagram\\n\\n(b) Validation-Accuracy Diagram\\n\\nFigure 28. Accuracy and validation-accuracy of deep learning models on IMDB dataset.\\n\\n33\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n(a) Loss Diagram\\n\\n(b) Validation-Loss Diagram\\n\\nFigure 29. Loss and validation- loss diagrams of deep learning models on IMDB dataset.\\n\\nFig. 29 illustrates the loss and validation-loss diagram where the loss diagram is a visual representation of loss values during the training process for six different models, and the validation- loss diagram depicts the variation in loss values on the testing set during the evaluation process for the different models. The loss function measures the discrepancy between the predicted sentiment labels and the actual labels.\\n\\nFurthermore, the confusion matrices for the various deep learning models are displayed in Fig. 30. These matrices provide a detailed breakdown of each model\\'s performance, highlighting how well the models classify different classes. By closely examining these confusion matrices, we can gain insights into the precision of the models and identify patterns of misclassification for each class. This analysis helps in understanding the strengths and weaknesses of the models\\' predictions.\\n\\nCNN\\n\\nRNN\\n\\nLSTM\\n\\nBi-LSTM\\n\\nGRU\\n\\nBi-GRU\\n\\nTCN\\n\\nTransformer\\n\\nFigure 30. Confusion matrix for different deep learning models on IMDB dataset.\\n\\n34\\n\\nF. M. Shiri et al.\\n\\nFigure 31. ROC-AUC diagrams for different deep learning models.\\n\\nAdditionally, Fig. 31 displays the ROC-AUC (Receiver Operating Characteristic-Area Under Curve) diagrams for eight different deep learning models. These diagrams offer valuable insights into the classification performance of the models, aiding in the assessment of their effectiveness. By analyzing the ROC-AUC curves, we can make informed decisions regarding model selection and threshold adjustments, ensuring a more accurate and effective classification approach.\\n\\nBased on the results provided, it can be concluded that the Transformer and Bi-GRU models achieved the best performance on the IMDB review dataset for sentiment analysis. Both models demonstrated high accuracy in classifying the sentiment of movie reviews. However, it is worth noting that the training time of the Transformer model was significantly less than that of the Bi- GRU model. This suggests that the Transformer model was faster to train compared to the Bi-GRU model while still achieving excellent performance. Additionally, the GRU model also exhibited good accuracy in sentiment classification and required less training time than the Bi-GRU model. Overall, the results suggest that the Transformer, and GRU models are effective deep learning models for sentiment analysis on the IMDB review dataset, with varying trade-offs between performance and training time.\\n\\n11.2 Methodology and Experiments on ARAS Dataset\\n\\nBased on the provided information, the ARAS dataset [299] is a valuable resource for recognizing human activities in smart environments. It consists of data streams collected from two houses over a period of 60 days, with 20 binary sensors installed to monitor resident activity. The dataset includes information on 27 different activities performed by two residents, and the sensor events are recorded on a per-second basis.\\n\\nEight distinct deep learning models were used in our investigation to recognize human activities: CNN, RNN, LSTM, Bidirectional LSTM, GRU, Bidirectional GRU, TCN, and Transformer. A structural overview of the deep learning model designed to analyze the performance of eight different models on the ARAS dataset is shown in Fig. 32.\\n\\nThe first phase involves preprocessing the sensor data to ensure it is in a suitable and standardized format for deep learning models. The initial task in this phase is data cleaning, where any recorded instances where all sensor events are zero, and the resident is inside the house, are\\n\\n35\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nremoved from the dataset. Next, a time-based static sliding window technique is applied for segmenting sensor events. This method groups sequences of sensor events into intervals of equal duration. Optimizing the time interval is crucial for effective segmentation; after evaluating intervals ranging from 30 to 360 seconds, a 90-second interval was determined to be optimal for the ARAS dataset. The segmentation task aids in decreasing training time and increasing accuracy for the deep learning models.\\n\\nFigure 32. The structural for analysis of different deep learning models on the ARAS dataset\\n\\nAfter preprocessing, the data is passed through an input layer. In the second layer, one of eight models: CNN, RNN, LSTM, Bi-LSTM, GRU, Bi-GRU, TCN, or Transformer is employed for feature extraction and training. This layer plays a vital role in capturing patterns and dependencies within the data. To mitigate overfitting, a dropout layer follows, which randomly deactivates a portion of the neurons during training, thereby improving the model\\'s generalization. Subsequently, a flatten layer is used to convert the multi-dimensional vector into a one-dimensional vector, making it compatible with fully connected layers. Finally, the output passes through a fully connected (Dense) layer, which uses a Softmax function for classification, transforming the model’s predictions into probability distributions across the classes.\\n\\nIn the experimental phase, we split the data from the first resident of house B, allocating 70% for training and 30% for testing, using a random split. Additionally, 20% of the training data was set aside for validation. The models were trained with a fixed set of parameters: 30 epochs, a batch size of 64, a dropout rate of 0.2, the \"Categorical Crossentropy\" loss function, and the Adam optimizer. For the CNN model, we used 100 filters with a kernel size of 3 and the rectified linear unit (ReLU) activation function. The RNN, LSTM, Bi-LSTM, GRU, and Bi-GRU models were configured with 64 units each. The TCN model was set with 16 filters, a kernel size of 5, and dilation rates of [1, 2, 4, 8]. The Transformer model utilized 2 attention heads, a hidden layer size of 64 in the feedforward network, and the ReLU activation function.\\n\\nTable 3 illustrates the results of experiments on ARAS dataset with various metrices including\\n\\nAccuracy, Precision, Recall, F1-Score, and Time of training.\\n\\n36\\n\\nF. M. Shiri et al.\\n\\nTable 3: Result of different deep learning models on the ARAS dataset\\n\\nmodel\\n\\nAccuracy % Precision % Recall % F1-Score % Time (h:m:s)\\n\\nCNN\\n\\n93.14\\n\\n95.59\\n\\n92.43\\n\\n93.98\\n\\n0:01:18\\n\\nRNN\\n\\n93.17\\n\\n96.19\\n\\n91.67\\n\\n93.88\\n\\n0:04:09\\n\\nLSTM\\n\\n93.29\\n\\n95.56\\n\\n92.82\\n\\n93.81\\n\\n0:03:23\\n\\nBi-LSTM\\n\\n93.33\\n\\n96.66\\n\\n92.12\\n\\n94.15\\n\\n0:04:01\\n\\nGRU\\n\\n93.65\\n\\n96.08\\n\\n91.78\\n\\n94.31\\n\\n0:03:15\\n\\nBI-GRU\\n\\n93.90\\n\\n95.87\\n\\n92.61\\n\\n94.49\\n\\n0:03:56\\n\\nTCN\\n\\n94.04\\n\\n95.37\\n\\n93.48\\n\\n94.42\\n\\n0:04:06\\n\\nTransformer\\n\\n94.56\\n\\n95.61\\n\\n94.06\\n\\n94.83\\n\\n0:03:14\\n\\nAlso, Fig. 33 presents the accuracy diagram and validation-accuracy diagram for the deep learning models, while Fig. 34 shows the loss diagram and validation-loss diagram for deep learning models.\\n\\n(a) Accuracy Diagram\\n\\n(b) Validation-Accuracy Diagram\\n\\nFigure 33. Accuracy and validation- accuracy diagrams of deep learning models on ARAS dataset.\\n\\n(a) Loss Diagram\\n\\n(b) Validation-Loss Diagram\\n\\nFigure 34. Loss and validation- loss diagrams of deep learning models on ARAS dataset\\n\\n37\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\nSince we performed preprocessing tasks like data cleaning and segmentation, the data is nearly normalized and balanced, leading to consistent and closely grouped results across all models. However, the results indicate that the Transformer and TCN models outperformed the others on the ARAS dataset. This outcome aligns with the dataset\\'s nature, which comprises spatial and temporal sequences of sensor events. Among the models, the Transformer exhibited the highest performance in terms of accuracy, recall, and F1-score, while the Bi-LSTM model excelled in the precision metric. Moreover, the Transformer model demonstrated a notable advantage in training time, second only to the CNN model, underscoring its efficiency in processing and learning from time-series data. Additionally, when examining the accuracy and loss curves, it is evident that the Transformer, TCN, and CNN models stabilized earlier than the others. Overall, the Transformer model proved to be the most effective for working with the ARAS dataset, striking a balance between accuracy, training time, and consistency throughout the training phases, making it the optimal choice for recognizing human activities based on sensor data.\\n\\n11.3 Methodology and Experiments on the Fruit-360 Dataset\\n\\nSince images are not sequential or time-dependent, recurrent models were less effective for these tasks. CNN-based models, on the other hand, are highly valuable for image analysis due to their ability to capture spatial relationships. Consequently, the analysis of deep learning models on the Fruit-360 dataset for image classification focused on eight CNN variants: VGG, Inception, ResNet, InceptionResNet, Xception, MobileNet, DenseNet, and NASNet. These models use deep transfer learning technique for training image data and improving classification accuracy. Fig. 35 provides a structural overview of the deep learning models used to evaluate the performance of these eight variants on the Fruit-360 dataset.\\n\\nFigure 35. The structural for analysis of different CNN-based models on Fruit-360 dataset.\\n\\n38\\n\\nF. M. Shiri et al.\\n\\nFirst, the fruit images are passed through an input layer. In the second layer, one of eight models (VGG, Inception, ResNet, InceptionResNet, Xception, MobileNet, DenseNet, or NASNet) is employed for feature extraction and training. Next, a Global Average Pooling 2D (GAP) layer is applied, which significantly reduces the spatial dimensions of the data by collapsing each feature map into a single value. To combat overfitting, a dropout layer is then introduced, randomly deactivating a portion of the neurons during training, which enhances the model\\'s ability to generalize. Finally, the output is passed through a fully connected (Dense) layer, where a Softmax function is used to classify the fruit images.\\n\\nThe dataset comprises 55,244 images of 81 different fruit classes, each with a resolution of 100 × 100 pixels. For the experiments, a subset of 60 fruit classes was selected, containing 28,484 images for training and 9,558 images for testing. Non-fruit items such as chestnuts and ginger root were removed from the dataset.\\n\\nAll models were trained with a consistent set of parameters: 20 epochs, a batch size of 512, a dropout rate of 0.2, the \"Categorical Crossentropy\" loss function, and the Adam optimizer. Additionally, all models utilized the “ImageNet” dataset for pre-training.\\n\\nTable 4 presents the experimental results for various models on the Fruit-360 dataset, including VGG16, InceptionResNetV2, Xception, MobileNet, DenseNet121, and NASNetLarge. The table includes metrics such as Accuracy, Precision, Recall, F1-Score, and Time of training.\\n\\nInceptionV3, ResNet50,\\n\\nTable 4: Result of different deep learning models on the Fruit360 dataset\\n\\nmodel\\n\\nAccuracy % Precision % Recall % F1-Score % Time (h:m:s)\\n\\nVGG\\n\\n94.39\\n\\n99.79\\n\\n80.65\\n\\n89.20\\n\\n2:17:32\\n\\nInception\\n\\n95.86\\n\\n96.65\\n\\n95.14\\n\\n95.89\\n\\n0:23:34\\n\\nResNet\\n\\n94.59\\n\\n95.30\\n\\n93.64\\n\\n94.46\\n\\n1:12:56\\n\\nInceptionResNet 96.05\\n\\n97.01\\n\\n95.36\\n\\n96.18\\n\\n0:54:18\\n\\nXception\\n\\n97.38\\n\\n98.28\\n\\n96.61\\n\\n97.44\\n\\n1:01:11\\n\\nMobileNet\\n\\n98.54\\n\\n98.88\\n\\n98.28\\n\\n98.58\\n\\n0:17:22\\n\\nDenseNet\\n\\n98.94\\n\\n99.12\\n\\n98.75\\n\\n98.94\\n\\n1:10:30\\n\\nNASNet\\n\\n96.99\\n\\n97.69\\n\\n96.56\\n\\n97.12\\n\\n3:50:05\\n\\nFurthermore, the accuracy, validation-accuracy, loss, and validation-loss diagrams were used to compare the performance of various models. When assessing the models\\' performance for tasks involving the categorization of fruit photos, these graphs offer valuable insights into how effectively the models are learning from the data. Fig. 36 shows the accuracy and validation- accuracy diagram of the deep learning models, while Fig. 37 illustrates the loss diagram and validation-loss diagram of the deep learning models.\\n\\nBased on the results, it can be concluded that the DenseNet and MobileNet models achieved the best performance for fruit image classification on the Fruit-360 dataset. Both models demonstrated high accuracy in classifying fruit images. Notably, MobileNet had a significantly shorter training time compared to DenseNet, indicating that it was faster to train while still delivering performance close to that of DenseNet. Additionally, the Xception model also showed good accuracy and required less training time than DenseNet. Overall, the MobileNet model stands out as a favorable choice due to its balance between accuracy and training efficiency.\\n\\n39\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n(a) Accuracy Diagram\\n\\n(b) Validation-Accuracy Diagram\\n\\nFigure 36. Accuracy and validation- accuracy diagrams of different CNN-based deep learning models on Friut-360 dataset.\\n\\n(a) Loss Diagram\\n\\n(b) Validation-Loss Diagram\\n\\nFigure 37. Loss and validation- loss diagrams of different CNN-based deep learning models on Friut-360 dataset.\\n\\n12 Research Directions and Future Aspects\\n\\nIn the preceding sections, we explored a range of deep learning topics, highlighting both the advantages and limitations of various deep learning models. Additionally, we examined the application of several models across different domains. Despite the benefits demonstrated, our research has identified certain gaps, indicating that further advancements are necessary. This section outlines potential future research directions based on our analysis. - Generative (Unsupervised) Models: Generative models, a key category of deep learning models discussed in Section 4, hold significant promise for future research. These models enable the creation of new data representations through exploratory analysis and can identify high-order correlations or features in data. Unlike supervised learning, unsupervised models can derive insights from data without the need for labeled examples, making them valuable for various applications. Several generative models, including Autoencoders, Generative Adversarial Networks (GANs), Deep Belief Networks (DBNs), and Self-Organizing Maps (SOMs), have been developed and employed across diverse contexts. A promising research avenue involves analyzing these models in various settings and developing new methods or variations that enhance data modeling or representation for specific real-world applications. The rising interest in GANs is\\n\\n40\\n\\nF. M. Shiri et al.\\n\\nparticularly noteworthy, as they excel in leveraging unlabeled image data for deep representation learning and training highly non-linear mappings between latent and data spaces. The GAN framework offers flexibility to formulate new theories and methods tailored to emerging deep learning applications, positioning it as a pivotal area for future exploration. - Hybrid/Ensemble Modeling: Hybrid deep learning architectures have shown great potential in enhancing model performance by combining components from multiple models. For instance, the integration of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) can capture both temporal and spatial dependencies in data, leveraging the strengths of each model. Hybrid models also benefit from combining generative and supervised learning, offering superior performance and improved uncertainty handling in high-risk scenarios. Developing effective hybrid models, whether supervised or unsupervised, presents a significant research opportunity to address a wide range of real-world problems, including semi-supervised learning tasks and model uncertainty. This approach moves beyond conventional, isolated models, emphasizing the need for sophisticated methods that can handle the complexity of various data types and applications. - Hyperparameter Optimization for Efficient Deep Learning: As deep learning models have evolved, the number of parameters, computational latency, and resource requirements have increased substantially [150]. Selecting the appropriate hyperparameters is critical to building a neural network with high accuracy. Key hyperparameters include learning rate, loss function, batch size, number of training iterations, and dropout rate, among others. The challenge lies in finding an optimal balance of these parameters, as they significantly influence network performance. However, iterating through all possible combinations of hyperparameters is computationally expensive. To address this, metaheuristic optimization techniques, such as Genetic Algorithm (GA) [302], Particle Swarm Optimization (PSO) [303], and others, can be employed to explore the search space more efficiently than exhaustive methods. Meta-heuristic algorithms are a type of heuristic optimization algorithm that include mechanisms to avoid getting trapped in local optimization [304]. Future research should focus on optimizing hyperparameters tailored to specific data types and contexts. For example, the learning rate plays a crucial role in training, where a rate too high may cause the model to converge prematurely, while a rate too low can lead to slow convergence and prolonged training times. Adaptive learning rate techniques, such as including Adaptive Moment Estimation (Adam) [305], Stochastic Gradient Descent (SGD) [306], adaptive gradient algorithm (ADAGRAD) [307], and Nesterov-accelerated Adaptive Moment Estimation (Nadam) [308], and more recent innovations like Evolved Sign Momentum (Lion) [309], offer promising avenues for improving network performance and minimizing loss functions. Future research could further explore these optimizers, focusing on their comparative effectiveness in enhancing model performance through iterative weight and bias adjustments. - Federated Learning: Federated learning is an emerging deep learning paradigm that enables collaborative model training across multiple organizations or teams without the need to share raw data. This approach is particularly relevant in contexts where data privacy is paramount. However, federated learning introduces new challenges, especially with the advent of data fusion technologies that combine data from multiple sources with varying formats. As data diversity and volume continue to grow, optimizing data and model utilization in federated learning becomes increasingly important. Addressing challenges such as safeguarding user privacy, developing universal models, and ensuring the stability of data fusion outcomes will be crucial for the future application of federated learning across multiple domains [310]. - Quantum Deep Learning: Quantum computing and deep learning have both seen significant advancements over the past few decades. Quantum computing, which leverages the principles of quantum mechanics to store and process information, has the potential to outperform classical supercomputers on certain tasks, making it a powerful tool for complex problem-solving. The intersection of quantum computing and deep learning has led to the emergence of quantum deep learning and quantum-inspired deep learning algorithms. Future research directions in this area\\n\\n41\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\ninclude investigating and developing quantum deep learning models, such as Quantum Convolutional Neural Network (Quantum CNN) [311], Quantum Recurrent Neural Network (Quantum RNN) [312], Quantum Generative Adversarial Network (Quantum GAN) [313], and others. Additionally, exploring the application of these models across various domains and creating novel quantum deep learning architectures represents a cutting-edge frontier in the field [314, 315]. In conclusion, the research directions outlined above underscore the dynamic and evolving nature of deep learning. By addressing these challenges and exploring new avenues, the field can continue to advance, driving innovation and enabling the development of more powerful and efficient models for a wide range of applications.\\n\\n13 Conclusion\\n\\nThis article provides an extensive overview of deep learning technology and its applications in machine learning and artificial intelligence. The article covers various aspects of deep learning, including neural networks, MLP models, and different types of deep learning models such as CNN, RNN, TCN, Transformer, generative models, DRL, and transfer learning. The classification of deep learning models allows for a better understanding of their specific applications and characteristics. The RNN models, including LSTM, Bi-LSTM, GRU, and Bi-GRU, are particularly suited for time series data due to their ability to capture temporal dependencies. On the other hand, CNN-based models excel in image data analysis by effectively capturing spatial features.\\n\\nThe experiments conducted on three public datasets, namely IMDB, ARAS, and Fruit-360, further reinforce the suitability of specific deep learning models for different data types. The results demonstrate that the CNN-based models such as DenseNet and MobileNet perform exceptionally well in image classification tasks. RNN models, such as LSTM and GRU, show strong performance in time series analysis. However, the Transformer model outperforms classical RNN-based models, particularly in text analysis, due to its use of the attention mechanism.\\n\\nOverall, this article highlights the diverse applications and effectiveness of deep learning models in various domains. It emphasizes the importance of selecting the appropriate deep learning model based on the nature of the data and the task at hand. The insights gained from the experiments contribute to a better understanding of the strengths and weaknesses of different deep learning models, facilitating informed decision-making in practical applications.\\n\\nReferences\\n\\n[1] P. P. Shinde and S. Shah, \"A review of machine learning and deep learning applications,\" in 4th Int.\\n\\nConf. Comput. Commun. Ctrl. Autom. (ICCUBEA), Pune, India, 16-18 Aug 2018: IEEE, pp. 1-6, doi:\\n\\n10.1109/ICCUBEA.2018.8697857.\\n\\n[2] C. Janiesch, P. Zschech, and K. Heinrich, \"Machine learning and deep learning,\" Electron. Mark., vol.\\n\\n31, no. 3, pp. 685-695, 2021, doi: 10.1007/s12525-021-00475-2.\\n\\n[3] W. Han et al., \"A survey of machine learning and deep learning in remote sensing of geological\\n\\nenvironment: Challenges, advances, and opportunities,\" ISPRS J. Photogramm. Remote. Sens., vol. 202,\\n\\npp. 87-113, 2023, doi: 10.1016/j.cogr.2023.04.001.\\n\\n[4] S. Zhang et al., \"Deep Learning in Human Activity Recognition with Wearable Sensors: A Review on\\n\\nAdvances,\" Sens., vol. 22, no. 4, Feb 14 2022, doi: 10.3390/s22041476.\\n\\n[5] S. Li, Y. Tao, E. Tang, T. Xie, and R. Chen, \"A survey of field programmable gate array (FPGA)-based\\n\\ngraph convolutional neural network accelerators: challenges and opportunities,\" PeerJ Computer\\n\\nScience, vol. 8, pp. e1166, 2022.\\n\\n42\\n\\nF. M. Shiri et al.\\n\\n[6] A. Mathew, P. Amudha, and S. Sivakumari, \"Deep learning techniques: an overview,\" in Adv. Mach.\\n\\nLearn. Technol. App.: AMLTA 2020, 2021, pp. 599-608.\\n\\n[7] J. Liu and Y. Jin, \"A comprehensive survey of robust deep learning in computer vision,\" J. Autom.\\n\\nIntell. , 2023, doi: 10.1016/j.jai.2023.10.002.\\n\\n[8] A. Shrestha and A. Mahmood, \"Review of deep learning algorithms and architectures,\" IEEE access.,\\n\\nvol. 7, pp. 53040-53065, 2019, doi: 10.1109/ACCESS.2019.2912200.\\n\\n[9] M. A. Wani, F. A. Bhat, S. Afzal, and A. I. Khan, Advances in deep learning. Springer, 2020.\\n\\n[10] L. Alzubaidi et al., \"Review of deep learning: Concepts, CNN architectures, challenges, applications,\\n\\nfuture directions,\" J. Big. Data., vol. 8, pp. 1-74, 2021, doi: 10.1186/s40537-021-00444-8.\\n\\n[11] I. H. Sarker, \"Deep learning: a comprehensive overview on techniques, taxonomy, applications and\\n\\nresearch directions,\" SN Comput. Sci., vol. 2, no. 6, pp. 420, 2021, doi: 10.1007/s42979-021-00815-1.\\n\\n[12] M. N. Hasan, T. Ahmed, M. Ashik, M. J. Hasan, T. Azmin, and J. Uddin, \"An Analysis of Covid-19\\n\\nPandemic Outbreak on Economy using Neural Network and Random Forest,\" J. Inf. Syst. Telecommun.\\n\\n(JIST), vol. 2, no. 42, pp. 163, 2023, doi: 10.52547/jist.34246.11.42.163.\\n\\n[13] N. B. Gaikwad, V. Tiwari, A. Keskar, and N. Shivaprakash, \"Efficient FPGA implementation of\\n\\nmultilayer perceptron for real-time human activity classification,\" IEEE Access., vol. 7, pp. 26696-\\n\\n26706, 2019, doi: 10.1109/ACCESS.2019.2900084.\\n\\n[14] K.-C. Ke and M.-S. Huang, \"Quality prediction for injection molding by using a multilayer perceptron\\n\\nneural network,\" Polym., vol. 12, no. 8, pp. 1812, 2020, doi: 10.3390/polym12081812.\\n\\n[15] A. Tasdelen and B. Sen, \"A hybrid CNN-LSTM model for pre-miRNA classification,\" Sci. Rep., vol.\\n\\n11, no. 1, pp. 1-9, 2021, doi: 10.1038/s41598-021-93656-0.\\n\\n[16] L. Qin, N. Yu, and D. Zhao, \"Applying the convolutional neural network deep learning technology to\\n\\nbehavioural recognition in intelligent video,\" Tehnički vjesnik, vol. 25, no. 2, pp. 528-535, 2018, doi:\\n\\n10.17559/TV-20171229024444.\\n\\n[17] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, \"A Survey of Convolutional Neural Networks: Analysis,\\n\\nApplications, and Prospects,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 33, no. 12, pp. 6999-7019,\\n\\nDec 2022, doi: 10.1109/TNNLS.2021.3084827.\\n\\n[18] B. P. Babu and S. J. Narayanan, \"One-vs-All Convolutional Neural Networks for Synthetic Aperture\\n\\nRadar Target Recognition,\" Cybern. Inf. Technol, vol. 22, pp. 179-197, 2022, doi: 10.2478/cait-2022-\\n\\n0035.\\n\\n[19] S. Mekruksavanich and A. Jitpattanakul, \"Deep convolutional neural network with rnns for complex\\n\\nactivity recognition using wrist-worn wearable sensor data,\" Electro., vol. 10, no. 14, pp. 1685, 2021,\\n\\ndoi: 10.3390/electronics10141685.\\n\\n[20] W. Lu, J. Li, J. Wang, and L. Qin, \"A CNN-BiLSTM-AM method for stock price prediction,\" Neural\\n\\nComput. Appl., vol. 33, pp. 4741-4753, 2021, doi: 10.1007/s00521-020-05532-z.\\n\\n[21] W. Rawat and Z. Wang, \"Deep convolutional neural networks for image classification: A\\n\\ncomprehensive\\n\\nreview,\" Neural Comput., vol. 29, no. 9, pp. 2352-2449, 2017, doi:\\n\\n10.1162/NECO_a_00990.\\n\\n[22] L. Chen, S. Li, Q. Bai, J. Yang, S. Jiang, and Y. Miao, \"Review of image classification algorithms based\\n\\non convolutional neural networks,\" Remote Sens., vol. 13, no. 22, pp. 4712, 2021, doi:\\n\\n10.3390/rs13224712.\\n\\n43\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n[23] J. Gu et al., \"Recent advances in convolutional neural networks,\" Pattern. Recognit., vol. 77, pp. 354-\\n\\n377, 2018, doi: 10.1016/j.patcog.2017.10.013.\\n\\n[24] S. Salman and X. Liu, \"Overfitting mechanism and avoidance in deep neural networks,\" arXiv preprint\\n\\narXiv:1901.06566, 2019.\\n\\n[25] A. Ajit, K. Acharya, and A. Samanta, \"A review of convolutional neural networks,\" in 2020 Int. Conf.\\n\\nEmerg. Tren. Inf. Technol. Engr. (ic-ETITE). 2020: IEEE, pp. 1-5.\\n\\n[26] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi, \"A survey of deep neural network\\n\\narchitectures and\\n\\ntheir applications,\" Neurocomputing., vol. 234, pp. 11-26, 2017, doi:\\n\\n10.1016/j.neucom.2016.12.038.\\n\\n[27] K. He, X. Zhang, S. Ren, and J. Sun, \"Spatial pyramid pooling in deep convolutional networks for visual\\n\\nrecognition,\" IEEE Trans. Pattern. Anal. Mach. Intell., vol. 37, no. 9, pp. 1904-1916, 2015, doi:\\n\\n10.1109/TPAMI.2015.2389824.\\n\\n[28] D. Yu, H. Wang, P. Chen, and Z. Wei, \"Mixed pooling for convolutional neural networks,\" in Rough.\\n\\nSets. Knwl. Technol.: 9th Int. Conf., RSKT Shanghai, China, October 24-26 2014: Springer, pp. 364-\\n\\n375, doi: 10.1007/978-3-319-11740-9_34.\\n\\n[29] Y. Gong, L. Wang, R. Guo, and S. Lazebnik, \"Multi-scale orderless pooling of deep convolutional\\n\\nactivation features,\" in Comput. Vis. (ECCV): 13th Europ. Conf., Zurich, Switzerland, September 6-12\\n\\n2014: Springer, pp. 392-407.\\n\\n[30] M. D. Zeiler and R. Fergus, \"Stochastic pooling for regularization of deep convolutional neural\\n\\nnetworks,\" arXiv preprint arXiv:1301.3557, 2013.\\n\\n[31] V. Dumoulin and F. Visin, \"A guide to convolution arithmetic for deep learning,\" arXiv preprint\\n\\narXiv:1603.07285, 2016.\\n\\n[32] M. Krichen, \"Convolutional neural networks: A survey,\" Comput. , vol. 12, no. 8, pp. 151, 2023, doi:\\n\\n10.3390/computers12080151.\\n\\n[33] S. Kılıçarslan, K. Adem, and M. Çelik, \"An overview of the activation functions used in deep learning\\n\\nalgorithms,\" J. New Results Sci., vol. 10, no. 3, pp. 75-88, 2021, doi: 10.54187/jnrs.1011739.\\n\\n[34] C. Nwankpa, W. Ijomah, A. Gachagan, and S. Marshall, \"Activation functions: Comparison of trends\\n\\nin practice and research for deep learning,\" arXiv preprint arXiv:1811.03378, 2018.\\n\\n[35] K. Hara, D. Saito, and H. Shouno, \"Analysis of function of rectified linear unit used in deep learning,\"\\n\\nin Int. Jt. Conf. Neural. Netw. (IJCNN), Killarney, Ireland, 2015, pp. 1-8.\\n\\n[36] A. L. Maas, A. Y. Hannun, and A. Y. Ng, \"Rectifier nonlinearities improve neural network acoustic\\n\\nmodels,\" in Proc. Int. Conf. Mach. Learn. (ICML), 2013, vol. 30, no. 1: Atlanta, GA, USA, pp. 3.\\n\\n[37] K. He, X. Zhang, S. Ren, and J. Sun, \"Delving deep into rectifiers: Surpassing human-level performance\\n\\non imagenet classification,\" in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 1026-1034.\\n\\n[38] B. Xu, N. Wang, T. Chen, and M. Li, \"Empirical evaluation of rectified activations in convolutional\\n\\nnetwork,\" arXiv preprint arXiv:1505.00853, 2015.\\n\\n[39] X. Jin, C. Xu, J. Feng, Y. Wei, J. Xiong, and S. Yan, \"Deep learning with s-shaped rectified linear\\n\\nactivation units,\" in Proc. AAAI Conf. Artif. Intell., 2016, vol. 30, no. 1, doi: 10.1609/aaai.v30i1.10287.\\n\\n[40] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, \"Fast and accurate deep network learning by\\n\\nexponential linear units (elus),\" arXiv preprint arXiv:1511.07289, 2015.\\n\\n44\\n\\nF. M. Shiri et al.\\n\\n[41] D. Hendrycks and K. Gimpel, \"Gaussian error linear units (gelus),\" arXiv preprint arXiv:1606.08415,\\n\\n2016.\\n\\n[42] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural\\n\\nnetworks,\" Adv. Neural Inf. Process. Syst., vol. 25, 2012.\\n\\n[43] K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\"\\n\\narXiv preprint arXiv:1409.1556, 2014.\\n\\n[44] C. Szegedy et al., \"Going deeper with convolutions,\" in Proc. IEEE Conf. Comput. Vis. Pattern.\\n\\nRecognit., 2015, pp. 1-9.\\n\\n[45] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking the inception architecture for\\n\\ncomputer vision,\" in Proc. IEEE Conf. Comput. Vis. Pattern. Recognit., 2016, pp. 2818-2826.\\n\\n[46] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. IEEE/CVF\\n\\nConf. Comput. Vis. Pattern. Recognit., 2016, pp. 770-778.\\n\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, \"Identity mappings in deep residual networks,\" in Comput. Vis.\\n\\n(ECCV): 14th Europ. Conf., Amsterdam, The Netherlands, October 11–14 2016: Springer, pp. 630-645.\\n\\n[48] S. Zagoruyko and N. Komodakis, \"Wide residual networks,\" arXiv preprint arXiv:1605.07146, 2016.\\n\\n[49] G. Larsson, M. Maire, and G. Shakhnarovich, \"Fractalnet: Ultra-deep neural networks without\\n\\nresiduals,\" arXiv preprint arXiv:1605.07648, 2016.\\n\\n[50] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, \"SqueezeNet:\\n\\nAlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size,\" arXiv preprint\\n\\narXiv:1602.07360, 2016.\\n\\n[51] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, \"Inception-v4, inception-resnet and the impact of\\n\\nresidual connections on learning,\" in Proc. AAAI Conf. Artif. Intell., 2017, vol. 31, no. 1, doi:\\n\\n10.1609/aaai.v31i1.11231.\\n\\n[52] F. Chollet, \"Xception: Deep learning with depthwise separable convolutions,\" in Proc. IEEE Conf.\\n\\nComput. Vis. Pattern. Recognit., 2017, pp. 1251-1258.\\n\\n[53] A. G. Howard et al., \"Mobilenets: Efficient convolutional neural networks for mobile vision\\n\\napplications,\" arXiv preprint arXiv:1704.04861, 2017.\\n\\n[54] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \"Mobilenetv2: Inverted residuals and\\n\\nlinear bottlenecks,\" in Proc. IEEE Conf. Comput. Vis. Pattern. Recognit., 2018, pp. 4510-4520.\\n\\n[55] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, \"Densely connected convolutional\\n\\nnetworks,\" in Proc. IEEE Conf. Comput. Vis. Pattern. Recognit., 2017, pp. 4700-4708.\\n\\n[56] J. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in Proc. IEEE Conf. Comput. Vis.\\n\\nPattern. Recognit., 2018, pp. 7132-7141.\\n\\n[57] M. Tan and Q. Le, \"Efficientnet: Rethinking model scaling for convolutional neural networks,\" in Int.\\n\\nConf. Mach. Learn., 2019: PMLR, pp. 6105-6114.\\n\\n[58] M. Tan and Q. Le, \"Efficientnetv2: Smaller models and faster training,\" in Int. Conf. Mach. Learn.,\\n\\n2021: PMLR, pp. 10096-10106.\\n\\n[59] S. Abbaspour, F. Fotouhi, A. Sedaghatbaf, H. Fotouhi, M. Vahabi, and M. Linden, \"A Comparative\\n\\nAnalysis of Hybrid Deep Learning Models for Human Activity Recognition,\" Sens., vol. 20, no. 19,\\n\\n2020, doi: 10.3390/s20195707.\\n\\n45\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n[60] W. Fang, Y. Chen, and Q. Xue, \"Survey on research of RNN-based spatio-temporal sequence prediction\\n\\nalgorithms,\" J. Big. Data., vol. 3, no. 3, pp. 97, 2021, doi: 10.32604/jbd.2021.016993.\\n\\n[61] J. Xiao and Z. Zhou, \"Research progress of RNN language model,\" in 2020 IEEE Int. Conf. Artif. Intell.\\n\\nComput. App. (ICAICA), Dalian, China, 27-29 June 2020: IEEE, pp. 1285-1288, doi:\\n\\n10.1109/ICAICA50127.2020.9182390.\\n\\n[62] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici, \"Beyond\\n\\nshort snippets: Deep networks for video classification,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern.\\n\\nRecognit., 2015, pp. 4694-4702.\\n\\n[63] A. Shewalkar, D. Nyavanandi, and S. A. Ludwig, \"Performance evaluation of deep neural networks\\n\\napplied to speech recognition: RNN, LSTM and GRU,\" J. Artif. Intell. Soft Comput. Res., vol. 9, no. 4,\\n\\npp. 235-245, 2019, doi: 10.2478/jaiscr-2019-0006.\\n\\n[64] H. Apaydin, H. Feizi, M. T. Sattari, M. S. Colak, S. Shamshirband, and K.-W. Chau, \"Comparative\\n\\nanalysis of recurrent neural network architectures for reservoir inflow forecasting,\" Water., vol. 12, no.\\n\\n5, pp. 1500, 2020, doi: 10.3390/w12051500.\\n\\n[65] S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural. Comput., vol. 9, no. 8, pp. 1735-\\n\\n1780, 1997. MIT-Press.\\n\\n[66] A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke, and J. Schmidhuber, \"A novel\\n\\nconnectionist system for unconstrained handwriting recognition,\" IEEE Trans. Pattern. Anal. Mach.\\n\\nIntell., vol. 31, no. 5, pp. 855-868, 2008, doi: 10.1109/TPAMI.2008.137.\\n\\n[67] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \"Empirical evaluation of gated recurrent neural networks\\n\\non sequence modeling,\" arXiv preprint arXiv:1412.3555, 2014.\\n\\n[68] J. Chen, D. Jiang, and Y. Zhang, \"A hierarchical bidirectional GRU model with attention for EEG-based\\n\\nemotion\\n\\nclassification,\"\\n\\nIEEE Access.,\\n\\nvol.\\n\\n7,\\n\\npp.\\n\\n118530-118540,\\n\\n2019,\\n\\n10.1109/ACCESS.2019.2936817.\\n\\n[69] M. Fortunato, C. Blundell, and O. Vinyals, \"Bayesian recurrent neural networks,\" arXiv preprint\\n\\narXiv:1704.02798, 2017.\\n\\n[70] F. Kratzert, D. Klotz, C. Brenner, K. Schulz, and M. Herrnegger, \"Rainfall–runoff modelling using long\\n\\nshort-term memory (LSTM) networks,\" Hydrol. Earth Syst. Sci., vol. 22, no. 11, pp. 6005-6022, 2018,\\n\\ndoi: 10.5194/hess-22-6005-2018.\\n\\n[71] A. Graves, \"Generating sequences with recurrent neural networks,\" arXiv preprint arXiv:1308.0850,\\n\\n2013.\\n\\n[72] S. Minaee, E. Azimi, and A. Abdolrashidi, \"Deep-sentiment: Sentiment analysis using ensemble of cnn\\n\\nand bi-lstm models,\" arXiv preprint arXiv:1904.04206, 2019.\\n\\n[73] D. Gaur and S. Kumar Dubey, \"Development of Activity Recognition Model using LSTM-RNN Deep\\n\\nLearning Algorithm,\" J. inf. organ. sci., vol. 46, no. 2, pp. 277-291, 2022, doi: 10.31341/jios.46.2.1.\\n\\n[74] X. Zhu, P. Sobihani, and H. Guo, \"Long short-term memory over recursive structures,\" in Int. Conf.\\n\\nMach. Learn., 2015: PMLR, pp. 1604-1612.\\n\\n[75] F. Gu, M.-H. Chung, M. Chignell, S. Valaee, B. Zhou, and X. Liu, \"A survey on deep learning for\\n\\nhuman activity recognition,\" ACM Comput. Surv., vol. 54, no. 8, pp. 1-34, 2021, doi: 10.1145/3472290.\\n\\ndoi:\\n\\n46\\n\\nF. M. Shiri et al.\\n\\n[76] T. H. Aldhyani and H. Alkahtani, \"A bidirectional long short-term memory model algorithm for\\n\\npredicting COVID-19\\n\\nin gulf countries,\" Life., vol. 11, no. 11, pp. 1118, 2021, doi:\\n\\n10.3390/life11111118.\\n\\n[77] D. Liciotti, M. Bernardini, L. Romeo, and E. Frontoni, \"A sequential deep learning application for\\n\\nrecognising human activities in smart homes,\" Neurocomputing., vol. 396, pp. 501-513, 2020, doi:\\n\\n10.1016/j.neucom.2018.10.104.\\n\\n[78] A. Dutta, S. Kumar, and M. Basu, \"A gated recurrent unit approach to bitcoin price prediction,\" J. Risk\\n\\nFinancial Manag., vol. 13, no. 2, pp. 23, 2020, doi: 10.3390/jrfm13020023.\\n\\n[79] A. Gumaei, M. M. Hassan, A. Alelaiwi, and H. Alsalman, \"A Hybrid Deep Learning Model for Human\\n\\nActivity Recognition Using Multimodal Body Sensing Data,\" IEEE Access., vol. 7, pp. 99152-99160,\\n\\n2019, doi: 10.1109/access.2019.2927134.\\n\\n[80] D. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation by jointly learning to align and\\n\\ntranslate,\" arXiv preprint arXiv:1409.0473, 2014.\\n\\n[81] C. Chai et al., \"A Multifeature Fusion Short‐Term Traffic Flow Prediction Model Based on Deep\\n\\nLearnings,\" J. Adv. Transp., vol. 2022, no. 1, pp. 1702766, 2022, doi: 10.1155/2022/1702766.\\n\\n[82] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, \"Temporal convolutional networks for action\\n\\nsegmentation and detection,\" in Proc. IEEE Conf. Comput. Vis. Pattern. Recognit., 2017, pp. 156-165.\\n\\n[83] S. Bai, J. Z. Kolter, and V. Koltun, \"An empirical evaluation of generic convolutional and recurrent\\n\\nnetworks for sequence modeling,\" arXiv preprint arXiv:1803.01271, 2018.\\n\\n[84] Y. He and J. Zhao, \"Temporal convolutional networks for anomaly detection in time series,\" J. Phys.:\\n\\nConf. Ser., vol. 1213, no. 4, pp. 042050, 2019, doi: 10.1088/1742-6596/1213/4/042050.\\n\\n[85] J. Zhu, L. Su, and Y. Li, \"Wind power forecasting based on new hybrid model with TCN residual\\n\\nmodification,\" Energy AI., vol. 10, pp. 100199, 2022, doi: 10.1016/j.egyai.2022.100199\\n\\n[86] D. Li, F. Jiang, M. Chen, and T. Qian, \"Multi-step-ahead wind speed forecasting based on a hybrid\\n\\ndecomposition method and temporal convolutional networks,\" Energy., vol. 238, pp. 121981, 2022, doi:\\n\\n10.3390/en16093792.\\n\\n[87] X. Zhang, F. Dong, G. Chen, and Z. Dai, \"Advance prediction of coastal groundwater levels with\\n\\ntemporal convolutional and long short-term memory networks,\" Hydrol. Earth Syst. Sci., vol. 27, no. 1,\\n\\npp. 83-96, 2023, doi: 10.5194/hess-27-83-2023.\\n\\n[88] F. Yu and V. Koltun, \"Multi-scale context aggregation by dilated convolutions,\" arXiv preprint\\n\\narXiv:1511.07122, 2015.\\n\\n[89] T. Salimans and D. P. Kingma, \"Weight normalization: A simple reparameterization to accelerate\\n\\ntraining of deep neural networks,\" Adv. Neural Inf. Process. Syst., vol. 29, no. 29, 2016.\\n\\n[90] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: a simple way\\n\\nto prevent neural networks from overfitting,\" The journal of machine learning research, vol. 15, no. 1,\\n\\npp. 1929-1958, 2014.\\n\\n[91] Z. Liu et al., \"Kan: Kolmogorov-arnold networks,\" arXiv preprint arXiv:2404.19756, 2024.\\n\\n[92] J. Braun and M. Griebel, \"On a constructive proof of Kolmogorov’s superposition theorem,\" Constr.\\n\\nApprox., vol. 30, pp. 653-675, 2009, doi: 10.1007/s00365-009-9054-2.\\n\\n[93] A. D. Bodner, A. S. Tepsich, J. N. Spolski, and S. Pourteau, \"Convolutional Kolmogorov-Arnold\\n\\nNetworks,\" arXiv preprint arXiv:2406.13155, 2024.\\n\\n47\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n[94] R. Genet and H. Inzirillo, \"Tkan: Temporal kolmogorov-arnold networks,\" arXiv preprint\\n\\narXiv:2405.07344, 2024.\\n\\n[95] K. Pan, X. Zhang, and L. Chen, \"Research on the Training and Application Methods of a Lightweight\\n\\nAgricultural Domain-Specific Large Language Model Supporting Mandarin Chinese and Uyghur,\" Appl.\\n\\nSci., vol. 14, no. 13, pp. 5764, 2024, doi: 10.3390/app14135764.\\n\\n[96] R. Genet and H. Inzirillo, \"A Temporal Kolmogorov-Arnold Transformer for Time Series Forecasting,\"\\n\\narXiv preprint arXiv:2406.02486, 2024.\\n\\n[97] K. Xu, L. Chen, and S. Wang, \"Kolmogorov-Arnold Networks for Time Series: Bridging Predictive\\n\\nPower and Interpretability,\" arXiv preprint arXiv:2406.02496, 2024.\\n\\n[98] A. A. Aghaei, \"fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions,\"\\n\\narXiv preprint arXiv:2406.07456, 2024.\\n\\n[99] Z. Bozorgasl and H. Chen, \"Wav-kan: Wavelet kolmogorov-arnold networks,\" arXiv preprint\\n\\narXiv:2405.12832, 2024.\\n\\n[100]\\n\\nF. Zhang and X. Zhang, \"GraphKAN: Enhancing Feature Extraction with Graph Kolmogorov\\n\\nArnold Networks,\" arXiv preprint arXiv:2406.13597, 2024.\\n\\n[101]\\n\\nA. Jabbar, X. Li, and B. Omar, \"A survey on generative adversarial networks: Variants,\\n\\napplications, and training,\" ACM Comput. Surv., vol. 54, no. 8, pp. 1-49, 2021, doi: 10.1145/3463475.\\n\\n[102]\\n\\nD. Bank, N. Koenigstein, and R. Giryes, \"Autoencoders,\" in Machine learning for data science\\n\\nhandbook:data mining and knowledge discovery handbook: Springer, 2023, pp. 353-374.\\n\\n[103]\\n\\nI. Goodfellow et al., \"Generative adversarial nets,\" Adv. Neural. Inf. Process. Syst., vol. 27, pp.\\n\\n2672–2680, 2014.\\n\\n[104]\\n\\nN. Zhang, S. Ding, J. Zhang, and Y. Xue, \"An overview on restricted Boltzmann machines,\"\\n\\nNeurocomputing., vol. 275, pp. 1186-1199, 2018, doi: 10.1016/j.neucom.2017.09.065.\\n\\n[105]\\n\\nG. E. Hinton, \"Deep belief networks,\" Scholarpedia, vol. 4, no. 5, pp. 5947, 2009, doi:\\n\\n10.4249/scholarpedia.5947.\\n\\n[106]\\n\\nI. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016.\\n\\n[107]\\n\\nJ. Zhai, S. Zhang, J. Chen, and Q. He, \"Autoencoder and its various variants,\" in 2018 IEEE Int.\\n\\nConf. Syst. Man. Cybern. (SMC), Miyazaki, Japan, 7-10 Oct 2018: IEEE, pp. 415-419, doi:\\n\\n10.1109/SMC.2018.00080.\\n\\n[108]\\n\\nA. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, \"Adversarial autoencoders,\" arXiv\\n\\npreprint arXiv:1511.05644, 2015.\\n\\n[109]\\n\\nY. Wang, H. Yao, and S. Zhao, \"Auto-encoder based dimensionality\\n\\nreduction,\"\\n\\nNEUROCOMPUTING, vol. 184, pp. 232-242, 2016, doi: 10.1016/j.neucom.2015.08.104.\\n\\n[110]\\n\\nY. N. Kunang, S. Nurmaini, D. Stiawan, and A. Zarkasi, \"Automatic features extraction using\\n\\nautoencoder in intrusion detection system,\" in 2018 Int. Conf. Electr. engr. Compu. Sci. (ICECOS),\\n\\nPangkal, Indonesia, 2-4 Oct 2018: IEEE, pp. 219-224, doi: 10.1109/ICECOS.2018.8605181.\\n\\n[111]\\n\\nC. Zhou and R. C. Paffenroth, \"Anomaly detection with robust deep autoencoders,\" in Proc. 23rd\\n\\nACM SIGKDD\\n\\nInt. Conf. Knwl. Discov. Data Mining., 2017, pp. 665-674, doi:\\n\\n10.1145/3097983.3098052.\\n\\n[112]\\n\\nA. Creswell and A. A. Bharath, \"Denoising adversarial autoencoders,\" IEEE Trans. Neural Netw.\\n\\nLearn. Syst., vol. 30, no. 4, pp. 968-984, 2018, doi: 10.1109/TNNLS.2018.2852738.\\n\\n48\\n\\nF. M. Shiri et al.\\n\\n[113]\\n\\nD. P. Kingma and M. Welling, \"Auto-encoding variational bayes,\" arXiv preprint\\n\\narXiv:1312.6114, 2013.\\n\\n[114]\\n\\nA. Ng, \"Sparse autoencoder,\" CS294A Lecture notes, vol. 72, no. 2011, pp. 1-19, 2011.\\n\\n[115]\\n\\nS. Rifai et al., \"Higher order contractive auto-encoder,\" in Mach. Learn. Knwl. Discov. DB.: Europ.\\n\\nConf. ECML PKDD, Athens, Greece, September 5-9 2011: Springer, pp. 645-660.\\n\\n[116]\\n\\nP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, \"Extracting and composing robust\\n\\nfeatures with denoising autoencoders,\" in Proc. 25th Int. Conf. Mach. Learn., 2008, pp. 1096-1103,\\n\\ndoi: 10.1145/1390156.1390294.\\n\\n[117]\\n\\nD. P. Kingma and M. Welling, \"An introduction to variational autoencoders,\" Foundations and\\n\\nTrends® in Machine Learning, vol. 12, no. 4, pp. 307-392, 2019.\\n\\n[118]\\n\\nM.-Y. Liu and O. Tuzel, \"Coupled generative adversarial networks,\" Adv. Neural Inf. Process.\\n\\nSyst., vol. 29, 2016.\\n\\n[119]\\n\\nC. Wang, C. Xu, X. Yao, and D. Tao, \"Evolutionary generative adversarial networks,\" IEEE Trans.\\n\\nEvol. Comput., vol. 23, no. 6, pp. 921-934, 2019, doi: 10.1109/TEVC.2019.2895748.\\n\\n[120]\\n\\nA. Aggarwal, M. Mittal, and G. Battineni, \"Generative adversarial network: An overview of theory\\n\\nand applications,\" Int. J. Inf. Manag. Data Insights., vol. 1, no. 1, pp. 100004, 2021,\\n\\ndoi:10.1016/j.jjimei.2020.100004.\\n\\n[121]\\n\\nB.-C. Chen and A. Kae, \"Toward realistic image compositing with adversarial learning,\" in Proc.\\n\\nIEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2019, pp. 8415-8424.\\n\\n[122]\\n\\nD. P. Jaiswal, S. Kumar, and Y. Badr, \"Towards an artificial intelligence aided design approach:\\n\\napplication to anime faces with generative adversarial networks,\" Procedia Comput. Sci., vol. 168, pp.\\n\\n57-64, 2020, doi: 10.1016/j.procs.2020.02.257.\\n\\n[123]\\n\\nY. Liu, Q. Li, and Z. Sun, \"Attribute-aware face aging with wavelet-based generative adversarial\\n\\nnetworks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2019, pp. 11877-11886.\\n\\n[124]\\n\\nJ. Islam and Y. Zhang, \"GAN-based synthetic brain PET image generation,\" Brain Inform., vol. 7,\\n\\npp. 1-12, 2020, doi: 10.1186/s40708-020-00104-2.\\n\\n[125]\\n\\nH. Lan, A. D. N. Initiative, A. W. Toga, and F. Sepehrband, \"SC-GAN: 3D self-attention\\n\\nconditional GAN with spectral normalization for multi-modal neuroimaging synthesis,\" BioRxiv, pp.\\n\\n2020.06. 09.143297, 2020, doi: 10.1101/2020.06.09.143297.\\n\\n[126]\\n\\nK. A. Zhang, A. Cuesta-Infante, L. Xu, and K. Veeramachaneni, \"SteganoGAN: High capacity\\n\\nimage steganography with GANs,\" arXiv preprint arXiv:1901.03892, 2019.\\n\\n[127]\\n\\nS. Nam, Y. Kim, and S. J. Kim, \"Text-adaptive generative adversarial networks: manipulating\\n\\nimages with natural language,\" Adv. Neural Inf. Process. Syst., vol. 31, 2018.\\n\\n[128]\\n\\nL. Sixt, B. Wild, and T. Landgraf, \"Rendergan: Generating realistic labeled data,\" Front. Robot.\\n\\nAI. , vol. 5, pp. 66, 2018, doi: 10.3389/frobt.2018.00066.\\n\\n[129]\\n\\nK. Lin, D. Li, X. He, Z. Zhang, and M.-T. Sun, \"Adversarial ranking for language generation,\"\\n\\nAdv. Neural Inf. Process. Syst., vol. 30, 2017.\\n\\n[130]\\n\\nD. Xu, C. Wei, P. Peng, Q. Xuan, and H. Guo, \"GE-GAN: A novel deep learning framework for\\n\\nroad traffic state estimation,\" Transp. Res. Part C Emerg., vol. 117, pp. 102635, 2020, doi:\\n\\n10.1016/j.trc.2020.102635.\\n\\n49\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n[131]\\n\\nA. Clark, J. Donahue, and K. Simonyan, \"Adversarial video generation on complex datasets,\"\\n\\narXiv preprint arXiv:1907.06571, 2019.\\n\\n[132]\\n\\nE. L. Denton, S. Chintala, and R. Fergus, \"Deep generative image models using a laplacian\\n\\npyramid of adversarial networks,\" Adv. Neural Inf. Process. Syst., vol. 28, 2015.\\n\\n[133]\\n\\nC. Li and M. Wand, \"Precomputed real-time texture synthesis with markovian generative\\n\\nadversarial networks,\" in Comput. Vis. (ECCV): 14th Europ. Conf., Amsterdam, Netherlands, October\\n\\n11-14 2016: Springer, pp. 702-716, doi: 10.1007/978-3-319-46487-9_43.\\n\\n[134]\\n\\nL. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein, \"Unrolled generative adversarial networks,\"\\n\\narXiv preprint arXiv:1611.02163, 2016.\\n\\n[135]\\n\\nM. Arjovsky, S. Chintala, and L. Bottou, \"Wasserstein generative adversarial networks,\" in Int.\\n\\nConf. Mach. Learn., 2017: PMLR, pp. 214-223.\\n\\n[136]\\n\\nD. Berthelot, T. Schumm, and L. Metz, \"Began: Boundary equilibrium generative adversarial\\n\\nnetworks,\" arXiv preprint arXiv:1703.10717, 2017.\\n\\n[137]\\n\\nJ.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \"Unpaired image-to-image translation using cycle-\\n\\nconsistent adversarial networks,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 2223-2232.\\n\\n[138]\\n\\nT. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim, \"Learning to discover cross-domain relations with\\n\\ngenerative adversarial networks,\" in Int. Conf. Mach. Learn., 2017: PMLR, pp. 1857-1865.\\n\\n[139]\\n\\nA. Jolicoeur-Martineau, \"The relativistic discriminator: a key element missing from standard\\n\\nGAN,\" arXiv preprint arXiv:1807.00734, 2018.\\n\\n[140]\\n\\nT. Karras, S. Laine, and T. Aila, \"A style-based generator architecture for generative adversarial\\n\\nnetworks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2019, pp. 4401-4410.\\n\\n[141]\\n\\nG. Zhao, M. E. Meyerand, and R. M. Birn, \"Bayesian conditional GAN for MRI brain image\\n\\nsynthesis,\" arXiv preprint arXiv:2005.11875, 2020.\\n\\n[142]\\n\\nK. Chen, D. Zhang, L. Yao, B. Guo, Z. Yu, and Y. Liu, \"Deep learning for sensor-based human\\n\\nactivity recognition: Overview, challenges, and opportunities,\" ACM Comput. Surv., vol. 54, no. 4, pp.\\n\\n1-40, 2021, doi: 10.1145/3447744.\\n\\n[143]\\n\\nN. Alqahtani et al., \"Deep belief networks (DBN) with IoT-based alzheimer’s disease detection\\n\\nand classification,\" Appl. Sci., vol. 13, no. 13, pp. 7833, 2023, doi: 10.3390/app13137833.\\n\\n[144]\\n\\nA. P. Kale, R. M. Wahul, A. D. Patange, R. Soman, and W. Ostachowicz, \"Development of Deep\\n\\nbelief network for tool faults recognition,\" Sens., vol. 23, no. 4, pp. 1872, 2023, doi: 10.3390/s23041872.\\n\\n[145]\\n\\nE. Sansano, R. Montoliu, and O. Belmonte Fernandez, \"A study of deep neural networks for human\\n\\nactivity recognition,\" Comput. Intell., vol. 36, no. 3, pp. 1113-1139, 2020, doi: 10.1111/coin.12318.\\n\\n[146]\\n\\nA. Vaswani et al., \"Attention is all you need,\" Advances in neural information processing systems,\\n\\nvol. 30, 2017.\\n\\n[147]\\n\\nJ. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv preprint arXiv:1607.06450,\\n\\n2016.\\n\\n[148]\\n\\nK. Gavrilyuk, R. Sanford, M. Javan, and C. G. Snoek, \"Actor-transformers for group activity\\n\\nrecognition,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2020, pp. 839-848.\\n\\n[149]\\n\\nY. Tay, M. Dehghani, D. Bahri, and D. Metzler, \"Efficient transformers: A survey,\" ACM Comput.\\n\\nSurv., vol. 55, no. 6, pp. 1-28, 2022, doi: 10.24963/ijcai.2023/764.\\n\\n50\\n\\nF. M. Shiri et al.\\n\\n[150]\\n\\nG. Menghani, \"Efficient deep learning: A survey on making deep learning models smaller, faster,\\n\\nand better,\" ACM Comput. Surv., vol. 55, no. 12, pp. 1-37, 2023, doi: 10.1145/3578938.\\n\\n[151]\\n\\nY. Liu and L. Wu, \"Intrusion Detection Model Based on Improved Transformer,\" Applied Sciences,\\n\\nvol. 13, no. 10, pp. 6251, 2023.\\n\\n[152]\\n\\nD. Chen, S. Yongchareon, E. M. K. Lai, J. Yu, Q. Z. Sheng, and Y. Li, \"Transformer With\\n\\nBidirectional GRU for Nonintrusive, Sensor-Based Activity Recognition in a Multiresident\\n\\nEnvironment,\" IEEE Internet Things J., vol. 9, no. 23, pp. 23716-23727, 2022, doi:\\n\\n10.1109/jiot.2022.3190307.\\n\\n[153]\\n\\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training of deep bidirectional\\n\\ntransformers for language understanding,\" arXiv preprint arXiv:1810.04805, 2018.\\n\\n[154]\\n\\nA. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, \"Improving language understanding\\n\\nby generative pre-training,\" 2018.\\n\\n[155]\\n\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language models are\\n\\nunsupervised multitask learners,\" OpenAI blog, vol. 1, no. 8, pp. 9, 2019.\\n\\n[156]\\n\\nZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, \"Transformer-xl:\\n\\nAttentive language models beyond a fixed-length context,\" arXiv preprint arXiv:1901.02860, 2019.\\n\\n[157]\\n\\nZ. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, \"Xlnet: Generalized\\n\\nautoregressive pretraining for language understanding,\" Adv. Neural Inf. Process. Syst., vol. 32, 2019.\\n\\n[158]\\n\\nN. Shazeer, \"Fast transformer decoding: One write-head is all you need,\" arXiv preprint\\n\\narXiv:1911.02150, 2019.\\n\\n[159]\\n\\nY.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and R. Salakhutdinov, \"Multimodal\\n\\ntransformer for unaligned multimodal language sequences,\" in Proc. Conf. Assoc. Comput. Linguist.\\n\\nMtg., 2019, vol. 2019: NIH Public Access, p. 6558.\\n\\n[160]\\n\\nA. Dosovitskiy et al., \"An image is worth 16x16 words: Transformers for image recognition at\\n\\nscale,\" arXiv preprint arXiv:2010.11929, 2020.\\n\\n[161] W. Wang et al., \"Pyramid vision transformer: A versatile backbone for dense prediction without\\n\\nconvolutions,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2021, pp. 568-578.\\n\\n[162]\\n\\nZ. Liu et al., \"Swin transformer: Hierarchical vision transformer using shifted windows,\" in Proc.\\n\\nIEEE/CVF Int. Conf. Comput. Vis. , 2021, pp. 10012-10022.\\n\\n[163]\\n\\nL. Yuan et al., \"Tokens-to-token vit: Training vision transformers from scratch on imagenet,\" in\\n\\nProc. IEEE/CVF Int. Conf. Comput. Vis. , 2021, pp. 558-567.\\n\\n[164]\\n\\nK. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, \"Transformer in transformer,\" Adv. Neural.\\n\\nInf. Process. Syst., vol. 34, pp. 15908-15919, 2021.\\n\\n[165]\\n\\nK. Han, J. Guo, Y. Tang, and Y. Wang, \"Pyramidtnt: Improved transformer-in-transformer\\n\\nbaselines with pyramid architecture,\" arXiv preprint arXiv:2201.00978, 2022.\\n\\n[166] W. Fedus, B. Zoph, and N. Shazeer, \"Switch transformers: Scaling to trillion parameter models\\n\\nwith simple and efficient sparsity,\" J. Mach. Learn. Res. , vol. 23, no. 120, pp. 1-39, 2022.\\n\\n[167]\\n\\nZ. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, \"A convnet for the 2020s,\" in\\n\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 11976-\\n\\n11986.\\n\\n51\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n[168]\\n\\nJ. Zhang et al., \"Eatformer: Improving vision transformer inspired by evolutionary algorithm,\" Int.\\n\\nJ. Comput. Vis., pp. 1-28, 2024, doi: 10.1007/s11263-024-02034-6.\\n\\n[169]\\n\\nN. Vithayathil Varghese and Q. H. Mahmoud, \"A survey of multi-task deep reinforcement\\n\\nlearning,\" Electron., vol. 9, no. 9, pp. 1363, 2020, doi: 10.3390/electronics9091363.\\n\\n[170]\\n\\nN. Le, V. S. Rathour, K. Yamazaki, K. Luu, and M. Savvides, \"Deep reinforcement learning in\\n\\ncomputer vision: a comprehensive survey,\" Artif. Intell. Rev., pp. 1-87, 2022, doi: 10.1007/s10462-021-\\n\\n10061-9.\\n\\n[171]\\n\\nM. L. Puterman, Markov decision processes: discrete stochastic dynamic programming. John\\n\\nWiley & Sons, 2014.\\n\\n[172]\\n\\nZ. Zhang, D. Zhang, and R. C. Qiu, \"Deep reinforcement learning for power system applications:\\n\\nAn overview,\" CSEE J. Power Energy Syst., vol. 6, no. 1, pp. 213-225, 2019, doi:\\n\\n10.17775/CSEEJPES.2019.00920.\\n\\n[173]\\n\\nS. E. Li, \"Deep reinforcement learning,\" in Reinforcement learning for sequential decision and\\n\\noptimal control: Springer, 2023, pp. 365-402.\\n\\n[174]\\n\\nV. Mnih et al., \"Human-level control through deep reinforcement learning,\" NATURE, vol. 518,\\n\\nno. 7540, pp. 529-533, 2015, doi: 10.1038/nature14236.\\n\\n[175]\\n\\nH. Van Hasselt, A. Guez, and D. Silver, \"Deep reinforcement learning with double q-learning,\" in\\n\\nProc. AAAI Conf. Artif. Intell., 2016, vol. 30, no. 1, doi: 10.1609/aaai.v30i1.10295.\\n\\n[176]\\n\\nZ. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas, \"Dueling network\\n\\narchitectures for deep reinforcement learning,\" in Int. Conf. Mach. Learn., 2016: PMLR, pp. 1995-\\n\\n2003.\\n\\n[177]\\n\\nR. Coulom, \"Efficient selectivity and backup operators in Monte-Carlo tree search,\" in Comput.\\n\\nGam.: 5th Int. Conf., Turin, Italy, 2007: Springer, pp. 72-83.\\n\\n[178]\\n\\nN. Justesen, P. Bontrager, J. Togelius, and S. Risi, \"Deep learning for video game playing,\" IEEE\\n\\nTrans. Games., vol. 12, no. 1, pp. 1-20, 2019, doi: 10.1109/TG.2019.2896986.\\n\\n[179]\\n\\nK. Souchleris, G. K. Sidiropoulos, and G. A. Papakostas, \"Reinforcement learning in game\\n\\nindustry—Review, prospects and challenges,\" Appl. Sci., vol. 13, no. 4, pp. 2443, 2023, doi:\\n\\n10.3390/app13042443.\\n\\n[180]\\n\\nS. Gu, E. Holly, T. Lillicrap, and S. Levine, \"Deep reinforcement learning for robotic manipulation\\n\\nwith asynchronous off-policy updates,\" in 2017 IEEE Int. Conf. robot. autom. (ICRA), 2017: IEEE, pp.\\n\\n3389-3396.\\n\\n[181]\\n\\nD. Han, B. Mulyana, V. Stankovic, and S. Cheng, \"A survey on deep reinforcement learning\\n\\nalgorithms for robotic manipulation,\" Sens., vol. 23, no. 7, pp. 3762, 2023, doi: 10.3390/s23073762.\\n\\n[182]\\n\\nK. M. Lee, H. Myeong, and G. Song, \"SeedNet: Automatic Seed Generation with Deep\\n\\nReinforcement Learning for Robust Interactive Segmentation,\" in IEEE/CVF Conf. Comput. Vis.\\n\\nPattern. Recognit. (CVPR), Salt Lake City, UT, USA, 18-23 June 2018: IEEE Computer Society, pp.\\n\\n1760-1768, doi: 10.1109/cvpr.2018.00189.\\n\\n[183]\\n\\nH. Allioui et al., \"A multi-agent deep reinforcement learning approach for enhancement of\\n\\nCOVID-19 CT image segmentation,\" J. Pers. Med., vol. 12, no. 2, pp. 309, 2022, doi:\\n\\n10.3390/jpm12020309.\\n\\n52\\n\\nF. M. Shiri et al.\\n\\n[184]\\n\\nF. Sahba, \"Deep reinforcement learning for object segmentation in video sequences,\" in 2016 Int.\\n\\nConf. Comput. Sci. Comput. Intell. (CSCI), Las Vegas, NV, USA, 15-17 Dec 2016: IEEE, pp. 857-860,\\n\\ndoi: 10.1109/CSCI.2016.0166.\\n\\n[185]\\n\\nH. Liu et al., \"Learning to identify critical states for reinforcement learning from videos,\" in Proc.\\n\\nIEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2023, pp. 1955-1965.\\n\\n[186]\\n\\nA. Shojaeighadikolaei, A. Ghasemi, A. G. Bardas, R. Ahmadi, and M. Hashemi, \"Weather-Aware\\n\\nData-Driven Microgrid Energy Management Using Deep Reinforcement Learning,\" in 2021 North.\\n\\nAmerican. Power. Symp. (NAPS), College Station, TX, USA, 14-16 Nov 2021: IEEE, pp. 1-6, doi:\\n\\n10.1109/NAPS52732.2021.9654550.\\n\\n[187]\\n\\nB. Zhang, W. Hu, A. M. Ghias, X. Xu, and Z. Chen, \"Multi-agent deep reinforcement learning\\n\\nbased distributed control architecture for interconnected multi-energy microgrid energy management\\n\\nand\\n\\noptimization,\" Energy Conv. Manag.,\\n\\nvol.\\n\\n277,\\n\\npp.\\n\\n116647,\\n\\n2023,\\n\\ndoi:\\n\\n10.1016/j.enconman.2022.116647.\\n\\n[188]\\n\\nM. Long, H. Zhu, J. Wang, and M. I. Jordan, \"Deep transfer learning with joint adaptation\\n\\nnetworks,\" in Int. Conf. Mach. Learn., 2017: PMLR, pp. 2208-2217.\\n\\n[189]\\n\\nC. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, \"A survey on deep transfer learning,\" in\\n\\nArtif. Neural NET. Mach. Learn. ICANN 2018: 27th Int. Conf. Artif. Neural NET., Rhodes, Greece,\\n\\nOctober 4-7 2018: Springer, pp. 270-279, doi: 10.1007/978-3-030-01424-7_27.\\n\\n[190]\\n\\nF. Zhuang et al., \"A comprehensive survey on transfer learning,\" P IEEE, vol. 109, no. 1, pp. 43-\\n\\n76, 2020.\\n\\n[191]\\n\\nM. K. Rusia and D. K. Singh, \"A Color-Texture-Based Deep Neural Network Technique to Detect\\n\\nFace Spoofing Attacks,\" Cybern. Inf. Technol., vol. 22, no. 3, pp. 127-145, 2022, doi: 10.2478/cait-\\n\\n2022-0032.\\n\\n[192]\\n\\nY. Yao and G. Doretto, \"Boosting for transfer learning with multiple sources,\" in 2010 IEEE\\n\\nComput. Conf. Comput. socy. Vis. Pattern. Recognit., San Francisco, CA, USA, 13-18 June 2010: IEEE,\\n\\npp. 1855-1862, doi: 10.1109/CVPR.2010.5539857.\\n\\n[193]\\n\\nD. Pardoe and P. Stone, \"Boosting for regression transfer,\" in Proc. 27th Int. Conf. Mach. Learn.,\\n\\n2010, pp. 863-870.\\n\\n[194]\\n\\nE. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, \"Deep domain confusion: Maximizing\\n\\nfor domain invariance,\" arXiv preprint arXiv:1412.3474, 2014.\\n\\n[195]\\n\\nM. Long, Y. Cao, J. Wang, and M. Jordan, \"Learning transferable features with deep adaptation\\n\\nnetworks,\" in Int. Conf. Mach. Learn., 2015: PMLR, pp. 97-105.\\n\\n[196]\\n\\nM. Iman, H. R. Arabnia, and K. Rasheed, \"A review of deep transfer learning and recent\\n\\nadvancements,\" Technol., vol. 11, no. 2, pp. 40, 2023, doi: 10.3390/technologies11020040.\\n\\n[197]\\n\\nA. A. Rusu et al., \"Progressive neural networks,\" arXiv preprint arXiv:1606.04671, 2016.\\n\\n[198]\\n\\nY. Guo, J. Zhang, B. Sun, and Y. Wang, \"Adversarial Deep Transfer Learning in Fault Diagnosis:\\n\\nProgress, Challenges, and Future Prospects,\" Sens., vol. 23, no. 16, pp. 7263, 2023, doi:\\n\\n10.3390/s23167263.\\n\\n[199]\\n\\nY. Gulzar, \"Fruit image classification model based on MobileNetV2 with deep transfer learning\\n\\ntechnique,\" Sustain., vol. 15, no. 3, pp. 1906, 2023, doi: 10.3390/su15031906.\\n\\n53\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n[200]\\n\\nN. Kumar, M. Gupta, D. Gupta, and S. Tiwari, \"Novel deep transfer learning model for COVID-\\n\\n19 patient detection using X-ray chest images,\" J. Ambient Intell. Humaniz. Comput., vol. 14, no. 1, pp.\\n\\n469-478, 2023, doi: 10.1007/s12652-021-03306-6.\\n\\n[201]\\n\\nH. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, and F. Bensaali, \"Deep transfer learning for\\n\\nautomatic speech recognition: Towards better generalization,\" Knowl.-Based Syst., vol. 277, pp. 110851,\\n\\n2023, doi: 10.1016/j.knosys.2023.110851.\\n\\n[202]\\n\\nL. Yuan, T. Wang, G. Ferraro, H. Suominen, and M.-A. Rizoiu, \"Transfer learning for hate speech\\n\\ndetection in social media,\" Journal of Computational Social Science, vol. 6, no. 2, pp. 1081-1101, 2023.\\n\\n[203]\\n\\nA. Ray, M. H. Kolekar, R. Balasubramanian, and A. Hafiane, \"Transfer learning enhanced vision-\\n\\nbased human activity recognition: A decade-long analysis,\" Int. J. Inf. Manag. Data Insights. , vol. 3,\\n\\nno. 1, pp. 100142, 2023, doi: 10.1016/j.jjimei.2022.100142.\\n\\n[204]\\n\\nT. Kujani and V. D. Kumar, \"Head movements for behavior recognition from real time video based\\n\\non deep learning ConvNet transfer learning,\" J. Ambient Intell. Humaniz. Comput., vol. 14, no. 6, pp.\\n\\n7047-7061, 2023, doi: 10.1007/s12652-021-03558-2.\\n\\n[205]\\n\\nA. Maity, A. Pathak, and G. Saha, \"Transfer learning based heart valve disease classification from\\n\\nPhonocardiogram signal,\" Biomed. Signal Process. Control. , vol. 85, pp. 104805, 2023, doi:\\n\\n10.1016/j.bspc.2023.104805.\\n\\n[206]\\n\\nK. Rezaee, S. Savarkar, X. Yu, and J. Zhang, \"A hybrid deep transfer learning-based approach for\\n\\nParkinson\\'s disease classification in surface electromyography signals,\" Biomed. Signal Process.\\n\\nControl., vol. 71, pp. 103161, 2022, doi: 10.1016/j.bspc.2021.103161.\\n\\n[207]\\n\\nB. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, \"Learning transferable architectures for scalable\\n\\nimage recognition,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2018, pp. 8697-8710.\\n\\n[208]\\n\\nY. Zhang et al., \"Deep learning in food category recognition,\" Inf. Fusion., vol. 98, pp. 101859,\\n\\n2023, doi: 10.1016/j.inffus.2023.101859.\\n\\n[209]\\n\\nE. Ramanujam and T. Perumal, \"MLMO-HSM: Multi-label Multi-output Hybrid Sequential\\n\\nModel for multi-resident smart home activity recognition,\" J. Ambient Intell. Humaniz. Comput., vol.\\n\\n14, no. 3, pp. 2313-2325, 2023, doi: 10.1007/s12652-022-04487-4.\\n\\n[210]\\n\\nM. Ren, X. Liu, Z. Yang, J. Zhang, Y. Guo, and Y. Jia, \"A novel forecasting based scheduling\\n\\nmethod for household energy management system based on deep reinforcement learning,\" Sustain.\\n\\nCities Soc., vol. 76, pp. 103207, 2022, doi: 10.1016/j.scs.2021.103207.\\n\\n[211]\\n\\nS. M. Abdullah et al., \"Optimizing traffic flow in smart cities: Soft GRU-based recurrent neural\\n\\nnetworks for enhanced congestion prediction using deep learning,\" Sustain., vol. 15, no. 7, pp. 5949,\\n\\n2023, doi: 10.3390/su15075949.\\n\\n[212]\\n\\nM. I. B. Ahmed et al., \"Deep learning approach to recyclable products classification: Towards\\n\\nsustainable waste management,\" Sustain., vol. 15, no. 14, pp. 11138, 2023, doi: 10.3390/su151411138.\\n\\n[213]\\n\\nC. Zeng, C. Ma, K. Wang, and Z. Cui, \"Parking occupancy prediction method based on multi\\n\\nfactors and stacked GRU-LSTM,\" IEEE Access., vol. 10, pp. 47361-47370, 2022, doi:\\n\\n10.1109/ACCESS.2022.3171330.\\n\\n[214]\\n\\nN. K. Mehta, S. S. Prasad, S. Saurav, R. Saini, and S. Singh, \"Three-dimensional DenseNet self-\\n\\nattention neural network for automatic detection of student’s engagement,\" Appl. Intell., vol. 52, no. 12,\\n\\npp. 13803-13823, 2022, doi: 10.1007/s10489-022-03200-4.\\n\\n54\\n\\nF. M. Shiri et al.\\n\\n[215]\\n\\nA. K. Shukla, A. Shukla, and R. Singh, \"Automatic attendance system based on CNN–LSTM and\\n\\nface recognition,\" Int. J. Inf. Technol., vol. 16, no. 3, pp. 1293-1301, 2024, doi: 10.1007/s41870-023-\\n\\n01495-1.\\n\\n[216]\\n\\nB. Rajalakshmi, V. K. Dandu, S. L. Tallapalli, and H. Karanwal, \"ACE: Automated Exam Control\\n\\nand E-Proctoring System Using Deep Face Recognition,\" in 2023 Int. Conf. Circuit. Power. Comput.\\n\\nTechnol.\\n\\n(ICCPCT), Kollam,\\n\\nIndia,\\n\\n10-11 Aug\\n\\n2023:\\n\\nIEEE,\\n\\npp.\\n\\n301-306,\\n\\ndoi:\\n\\n10.1109/ICCPCT58313.2023.10245126.\\n\\n[217]\\n\\nI. Pacal, \"MaxCerVixT: A novel lightweight vision transformer-based Approach for precise\\n\\ncervical cancer detection,\" Knowl.-Based Syst.\\n\\n, vol. 289, pp. 111482, 2024, doi:\\n\\n10.1016/j.knosys.2024.111482.\\n\\n[218]\\n\\nM. M. Rana et al., \"A robust and clinically applicable deep learning model for early detection of\\n\\nAlzheimer\\'s,\" IET Image Process., vol. 17, no. 14, pp. 3959-3975, 2023, doi: 10.1049/ipr2.12910.\\n\\n[219]\\n\\nS. Vimal, Y. H. Robinson, S. Kadry, H. V. Long, and Y. Nam, \"IoT based smart health monitoring\\n\\nwith CNN using edge computing,\" J. Internet Technol., vol. 22, no. 1, pp. 173-185, 2021, doi:\\n\\n10.3966/160792642021012201017.\\n\\n[220]\\n\\nT. S. Johnson et al., \"Diagnostic Evidence GAuge of Single cells (DEGAS): a flexible deep\\n\\ntransfer learning framework for prioritizing cells in relation to disease,\" Genome Med., vol. 14, no. 1,\\n\\npp. 11, 2022, doi: 10.1186/s13073-022-01012-2.\\n\\n[221] W. Zheng, S. Lu, Z. Cai, R. Wang, L. Wang, and L. Yin, \"PAL-BERT: an improved question\\n\\nanswering model,\" Comput. Model. engr. Sci., pp. 1-10, 2023, doi: 10.32604/cmes.2023.046692.\\n\\n[222]\\n\\nF. Wang et al., \"TEDT: transformer-based encoding–decoding translation network for multimodal\\n\\nsentiment analysis,\" Cogn. Comput., vol. 15, no. 1, pp. 289-303, 2023, doi: 10.1007/s12559-022-10073-\\n\\n9.\\n\\n[223]\\n\\nM. Nafees Muneera and P. Sriramya, \"An enhanced optimized abstractive text summarization\\n\\ntraditional approach employing multi-layered attentional stacked LSTM with the attention RNN,\" in\\n\\nComput. Vis. Mach. Intell. Paradigm., 2023: Springer, pp. 303-318, doi: 10.1007/978-981-19-7169-\\n\\n3_28.\\n\\n[224]\\n\\nM. A. Uddin, M. S. Uddin Chowdury, M. U. Khandaker, N. Tamam, and A. Sulieman, \"The\\n\\nEfficacy of Deep Learning-Based Mixed Model for Speech Emotion Recognition,\" Comput. Mater.\\n\\nContin., vol. 74, no. 1, 2023, doi: 10.32604/cmc.2023.031177.\\n\\n[225]\\n\\nM. De Silva and D. Brown, \"Multispectral Plant Disease Detection with Vision Transformer–\\n\\nConvolutional Neural Network Hybrid Approaches,\" Sens., vol. 23, no. 20, pp. 8531, 2023, doi:\\n\\n10.3390/s23208531.\\n\\n[226]\\n\\nT. Akilan and K. Baalamurugan, \"Automated weather forecasting and field monitoring using\\n\\nGRU-CNN model along with IoT to support precision agriculture,\" Expert Syst. Appl, vol. 249, pp.\\n\\n123468, 2024, doi: 10.1016/j.eswa.2024.123468.\\n\\n[227]\\n\\nR. Benameur, A. Dahane, B. Kechar, and A. E. H. Benyamina, \"An Innovative Smart and\\n\\nSustainable Low-Cost Irrigation System for Anomaly Detection Using Deep Learning,\" Sens., vol. 24,\\n\\nno. 4, pp. 1162, 2024, doi: 10.3390/s24041162.\\n\\n55\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n[228]\\n\\nM. Hosseinpour-Zarnaq, M. Omid, F. Sarmadian, and H. Ghasemi-Mobtaker, \"A CNN model for\\n\\npredicting soil properties using VIS–NIR spectral data,\" Environ. Earth. Sci., vol. 82, no. 16, pp. 382,\\n\\n2023, doi: 10.1007/s12665-023-11073-0.\\n\\n[229]\\n\\nM. Shakeel, K. Itoyama, K. Nishida, and K. Nakadai, \"Detecting earthquakes: a novel deep\\n\\nlearning-based approach for effective disaster response,\" Appl. Intell., vol. 51, no. 11, pp. 8305-8315,\\n\\n2021, doi: 10.1007/s10489-021-02285-7.\\n\\n[230]\\n\\nY. Zhang, Z. Zhou, J. Van Griensven Thé, S. X. Yang, and B. Gharabaghi, \"Flood Forecasting\\n\\nUsing Hybrid LSTM and GRU Models with Lag Time Preprocessing,\" Water, vol. 15, no. 22, pp. 3982,\\n\\n2023, doi: 10.3390/w15223982.\\n\\n[231]\\n\\nH. Xu and H. Wu, \"Accurate tsunami wave prediction using long short-term memory based neural\\n\\nnetworks,\" Ocean Model., vol. 186, pp. 102259, 2023, doi: 10.1016/j.ocemod.2023.102259.\\n\\n[232]\\n\\nJ. Yao, B. Zhang, C. Li, D. Hong, and J. Chanussot, \"Extended vision transformer (ExViT) for\\n\\nland use and land cover classification: A multimodal deep learning framework,\" IEEE Trans. Geosci.\\n\\nRemote Sens., vol. 61, pp. 1-15, 2023, doi: 10.1109/TGRS.2023.3284671.\\n\\n[233]\\n\\nA. Y. Cho, S.-e. Park, D.-j. Kim, J. Kim, C. Li, and J. Song, \"Burned area mapping using\\n\\nUnitemporal Planetscope imagery with a deep learning based approach,\" IEEE J. Sel. Top. Appl. Earth\\n\\nObs. Remote Sens., vol. 16, pp. 242-253, 2022, doi: 10.1109/JSTARS.2022.3225070.\\n\\n[234]\\n\\nM. Alshehri, A. Ouadou, and G. J. Scott, \"Deep Transformer-based Network Deforestation\\n\\nDetection in the Brazilian Amazon Using Sentinel-2 Imagery,\" IEEE Geosci. Remote Sens. Lett., 2024,\\n\\ndoi: 10.1109/LGRS.2024.3355104.\\n\\n[235]\\n\\nV. Hnamte and J. Hussain, \"DCNNBiLSTM: An efficient hybrid deep learning-based intrusion\\n\\ndetection system,\" Telemat. Inform. Rep., vol. 10, pp. 100053, 2023, doi: 10.1016/j.teler.2023.100053.\\n\\n[236]\\n\\nE. S. Alomari et al., \"Malware detection using deep learning and correlation-based feature\\n\\nselection,\" Symmetry., vol. 15, no. 1, pp. 123, 2023, doi: 10.3390/sym15010123.\\n\\n[237]\\n\\nZ. Alshingiti, R. Alaqel, J. Al-Muhtadi, Q. E. U. Haq, K. Saleem, and M. H. Faheem, \"A deep\\n\\nlearning-based phishing detection system using CNN, LSTM, and LSTM-CNN,\" Electron., vol. 12, no.\\n\\n1, pp. 232, 2023, doi: 10.3390/electronics12010232.\\n\\n[238]\\n\\nH. Fanai and H. Abbasimehr, \"A novel combined approach based on deep Autoencoder and deep\\n\\nclassifiers for credit card fraud detection,\" Expert Syst. Appl., vol. 217, pp. 119562, 2023, doi:\\n\\n10.1016/j.eswa.2023.119562.\\n\\n[239]\\n\\nR. A. Joshi and N. Sambre, \"Personalized CNN Architecture for Advanced Multi-Modal Biometric\\n\\nAuthentication,\" in 2024 Int. Conf. Invent. Comput. Technol. (ICICT), Lalitpur, Nepal, 24-26 April 2024:\\n\\nIEEE, pp. 890-894, doi: 10.1109/ICICT60155.2024.10544987.\\n\\n[240]\\n\\nJ. Sohafi-Bonab, M. H. Aghdam, and K. Majidzadeh, \"DCARS: Deep context-aware\\n\\nrecommendation system based on session latent context,\" Appl. Soft Comput., vol. 143, pp. 110416,\\n\\n2023, doi: 10.1016/j.asoc.2023.110416.\\n\\n[241]\\n\\nJ. Duan, P.-F. Zhang, R. Qiu, and Z. Huang, \"Long short-term enhanced memory for sequential\\n\\nrecommendation,\" World. Wide. Web., vol. 26, no. 2, pp. 561-583, 2023, doi: 10.1007/s11280-022-\\n\\n01056-9.\\n\\n56\\n\\nF. M. Shiri et al.\\n\\n[242]\\n\\nP. Mondal, D. Chakder, S. Raj, S. Saha, and N. Onoe, \"Graph convolutional neural network for\\n\\nmultimodal movie recommendation,\" in Proc. 38th ACM/SIGAPP Symp. Appl. Comput., 2023, pp.\\n\\n1633-1640, doi: 10.1145/3555776.3577853.\\n\\n[243]\\n\\nZ. Liu, \"Prediction Model of E-commerce Users\\' Purchase Behavior Based on Deep Learning,\"\\n\\nFront. Bus. Econ. Manag., vol. 15, no. 2, pp. 147-149, 2024, doi: 10.54097/p22ags78.\\n\\n[244]\\n\\nS. Deng, R. Li, Y. Jin, and H. He, \"CNN-based feature cross and classifier for loan default\\n\\nprediction,\" in 2020 Int. Conf. Image. video. Process. Artif. Intell., 2020, vol. 11584: SPIE, pp. 368-\\n\\n373.\\n\\n[245]\\n\\nC. Han and X. Fu, \"Challenge and opportunity: deep learning-based stock price prediction by using\\n\\nBi-directional LSTM model,\" Front. Bus. Econ. Manag., vol. 8, no. 2, pp. 51-54, 2023, doi:\\n\\n10.54097/fbem.v8i2.6616.\\n\\n[246]\\n\\nY. Cao, C. Li, Y. Peng, and H. Ru, \"MCS-YOLO: A multiscale object detection method for\\n\\nautonomous driving road environment recognition,\" IEEE Access., vol. 11, pp. 22342-22354, 2023, doi:\\n\\n10.1109/ACCESS.2023.3252021.\\n\\n[247]\\n\\nD. K. Jain, X. Zhao, G. González-Almagro, C. Gan, and K. Kotecha, \"Multimodal pedestrian\\n\\ndetection using metaheuristics with deep convolutional neural network in crowded scenes,\" Inf. Fusion.,\\n\\nvol. 95, pp. 401-414, 2023, doi: 10.1016/j.inffus.2023.02.014.\\n\\n[248]\\n\\nS. Sindhu and M. Saravanan, \"An optimised extreme learning machine (OELM) for simultaneous\\n\\nlocalisation and mapping in autonomous vehicles,\" Int. J. Syst. Syst. Eng., vol. 13, no. 2, pp. 140-159,\\n\\n2023, doi: 10.1504/IJSSE.2023.131231.\\n\\n[249]\\n\\nG. Singal, H. Singhal, R. Kushwaha, V. Veeramsetty, T. Badal, and S. Lamba, \"RoadWay: lane\\n\\ndetection for autonomous driving vehicles via deep learning,\" Multimed. Tools Appl., vol. 82, no. 4, pp.\\n\\n4965-4978, 2023, doi: 10.1007/s11042-022-12171-0.\\n\\n[250]\\n\\nH. Shang, C. Sun, J. Liu, X. Chen, and R. Yan, \"Defect-aware transformer network for intelligent\\n\\nvisual surface defect detection,\" Adv. Eng.\\n\\nInform., vol. 55, pp. 101882, 2023, doi:\\n\\n10.1016/j.aei.2023.101882.\\n\\n[251]\\n\\nT. Zonta, C. A. Da Costa, F. A. Zeiser, G. de Oliveira Ramos, R. Kunst, and R. da Rosa Righi, \"A\\n\\npredictive maintenance model for optimizing production schedule using deep neural networks,\" J.\\n\\nManuf. Syst., vol. 62, pp. 450-462, 2022, doi: 10.1016/j.jmsy.2021.12.013.\\n\\n[252]\\n\\nZ. He, K.-P. Tran, S. Thomassey, X. Zeng, J. Xu, and C. Yi, \"A deep reinforcement learning based\\n\\nmulti-criteria decision support system for optimizing textile chemical process,\" Comput. Ind., vol. 125,\\n\\npp. 103373, 2021, doi: 10.1016/j.compind.2020.103373.\\n\\n[253]\\n\\nM. Pacella and G. Papadia, \"Evaluation of deep learning with long short-term memory networks\\n\\nfor time series forecasting in supply chain management,\" PROC CIRP, vol. 99, pp. 604-609, 2021, doi:\\n\\n10.1016/j.procir.2021.03.081.\\n\\n[254]\\n\\nP. Shukla, H. Kumar, and G. C. Nandi, \"Robotic grasp manipulation using evolutionary computing\\n\\nand deep reinforcement learning,\" Intell. Serv. Robot., vol. 14, no. 1, pp. 61-77, 2021, doi:\\n\\n10.1007/s11370-020-00342-7.\\n\\n[255]\\n\\nK. Kamali, I. A. Bonev, and C. Desrosiers, \"Real-time motion planning for robotic teleoperation\\n\\nusing dynamic-goal deep reinforcement learning,\" in 2020 17th Conf. Comput. Robot. Vis. (CRV), 13-\\n\\n15 May 2020: IEEE, pp. 182-189, doi: 10.1109/CRV50864.2020.00032.\\n\\n57\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n[256]\\n\\nJ. Zhang, H. Liu, Q. Chang, L. Wang, and R. X. Gao, \"Recurrent neural network for motion\\n\\ntrajectory prediction in human-robot collaborative assembly,\" CIRP annals, vol. 69, no. 1, pp. 9-12,\\n\\n2020, doi: 10.1016/j.cirp.2020.04.077.\\n\\n[257]\\n\\nB. K. Iwana and S. Uchida, \"An empirical survey of data augmentation for time series\\n\\nclassification with neural networks,\" PLOS ONE, vol. 16, no. 7, pp. e0254841, 2021, doi:\\n\\n10.1371/journal.pone.0254841.\\n\\n[258]\\n\\nC. Khosla and B. S. Saini, \"Enhancing performance of deep learning models with different data\\n\\naugmentation techniques: A survey,\" in 2020 Int. Conf. Intell. engr. Mgmt. (ICIEM), London, UK, 17-\\n\\n19 June 2020: IEEE, pp. 79-85, doi: 10.1109/ICIEM48762.2020.9160048.\\n\\n[259]\\n\\nM. Paschali, W. Simson, A. G. Roy, R. Göbl, C. Wachinger, and N. Navab, \"Manifold exploring\\n\\ndata augmentation with geometric transformations for increased performance and robustness,\" in Inf.\\n\\nProcess. Medical. Image.: 26th Int. Conf., IPMI 2019, Hong Kong, China, June 2–7 2019: Springer, pp.\\n\\n517-529.\\n\\n[260]\\n\\nH. Guo, Y. Mao, and R. Zhang, \"Augmenting data with mixup for sentence classification: An\\n\\nempirical study,\" arXiv preprint arXiv:1905.08941, 2019.\\n\\n[261]\\n\\nO. O. Abayomi-Alli, R. Damaševičius, A. Qazi, M. Adedoyin-Olowe, and S. Misra, \"Data\\n\\naugmentation and deep learning methods in sound classification: A systematic review,\" Electro., vol.\\n\\n11, no. 22, pp. 3795, 2022, doi: 10.3390/electronics11223795.\\n\\n[262]\\n\\nT.-H. Cheung and D.-Y. Yeung, \"Modals: Modality-agnostic automated data augmentation in the\\n\\nlatent space,\" in Int. Conf. Learn. Represen., 2020.\\n\\n[263]\\n\\nC. Shorten, T. M. Khoshgoftaar, and B. Furht, \"Text data augmentation for deep learning,\" J. Big\\n\\nData, vol. 8, no. 1, pp. 101, 2021, doi: 10.1186/s40537-021-00492-0.\\n\\n[264]\\n\\nF. Wang, H. Wang, H. Wang, G. Li, and G. Situ, \"Learning from simulation: An end-to-end deep-\\n\\nlearning approach for computational ghost imaging,\" Opt. Express, vol. 27, no. 18, pp. 25560-25572,\\n\\n2019, doi: 10.1364/OE.27.025560.\\n\\n[265]\\n\\nK. Ghosh, C. Bellinger, R. Corizzo, P. Branco, B. Krawczyk, and N. Japkowicz, \"The class\\n\\nimbalance problem in deep learning,\" Mach. Learn., vol. 113, no. 7, pp. 4845-4901, 2024, doi:\\n\\n10.1007/s10994-022-06268-8.\\n\\n[266]\\n\\nD. Singh, E. Merdivan, J. Kropf, and A. Holzinger, \"Class imbalance in multi-resident activity\\n\\nrecognition: an evaluative study on explainability of deep learning approaches,\" Univers. Access. Inf.\\n\\nSoc., pp. 1-19, 2024, doi: 10.1007/s10209-024-01123-0.\\n\\n[267]\\n\\nA. S. Tarawneh, A. B. Hassanat, G. A. Altarawneh, and A. Almuhaimeed, \"Stop oversampling for\\n\\nclass imbalance learning: A review,\" IEEE ACCESS, vol. 10, pp. 47643-47660, 2022, doi:\\n\\n10.1109/ACCESS.2022.3169512.\\n\\n[268]\\n\\nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, \"SMOTE: synthetic minority\\n\\nover-sampling technique,\" J. Artif. Intell. Res., vol. 16, pp. 321-357, 2002, doi: 10.1613/jair.953.\\n\\n[269]\\n\\nH. Han, W.-Y. Wang, and B.-H. Mao, \"Borderline-SMOTE: a new over-sampling method in\\n\\nimbalanced data sets learning,\" in Int. Conf. Intell. Comput., 2005: Springer, pp. 878-887.\\n\\n[270]\\n\\nH. He, Y. Bai, E. A. Garcia, and S. Li, \"ADASYN: Adaptive synthetic sampling approach for\\n\\nimbalanced learning,\" in 2008 Int. Jt. Conf. Neural. Netw., 2008: IEEE, pp. 1322-1328.\\n\\n58\\n\\nF. M. Shiri et al.\\n\\n[271]\\n\\nY. Tang, Y.-Q. Zhang, N. V. Chawla, and S. Krasser, \"SVMs modeling for highly imbalanced\\n\\nclassification,\" IEEE Trans. Syst. Man. Cybern., Part B (Cybernetics), vol. 39, no. 1, pp. 281-288, 2008,\\n\\ndoi: 10.1109/TSMCB.2008.2002909.\\n\\n[272]\\n\\nS. Barua, M. M. Islam, X. Yao, and K. Murase, \"MWMOTE--majority weighted minority\\n\\noversampling technique for imbalanced data set learning,\" IEEE Trans. Knowl. Data Eng., vol. 26, no.\\n\\n2, pp. 405-425, 2012, doi: 10.2478/cait-2022-0035.\\n\\n[273]\\n\\nC. Bellinger, S. Sharma, N. Japkowicz, and O. R. Zaïane, \"Framework for extreme imbalance\\n\\nclassification: SWIM—sampling with the majority class,\" Knowl. Inf. Syst., vol. 62, pp. 841-866, 2020,\\n\\ndoi: 10.1007/s10115-019-01380-z.\\n\\n[274]\\n\\nR. Das, S. K. Biswas, D. Devi, and B. Sarma, \"An oversampling technique by integrating reverse\\n\\nnearest neighbor in SMOTE: Reverse-SMOTE,\" in 2020 Int. Conf. Smart. Electron. Commun.\\n\\n(ICOSEC), 2020: IEEE, pp. 1239-1244.\\n\\n[275]\\n\\nC. Liu et al., \"Constrained oversampling: An oversampling approach to reduce noise generation\\n\\nin imbalanced datasets with class overlapping,\" IEEE ACCESS, vol. 10, pp. 91452-91465, 2020, doi:\\n\\n10.1109/ACCESS.2020.3018911.\\n\\n[276]\\n\\nA. S. Tarawneh, A. B. Hassanat, K. Almohammadi, D. Chetverikov, and C. Bellinger, \"Smotefuna:\\n\\nSynthetic minority over-sampling technique based on furthest neighbour algorithm,\" IEEE ACCESS,\\n\\nvol. 8, pp. 59069-59082, 2020, doi: 10.1109/ACCESS.2020.2983003.\\n\\n[277]\\n\\nX.-Y. Liu, J. Wu, and Z.-H. Zhou, \"Exploratory undersampling for class-imbalance learning,\"\\n\\nIEEE Trans. Syst. Man. Cybern., Part B (Cybernetics), vol. 39, no. 2, pp. 539-550, 2008, doi:\\n\\n10.1109/TSMCB.2008.2007853.\\n\\n[278]\\n\\nM. A. Tahir, J. Kittler, and F. Yan, \"Inverse random under sampling for class imbalance problem\\n\\nand its application to multi-label classification,\" Pattern. Recognit., vol. 45, no. 10, pp. 3738-3750, 2012,\\n\\ndoi: 10.1016/j.patcog.2012.03.014.\\n\\n[279]\\n\\nV. Babar and R. Ade, \"A novel approach for handling imbalanced data in medical diagnosis using\\n\\nundersampling technique,\" Commun. Appl. Electron., vol. 5, no. 7, pp. 36-42, 2016.\\n\\n[280]\\n\\nZ. H. Zhou and X. Y. Liu, \"On multi‐class cost‐sensitive learning,\" Comput. Intell., vol. 26, no.\\n\\n3, pp. 232-257, 2010, doi: 10.1111/j.1467-8640.2010.00358.x.\\n\\n[281]\\n\\nC. X. Ling and V. S. Sheng, \"Cost-sensitive learning and the class imbalance problem,\" ency.\\n\\nMach. Learn., vol. 2011, pp. 231-235, 2008.\\n\\n[282]\\n\\nN. Seliya, A. Abdollah Zadeh, and T. M. Khoshgoftaar, \"A literature review on one-class\\n\\nclassification and its potential applications in big data,\" J. Big Data, vol. 8, pp. 1-31, 2021, doi:\\n\\n10.1186/s40537-021-00514-x.\\n\\n[283]\\n\\nV. S. Spelmen and R. Porkodi, \"A review on handling imbalanced data,\" in Int. Conf. Curr. Trend.\\n\\nToward. Converg. Technol. (ICCTCT), Coimbatore, India, 1-3 March 2018: IEEE, pp. 1-11, doi:\\n\\n10.1109/ICCTCT.2018.8551020.\\n\\n[284]\\n\\nG. Zhang, C. Wang, B. Xu, and R. Grosse, \"Three mechanisms of weight decay regularization,\"\\n\\narXiv preprint arXiv:1810.12281, 2018.\\n\\n[285]\\n\\nC. Laurent, G. Pereyra, P. Brakel, Y. Zhang, and Y. Bengio, \"Batch normalized recurrent neural\\n\\nnetworks,\" in 2016 IEEE Int. Conf. Acoust. Speech. Signal. Process. (ICASSP), Shanghai, China, 20-25\\n\\nMarch 2016: IEEE, pp. 2657-2661, doi: 10.1109/ICASSP.2016.7472159.\\n\\n59\\n\\nA Comprehensive Overview and Comparative Analysis on Deep Learning Models\\n\\n[286]\\n\\nS. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing\\n\\ninternal covariate shift,\" in Int. Conf. Mach. Learn., 2015: pmlr, pp. 448-456.\\n\\n[287]\\n\\nG. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, and G. Hinton, \"Regularizing neural networks by\\n\\npenalizing confident output distributions,\" arXiv preprint arXiv:1701.06548, 2017.\\n\\n[288]\\n\\nG. E. Dahl, T. N. Sainath, and G. E. Hinton, \"Improving deep neural networks for LVCSR using\\n\\nrectified linear units and dropout,\" in IEEE Int. Conf. Acoust. Speech. Signal. Process., 2013: IEEE, pp.\\n\\n8609-8613.\\n\\n[289]\\n\\nX. Glorot and Y. Bengio, \"Understanding the difficulty of training deep feedforward neural\\n\\nnetworks,\" in Proc. 13 Int. Conf. Artif. Intell. Stats., 2010: JMLR Workshop and Conference\\n\\nProceedings, pp. 249-256.\\n\\n[290]\\n\\nG. Srivastava, S. Vashisth, I. Dhall, and S. Saraswat, \"Behavior analysis of a deep feedforward\\n\\nneural network by varying the weight initialization methods,\" in Smart Innov. Commun. Comput. Sci.:\\n\\nProc. ICSICCS 2020, 2021: Springer, pp. 167-175, doi: 10.1007/978-981-15-5345-5_15.\\n\\n[291]\\n\\nJ. Serra, D. Suris, M. Miron, and A. Karatzoglou, \"Overcoming catastrophic forgetting with hard\\n\\nattention to the task,\" in Int. Conf. Mach. Learn., 2018: PMLR, pp. 4548-4557.\\n\\n[292]\\n\\nJ. Kirkpatrick et al., \"Overcoming catastrophic forgetting in neural networks,\" Proc. Natl. Acad.\\n\\nSci., vol. 114, no. 13, pp. 3521-3526, 2017, doi: 10.1073/pnas.1611835114.\\n\\n[293]\\n\\nS.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang, \"Overcoming catastrophic forgetting by\\n\\nincremental moment matching,\" Adv. Neural Inf. Process. Syst., vol. 30, 2017.\\n\\n[294]\\n\\nS.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \"icarl: Incremental classifier and\\n\\nrepresentation learning,\" in Proc. IEEEConf. Comput. Vis. Pattern. Recognit., 2017, pp. 2001-2010.\\n\\n[295]\\n\\nA. D\\'Amour et al., \"Underspecification presents challenges for credibility in modern machine\\n\\nlearning,\" J. Mach. Learn. Res., vol. 23, no. 226, pp. 1-61, 2022.\\n\\n[296]\\n\\nD. Teney, M. Peyrard, and E. Abbasnejad, \"Predicting is not understanding: Recognizing and\\n\\naddressing underspecification in machine learning,\" in Europ. Conf. Comput. Vis., 2022: Springer, pp.\\n\\n458-476.\\n\\n[297]\\n\\nN. Chotisarn, W. Pimanmassuriya, and S. Gulyanon, \"Deep learning visualization for\\n\\nunderspecification analysis in product design matching model development,\" IEEE ACCESS, vol. 9, pp.\\n\\n108049-108061, 2021, doi: 10.1109/ACCESS.2021.3102174.\\n\\n[298]\\n\\nA. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, \"Learning word vectors for\\n\\nsentiment analysis,\" in Proc. 49th Annual. Meeting. Assoc. Comput. Linguist.: Hum. langu. Tech.,\\n\\nPortland Oregon, June 19 - 2 2011, pp. 142-150.\\n\\n[299]\\n\\nH. Alemdar, H. Ertan, O. D. Incel, and C. Ersoy, \"ARAS human activity datasets in multiple homes\\n\\nwith multiple residents,\" in 2013 7th Int. Conf. Perv. Comput. Technol. Healthcare. Workshop., 2013:\\n\\nIEEE, pp. 232-235.\\n\\n[300]\\n\\nH. Mureşan and M. Oltean, \"Fruit recognition from images using deep learning,\" arXiv preprint\\n\\narXiv:1712.00580, 2017.\\n\\n[301]\\n\\nF. M. P. Shiri, T. Perumal, N. Mustapha, R. Mohamed, M. A. Ahmadon, and S. Yamaguchi, \"A\\n\\nSurvey on Multi-Resident Activity Recognition in Smart Environments,\" Evolution of Information,\\n\\nCommunication and Computing System, pp. 12-27, 2023.\\n\\n60\\n\\nF. M. Shiri et al.\\n\\n[302]\\n\\nX. Xiao, M. Yan, S. Basodi, C. Ji, and Y. Pan, \"Efficient hyperparameter optimization in deep\\n\\nlearning using a variable length genetic algorithm,\" arXiv preprint arXiv:2006.12703, 2020.\\n\\n[303]\\n\\nH. J. Escalante, M. Montes, and L. E. Sucar, \"Particle swarm model selection,\" J. Mach. Learn.\\n\\nRes., vol. 10, no. 2, pp. 405–440, 2009.\\n\\n[304]\\n\\nM. Parhizgar and F. M. Shiri, \"Solving quadratic assignment problem using water cycle\\n\\noptimization algorithm,\" International Journal of Intelligent Information Systems, vol. 3, no. 6-1, pp.\\n\\n75-79, 2014.\\n\\n[305]\\n\\nD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" arXiv preprint\\n\\narXiv:1412.6980, 2014.\\n\\n[306]\\n\\nL. Bottou, \"Stochastic gradient descent tricks,\" in Neural Networks: Tricks of the Trade: Second\\n\\nEdition: Springer, 2012, pp. 421-436.\\n\\n[307]\\n\\nJ. Duchi, E. Hazan, and Y. Singer, \"Adaptive subgradient methods for online learning and\\n\\nstochastic optimization,\" J. Mach. Learn. Res., vol. 12, no. 7, pp. 2121–2159, 2011.\\n\\n[308]\\n\\nT. Dozat, \"Incorporating nesterov momentum into adam,\" in Proc. 4th Int. Conf. Learn. Represent.\\n\\n(ICLR) Workshop Track., San Juan, Puerto Rico, 2016, pp. 1-4.\\n\\n[309]\\n\\nX. Chen et al., \"Symbolic discovery of optimization algorithms,\" Adv. Neural Inf. Process. Syst.,\\n\\nvol. 36, 2024.\\n\\n[310]\\n\\nL. Alzubaidi et al., \"A survey on deep learning tools dealing with data scarcity: definitions,\\n\\nchallenges, solutions, tips, and applications,\" J. Big. Data., vol. 10, no. 1, pp. 46, 2023, doi:\\n\\n10.1186/s40537-023-00727-2.\\n\\n[311]\\n\\nI. Cong, S. Choi, and M. D. Lukin, \"Quantum convolutional neural networks,\" Nat. Phys, vol. 15,\\n\\nno. 12, pp. 1273-1278, 2019, doi: 10.1038/s41567-019-0648-8.\\n\\n[312]\\n\\nY. Takaki, K. Mitarai, M. Negoro, K. Fujii, and M. Kitagawa, \"Learning temporal data with a\\n\\nvariational quantum recurrent neural network,\" Phys. Rev. A, vol. 103, no. 5, pp. 052414, 2021, doi:\\n\\n10.1103/PhysRevA.103.052414.\\n\\n[313]\\n\\nS. Lloyd and C. Weedbrook, \"Quantum generative adversarial learning,\" Phys. Rev. Lett., vol. 121,\\n\\nno. 4, pp. 040502, 2018, doi: 10.1103/PhysRevLett.121.040502.\\n\\n[314]\\n\\nS. Garg and G. Ramakrishnan, \"Advances in quantum deep learning: An overview,\" arXiv preprint\\n\\narXiv:2005.04316, 2020.\\n\\n[315]\\n\\nF. Valdez and P. Melin, \"A review on quantum computing and deep learning algorithms and their\\n\\napplications,\" Soft Comput., vol. 27, no. 18, pp. 13217-13236, 2023, doi: 10.1007/s00500-022-07037-\\n\\n4.\\n\\n61'),\n",
       " Document(metadata={'source': 'research_ppr/sequence_to_sequence_learning.pdf'}, page_content='4 1 0 2 c e D 4 1\\n\\n] L C . s c [\\n\\n3 v 5 1 2 3 . 9 0 4 1 : v i X r a\\n\\nSequence to Sequence Learning with Neural Networks\\n\\nIlya Sutskever Google ilyasu@google.com\\n\\nOriol Vinyals Google vinyals@google.com\\n\\nQuoc V. Le Google qvl@google.com\\n\\nAbstract\\n\\nDeep Neural Networks (DNNs) are powerful models that have achieved excel- lent performance on difﬁcult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a ﬁxed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT’14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difﬁculty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas- sive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\\n\\n1 Introduction\\n\\nDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex- cellent performance on difﬁcult problems such as speech recognition [13, 7] and visual object recog- nition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation for a modest number of steps. A surprising example of the power of DNNs is their ability to sort N N -bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are related to conventional statistical models, they learn an intricate computation. Furthermore, large DNNs can be trained with supervised backpropagation whenever the labeled training set has enough information to specify the network’s parameters. Thus, if there exists a parameter setting of a large DNN that achieves good results (for example, because humans can solve the task very rapidly), supervised backpropagation will ﬁnd these parameters and solve the problem.\\n\\nDespite their ﬂexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of ﬁxed dimensionality. It is a signiﬁcant limitation, since many important problems are best expressed with sequences whose lengths are not known a-priori. For example, speech recognition and machine translation are sequential problems. Likewise, ques- tion answering can also be seen as mapping a sequence of words representing the question to a\\n\\n1\\n\\nsequence of words representing the answer. It is therefore clear that a domain-independent method that learns to map sequences to sequences would be useful.\\n\\nSequences pose a challenge for DNNs because they require that the dimensionality of the inputs and outputs is known and ﬁxed. In this paper, we show that a straightforward application of the Long Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems. The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large ﬁxed- dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector (ﬁg. 1). The second LSTM is essentially a recurrent neural network language model [28, 23, 30] except that it is conditioned on the input sequence. The LSTM’s ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application due to the considerable time lag between the inputs and their corresponding outputs (ﬁg. 1).\\n\\nThere have been a number of related attempts to address the general sequence to sequence learning problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18] who were the ﬁrst to map the entire input sentence to vector, and is related to Cho et al. [5] although the latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10] introduced a novel differentiable attention mechanism that allows neural networks to focus on dif- ferent parts of their input, and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al. [2]. The Connectionist Sequence Classiﬁcation is another popular technique for mapping sequences to sequences with neural networks, but it assumes a monotonic alignment between the inputs and the outputs [11].\\n\\nFigure 1: Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence. The model stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the input sentence in reverse, because doing so introduces many short term dependencies in the data that make the optimization problem much easier.\\n\\nThe main result of this work is the following. On the WMT’14 English to French translation task, we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep LSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam- search decoder. This is by far the best result achieved by direct translation with large neural net- works. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81 BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized whenever the reference translation contained a word not covered by these 80k. This result shows that a relatively unoptimized small-vocabulary neural network architecture which has much room for improvement outperforms a phrase-based SMT system.\\n\\nFinally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on the same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by 3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).\\n\\nSurprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other researchers with related architectures [26]. We were able to do well on long sentences because we reversed the order of words in the source sentence but not the target sentences in the training and test set. By doing so, we introduced many short term dependencies that made the optimization problem much simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with long sentences. The simple trick of reversing the words in the source sentence is one of the key technical contributions of this work.\\n\\nA useful property of the LSTM is that it learns to map an input sentence of variable length into a ﬁxed-dimensional vector representation. Given that translations tend to be paraphrases of the source sentences, the translation objective encourages the LSTM to ﬁnd sentence representations that capture their meaning, as sentences with similar meanings are close to each other while different\\n\\n2\\n\\nsentences meanings will be far. A qualitative evaluation supports this claim, showing that our model is aware of word order and is fairly invariant to the active and passive voice.\\n\\n2 The model\\n\\nThe Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural networks to sequences. Given a sequence of inputs (x1, . . . , xT ), a standard RNN computes a sequence of outputs (y1, . . . , yT ) by iterating the following equation: ht = sigm (cid:0)W hxxt + W hhht−1(cid:1) yt = W yhht\\n\\nThe RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relation- ships.\\n\\nThe simplest strategy for general sequence learning is to map the input sequence to a ﬁxed-sized vector using one RNN, and then to map the vector to the target sequence with another RNN (this approach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is provided with all the relevant information, it would be difﬁcult to train the RNNs due to the resulting long term dependencies (ﬁgure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM) [16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed in this setting.\\n\\nThe goal of the LSTM is to estimate the conditional probability p(y1, . . . , yT ′|x1, . . . , xT ) where (x1, . . . , xT ) is an input sequence and y1, . . . , yT ′ is its corresponding output sequence whose length T ′ may differ from T . The LSTM computes this conditional probability by ﬁrst obtaining the ﬁxed- dimensional representation v of the input sequence (x1, . . . , xT ) given by the last hidden state of the LSTM, and then computing the probability of y1, . . . , yT ′ with a standard LSTM-LM formulation whose initial hidden state is set to the representation v of x1, . . . , xT :\\n\\nT ′\\n\\np(y1, . . . , yT ′ |x1, . . . , xT ) =\\n\\nY t=1\\n\\np(yt|v, y1, . . . , yt−1)\\n\\nIn this equation, each p(yt|v, y1, . . . , yt−1) distribution is represented with a softmax over all the words in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that each sentence ends with a special end-of-sentence symbol “<EOS>”, which enables the model to deﬁne a distribution over sequences of all possible lengths. The overall scheme is outlined in ﬁgure 1, where the shown LSTM computes the representation of “A”, “B”, “C”, “<EOS>” and then uses this representation to compute the probability of “W”, “X”, “Y”, “Z”, “<EOS>”.\\n\\nOur actual models differ from the above description in three important ways. First, we used two different LSTMs: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs signiﬁcantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found it extremely valuable to reverse the order of the words of the input sentence. So for example, instead of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to β, and so on, a fact that makes it easy for SGD to “establish communication” between the input and the output. We found this simple data transformation to greatly improve the performance of the LSTM.\\n\\n3 Experiments\\n\\nWe applied our method to the WMT’14 English to French MT task in two ways. We used it to directly translate the input sentence without using a reference SMT system and we it to rescore the n-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample translations, and visualize the resulting sentence representation.\\n\\n3\\n\\n(1)\\n\\n3.1 Dataset details\\n\\nWe used the WMT’14 English to French dataset. We trained our models on a subset of 12M sen- tences consisting of 348M French words and 304M English words, which is a clean “selected” subset from [29]. We chose this translation task and this speciﬁc training set subset because of the public availability of a tokenized training and test set together with 1000-best lists from the baseline SMT [29].\\n\\nAs typical neural language models rely on a vector representation for each word, we used a ﬁxed vocabulary for both languages. We used 160,000 of the most frequent words for the source language and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was replaced with a special “UNK” token.\\n\\n3.2 Decoding and Rescoring\\n\\nThe core of our experiments involved training a large deep LSTM on many sentence pairs. We trained it by maximizing the log probability of a correct translation T given the source sentence S, so the training objective is\\n\\n1/|S| X\\n\\nlog p(T |S)\\n\\n(T,S)∈S\\n\\nwhere S is the training set. Once training is complete, we produce translations by ﬁnding the most likely translation according to the LSTM:\\n\\nˆT = arg max\\n\\nT\\n\\np(T |S)\\n\\nWe search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number B of partial hypotheses, where a partial hypothesis is a preﬁx of some translation. At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but the B most likely hypotheses according to the model’s log probability. As soon as the “<EOS>” symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system performs well even with a beam size of 1, and a beam of size 2 provides most of the beneﬁts of beam search (Table 1).\\n\\nWe also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To rescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took an even average with their score and the LSTM’s score.\\n\\n3.3 Reversing the Source Sentences\\n\\nWhile the LSTM is capable of solving problems with long term dependencies, we discovered that the LSTM learns much better when the source sentences are reversed (the target sentences are not reversed). By doing so, the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU scores of its decoded translations increased from 25.9 to 30.6.\\n\\nWhile we do not have a complete explanation to this phenomenon, we believe that it is caused by the introduction of many short term dependencies to the dataset. Normally, when we concatenate a source sentence with a target sentence, each word in the source sentence is far from its corresponding word in the target sentence. As a result, the problem has a large “minimal time lag” [17]. By reversing the words in the source sentence, the average distance between corresponding words in the source and target language is unchanged. However, the ﬁrst few words in the source language are now very close to the ﬁrst few words in the target language, so the problem’s minimal time lag is greatly reduced. Thus, backpropagation has an easier time “establishing communication” between the source sentence and the target sentence, which in turn results in substantially improved overall performance.\\n\\nInitially, we believed that reversing the input sentences would only lead to more conﬁdent predic- tions in the early parts of the target sentence and to less conﬁdent predictions in the later parts. How- ever, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\\n\\n4\\n\\n(2)\\n\\ntrained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences results in LSTMs with better memory utilization.\\n\\n3.4 Training details\\n\\nWe found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers, with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary of 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to represent a sentence. We found deep LSTMs to signiﬁcantly outperform shallow LSTMs, where each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden state. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M for the “decoder” LSTM). The complete training details are given below:\\n\\nWe initialized all of the LSTM’s parameters with the uniform distribution between -0.08 and 0.08\\n\\nWe used stochastic gradient descent without momentum, with a ﬁxed learning rate of 0.7. After 5 epochs, we begun halving the learning rate every half epoch. We trained our models for a total of 7.5 epochs.\\n\\nWe used batches of 128 sequences for the gradient and divided it the size of the batch (namely, 128).\\n\\nAlthough LSTMs tend to not suffer from the vanishing gradient problem, they can have exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10, 25] by scaling it when its norm exceeded a threshold. For each training batch, we compute s = kgk2, where g is the gradient divided by 128. If s > 5, we set g = 5g s .\\n\\nDifferent sentences have different lengths. Most sentences are short (e.g., length 20-30) but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen training sentences will have many short sentences and few long sentences, and as a result, much of the computation in the minibatch is wasted. To address this problem, we made sure that all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.\\n\\n3.5 Parallelization\\n\\nA C++ implementation of deep LSTM with the conﬁguration from the previous section on a sin- gle GPU processes a speed of approximately 1,700 words per second. This was too slow for our purposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was executed on a different GPU and communicated its activations to the next GPU / layer as soon as they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate GPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible for multiplying by a 1000 × 20000 matrix. The resulting implementation achieved a speed of 6,300 (both English and French) words per second with a minibatch size of 128. Training took about a ten days with this implementation.\\n\\n3.6 Experimental Results\\n\\nWe used the cased BLEU score [24] to evaluate the quality of our translations. We computed our BLEU scores using multi-bleu.pl1 on the tokenized predictions and ground truth. This way of evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29]. However, if we evaluate the best WMT’14 system [9] (whose predictions can be downloaded from statmt.org\\\\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by statmt.org\\\\matrix.\\n\\nThe results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs that differ in their random initializations and in the random order of minibatches. While the decoded translations of the LSTM ensemble do not outperform the best WMT’14 system, it is the ﬁrst time that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT\\n\\n1There several variants of the BLEU score, and each variant is deﬁned with a perl script.\\n\\n5\\n\\nMethod Bahdanau et al. [2] Baseline System [29] Single forward LSTM, beam size 12 Single reversed LSTM, beam size 12 Ensemble of 5 reversed LSTMs, beam size 1 Ensemble of 2 reversed LSTMs, beam size 12 Ensemble of 5 reversed LSTMs, beam size 2 Ensemble of 5 reversed LSTMs, beam size 12\\n\\ntest BLEU score (ntst14) 28.45 33.30 26.17 30.59 33.00 33.27 34.50 34.81\\n\\nTable 1: The performance of the LSTM on WMT’14 English to French test set (ntst14). Note that an ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of size 12.\\n\\nMethod Baseline System [29] Cho et al. [5] Best WMT’14 result [9] Rescoring the baseline 1000-best with a single forward LSTM Rescoring the baseline 1000-best with a single reversed LSTM Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs Oracle Rescoring of the Baseline 1000-best lists\\n\\ntest BLEU score (ntst14) 33.30 34.54 37.0 35.61 35.85 36.5 ∼45\\n\\nTable 2: Methods that use neural networks together with an SMT system on the WMT’14 English to French test set (ntst14).\\n\\ntask by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is within 0.5 BLEU points of the best WMT’14 result if it is used to rescore the 1000-best list of the baseline system.\\n\\n3.7 Performance on long sentences\\n\\nWe were surprised to discover that the LSTM did well on long sentences, which is shown quantita- tively in ﬁgure 3. Table 3 presents several examples of long sentences and their translations.\\n\\n3.8 Model Analysis\\n\\n4\\n\\n15\\n\\nI was given a card by her in the garden\\n\\n3\\n\\nMary admires John\\n\\n10\\n\\nIn the garden , she gave me a card\\n\\nShe gave me a card in the garden\\n\\n2\\n\\nMary is in love with John\\n\\n5\\n\\n1\\n\\n0\\n\\nMary respects John\\n\\n0\\n\\n−1\\n\\nJohn admires Mary\\n\\n−2\\n\\nJohn is in love with Mary\\n\\n−5\\n\\nShe was given a card by me in the garden\\n\\nIn the garden , I gave her a card\\n\\n−3\\n\\n−10\\n\\n−4\\n\\n−5\\n\\nJohn respects Mary\\n\\n−15\\n\\nI gave her a card in the garden\\n\\n−6\\n\\n−20\\n\\n−8\\n\\n−6\\n\\n−4\\n\\n−2\\n\\n0\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\n−15\\n\\n−10\\n\\n−5\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\nFigure 2: The ﬁgure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained after processing the phrases in the ﬁgures. The phrases are clustered by meaning, which in these examples is primarily a function of word order, which would be difﬁcult to capture with a bag-of-words model. Notice that both clusters have similar internal structure.\\n\\nOne of the attractive features of our model is its ability to turn a sequence of words into a vector of ﬁxed dimensionality. Figure 2 visualizes some of the learned representations. The ﬁgure clearly shows that the representations are sensitive to the order of words, while being fairly insensitive to the\\n\\n6\\n\\nType Our model Ulrich UNK , membre du conseil d’ administration du constructeur automobile Audi ,\\n\\nSentence\\n\\nTruth\\n\\nOur model\\n\\nTruth\\n\\nafﬁrme qu’ il s’ agit d’ une pratique courante depuis des ann´ees pour que les t´el´ephones portables puissent ˆetre collect´es avant les r´eunions du conseil d’ administration aﬁn qu’ ils ne soient pas utilis´es comme appareils d’ ´ecoute `a distance . Ulrich Hackenberg , membre du conseil d’ administration du constructeur automobile Audi , d´eclare que la collecte des t´el´ephones portables avant les r´eunions du conseil , aﬁn qu’ ils ne puissent pas ˆetre utilis´es comme appareils d’ ´ecoute `a distance , est une pratique courante depuis des ann´ees . “ Les t´el´ephones cellulaires , qui sont vraiment une question , non seulement parce qu’ ils pourraient potentiellement causer des interf´erences avec les appareils de navigation , mais nous savons , selon la FCC , qu’ ils pourraient interf´erer avec les tours de t´el´ephone cellulaire lorsqu’ ils sont dans l’ air ” , dit UNK . “ Les t´el´ephones portables sont v´eritablement un probl`eme , non seulement parce qu’ ils pourraient ´eventuellement cr´eer des interf´erences avec les instruments de navigation , mais parce que nous savons , d’ apr`es la FCC , qu’ ils pourraient perturber les antennes-relais de t´el´ephonie mobile s’ ils sont utilis´es `a bord ” , a d´eclar´e Rosenker .\\n\\nOur model Avec la cr´emation , il y a un “ sentiment de violence contre le corps d’ un ˆetre cher ” , qui sera “ r´eduit `a une pile de cendres ” en tr`es peu de temps au lieu d’ un processus de d´ecomposition “ qui accompagnera les ´etapes du deuil ” . Il y a , avec la cr´emation , “ une violence faite au corps aim´e ” , qui va ˆetre “ r´eduit `a un tas de cendres ” en tr`es peu de temps , et non apr`es un processus de d´ecomposition , qui “ accompagnerait les phases du deuil ” .\\n\\nTruth\\n\\nTable 3: A few examples of long translations produced by the LSTM alongside the ground truth translations. The reader can verify that the translations are sensible using Google translate.\\n\\n40\\n\\nLSTM (34.8) baseline (33.3)\\n\\n40\\n\\nLSTM (34.8) baseline (33.3)\\n\\n35\\n\\n35\\n\\ne r o c s U E L B\\n\\n30\\n\\ne r o c s U E L B\\n\\n30\\n\\n25\\n\\n25\\n\\n20\\n\\n4 7 8\\n\\n12\\n\\n17\\n\\n22\\n\\n28\\n\\n35\\n\\n79\\n\\n20\\n\\n0\\n\\n500\\n\\n1000\\n\\n1500\\n\\n2000\\n\\n2500\\n\\n3000\\n\\n3500\\n\\ntest sentences sorted by their length\\n\\ntest sentences sorted by average word frequency rank\\n\\nFigure 3: The left plot shows the performance of our system as a function of sentence length, where the x-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths. There is no degradation on sentences with less than 35 words, there is only a minor degradation on the longest sentences. The right plot shows the LSTM’s performance on sentences with progressively more rare words, where the x-axis corresponds to the test sentences sorted by their “average word frequency rank”.\\n\\nreplacement of an active voice with a passive voice. The two-dimensional projections are obtained using PCA.\\n\\n4 Related work\\n\\nThere is a large body of work on applications of neural networks to machine translation. So far, the simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a\\n\\n7\\n\\nFeedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the n- best lists of a strong MT baseline [22], which reliably improves translation quality.\\n\\nMore recently, researchers have begun to look into ways of including information about the source language into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLM with a topic model of the input sentence, which improves rescoring performance. Devlin et al. [8] followed a similar approach, but they incorporated their NNLM into the decoder of an MT system and used the decoder’s alignment information to provide the NNLM with the most useful words in the input sentence. Their approach was highly successful and it achieved large improvements over their baseline.\\n\\nOur work is closely related to Kalchbrenner and Blunsom [18], who were the ﬁrst to map the input sentence into a vector and then back to a sentence, although they map sentences to vectors using convolutional neural networks, which lose the ordering of the words. Similarly to this work, Cho et al. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their primary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also attempted direct translations with a neural network that used an attention mechanism to overcome the poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging results. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et al. [5] by translating pieces of the source sentence in way that produces smooth translations, which is similar to a phrase-based approach. We suspect that they could achieve similar improvements by simply training their networks on reversed source sentences.\\n\\nEnd-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs and outputs by feedforward networks, and map them to similar points in space. However, their approach cannot generate translations directly: to get a translation, they need to do a look up for closest vector in the pre-computed database of sentences, or to rescore a sentence.\\n\\n5 Conclusion\\n\\nIn this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes almost no assumption about problem structure can outperform a standard SMT-based system whose vocabulary is unlimited on a large-scale MT task. The success of our simple LSTM-based approach on MT suggests that it should do well on many other sequence learning problems, provided they have enough training data.\\n\\nWe were surprised by the extent of the improvement obtained by reversing the words in the source sentences. We conclude that it is important to ﬁnd a problem encoding that has the greatest number of short term dependencies, as they make the learning problem much simpler. In particular, while we were unable to train a standard RNN on the non-reversed translation problem (shown in ﬁg. 1), we believe that a standard RNN should be easily trainable when the source sentences are reversed (although we did not verify it experimentally).\\n\\nWe were also surprised by the ability of the LSTM to correctly translate very long sentences. We were initially convinced that the LSTM would fail on long sentences due to its limited memory, and other researchers reported poor performance on long sentences with a model similar to ours [5, 2, 26]. And yet, LSTMs trained on the reversed dataset had little difﬁculty translating long sentences.\\n\\nMost importantly, we demonstrated that a simple, straightforward and a relatively unoptimized ap- proach can outperform an SMT system, so further work will likely lead to even greater translation accuracies. These results suggest that our approach will likely do well on other challenging sequence to sequence problems.\\n\\n6 Acknowledgments\\n\\nWe thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolf- gang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba, and the Google Brain team for useful comments and discussions.\\n\\n8\\n\\nReferences\\n\\n[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint language and translation modeling with recurrent\\n\\nneural networks. In EMNLP, 2013.\\n\\n[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\\n\\narXiv preprint arXiv:1409.0473, 2014.\\n\\n[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In Journal of\\n\\nMachine Learning Research, pages 1137–1155, 2003.\\n\\n[4] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.\\n\\nIEEE Transactions on Neural Networks, 5(2):157–166, 1994.\\n\\n[5] K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase represen- tations using RNN encoder-decoder for statistical machine translation. In Arxiv preprint arXiv:1406.1078, 2014.\\n\\n[6] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.\\n\\nIn CVPR, 2012.\\n\\n[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - Special Issue on Deep Learning for Speech and Language Processing, 2012.\\n\\n[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and robust neural network\\n\\njoint models for statistical machine translation. In ACL, 2014.\\n\\n[9] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s phrase-based machine\\n\\ntranslation systems for wmt-14. In WMT, 2014.\\n\\n[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,\\n\\n2013.\\n\\n[11] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber. Connectionist temporal classiﬁcation: labelling\\n\\nunsegmented sequence data with recurrent neural networks. In ICML, 2006.\\n\\n[12] K. M. Hermann and P. Blunsom. Multilingual distributed representations without word alignment. In\\n\\nICLR, 2014.\\n\\n[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 2012.\\n\\n[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Master’s thesis, Institut fur Infor-\\n\\nmatik, Technische Universitat, Munchen, 1991.\\n\\n[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient ﬂow in recurrent nets: the difﬁculty\\n\\nof learning long-term dependencies, 2001.\\n\\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997. [17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997. [18] N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. In EMNLP, 2013. [19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural\\n\\nnetworks. In NIPS, 2012.\\n\\n[20] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Building\\n\\nhigh-level features using large scale unsupervised learning. In ICML, 2012.\\n\\n[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\\n\\nProceedings of the IEEE, 1998.\\n\\n[22] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of\\n\\nTechnology, 2012.\\n\\n[23] T. Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based\\n\\nlanguage model. In INTERSPEECH, pages 1045–1048, 2010.\\n\\n[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machine\\n\\ntranslation. In ACL, 2002.\\n\\n[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks. arXiv\\n\\npreprint arXiv:1211.5063, 2012.\\n\\n[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio. Overcoming the curse of sentence length for neural machine translation using automatic segmentation. arXiv preprint arXiv:1409.1257, 2014.\\n\\n[27] A. Razborov. On small depth threshold circuits.\\n\\nIn Proc. 3rd Scandinavian Workshop on Algorithm\\n\\nTheory, 1992.\\n\\n[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.\\n\\nNature, 323(6088):533–536, 1986.\\n\\n[29] H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/˜schwenk/cslm_\\n\\njoint_paper/, 2014. [Online; accessed 03-September-2014].\\n\\n[30] M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In INTER-\\n\\nSPEECH, 2010.\\n\\n[31] P. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of IEEE, 1990.\\n\\n9'),\n",
       " Document(metadata={'source': 'research_ppr/attention_all_you_need.pdf'}, page_content='3 2 0 2\\n\\ng u A 2\\n\\n] L C . s c [\\n\\n7 v 2 6 7 3 0 . 6 0 7 1 : v i X r a\\n\\nProvided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\\n\\nAttention Is All You Need\\n\\nAshish Vaswani∗ Google Brain avaswani@google.com\\n\\nNoam Shazeer∗ Google Brain noam@google.com\\n\\nNiki Parmar∗ Google Research nikip@google.com\\n\\nJakob Uszkoreit∗ Google Research usz@google.com\\n\\nLlion Jones∗ Google Research llion@google.com\\n\\nAidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\\n\\nŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com\\n\\nIllia Polosukhin∗ ‡ illia.polosukhin@gmail.com\\n\\nAbstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\\n\\n†Work performed while at Google Brain. ‡Work performed while at Google Research.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n1\\n\\nIntroduction\\n\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\\n\\n3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\\n\\n2\\n\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\\n\\n3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n3\\n\\nScaled Dot-Product Attention\\n\\nMulti-Head Attention\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values.\\n\\n√\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\n\\nAttention(Q, K, V ) = softmax(\\n\\nQK T √ dk\\n\\n)V\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor . Additive attention computes the compatibility function using a feed-forward network with of a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\n1√\\n\\ndk\\n\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ dk\\n\\n.\\n\\n3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random i=1 qiki, has mean 0 and variance dk.\\n\\nvariables with mean 0 and variance 1. Then their dot product, q · k = (cid:80)dk\\n\\n4\\n\\n(1)\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\n\\nwhere headi = Attention(QW Q\\n\\ni , KW K i\\n\\n, V W V\\n\\ni )\\n\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel.\\n\\ni ∈ Rdmodel×dk , W K\\n\\ni ∈ Rdmodel×dk , W V\\n\\ni ∈ Rdmodel×dv\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\nIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n\\nThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n\\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n\\n(2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.\\n\\n3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax dmodel. linear transformation, similar to [30]. In the embedding layers, we multiply those weights by\\n\\n√\\n\\n5\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\\n\\nLayer Type\\n\\nSelf-Attention Recurrent Convolutional Self-Attention (restricted)\\n\\nComplexity per Layer\\n\\nO(n2 · d) O(n · d2) O(k · n · d2) O(r · n · d)\\n\\nSequential Maximum Path Length Operations O(1) O(n) O(1) O(1)\\n\\nO(1) O(n) O(logk(n)) O(n/r)\\n\\n3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\nP E(pos,2i) = sin(pos/100002i/dmodel) P E(pos,2i+1) = cos(pos/100002i/dmodel)\\n\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos.\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\n\\n6\\n\\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n5 Training\\n\\nThis section describes the training regime for our models.\\n\\n5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n5.3 Optimizer\\n\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula:\\n\\nlrate = d−0.5\\n\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\\n\\n5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\n7\\n\\n(3)\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\nModel\\n\\nByteNet [18] Deep-Att + PosUnk [39] GNMT + RL [38] ConvS2S [9] MoE [32] Deep-Att + PosUnk Ensemble [39] GNMT + RL Ensemble [38] ConvS2S Ensemble [9] Transformer (base model) Transformer (big)\\n\\nBLEU\\n\\nEN-DE EN-FR 23.75\\n\\n24.6 25.16 26.03\\n\\n26.30 26.36 27.3 28.4\\n\\n39.2 39.92 40.46 40.56 40.4 41.16 41.29 38.1 41.8\\n\\nTraining Cost (FLOPs)\\n\\nEN-DE\\n\\nEN-FR\\n\\n2.3 · 1019 9.6 · 1018 2.0 · 1019\\n\\n1.8 · 1020 7.7 · 1019\\n\\n1.0 · 1020 1.4 · 1020 1.5 · 1020 1.2 · 1020 8.0 · 1020 1.1 · 1021 1.2 · 1021\\n\\n3.3 · 1018 2.3 · 1019\\n\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\\n\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n6 Results\\n\\n6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.\\n\\n6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n\\n8\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\nbase\\n\\n(A)\\n\\n(B)\\n\\n(C)\\n\\n(D)\\n\\nN dmodel\\n\\n6\\n\\n512\\n\\n2 4 8\\n\\n256 1024\\n\\ndff\\n\\n2048\\n\\n1024 4096\\n\\nh\\n\\n8 1 4 16 32\\n\\ndk\\n\\n64 512 128 32 16 16 32\\n\\n32 128\\n\\ndv\\n\\n64 512 128 32 16\\n\\n32 128\\n\\nPdrop\\n\\n0.1\\n\\n0.0 0.2\\n\\nϵls\\n\\n0.1\\n\\n0.0 0.2\\n\\nPPL train steps (dev) 100K 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 300K 4.33\\n\\nBLEU params ×106 (dev) 25.8 65 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4\\n\\n58 60 36 50 80 28 168 53 90\\n\\n(E) big\\n\\n6\\n\\npositional embedding instead of sinusoids\\n\\n1024\\n\\n4096\\n\\n16\\n\\n0.3\\n\\n213\\n\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\\n\\n6.3 English Constituency Parsing\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\\n\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\\n\\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\\n\\n9\\n\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\nParser\\n\\nTraining\\n\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative\\n\\nPetrov et al. (2006) [29] Zhu et al. (2013) [40] Dyer et al. (2016) [8] Transformer (4 layers) Zhu et al. (2013) [40] Huang & Harper (2009) [14] McClosky et al. (2006) [26] Vinyals & Kaiser el al. (2014) [37] Transformer (4 layers) Luong et al. (2015) [23] Dyer et al. (2016) [8]\\n\\nWSJ 23 F1 88.3 90.4 90.4 91.7 91.3 91.3 91.3 92.1 92.1 92.7 93.0 93.3\\n\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.\\n\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences.\\n\\n7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\nReferences\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\n\\narXiv:1607.06450, 2016.\\n\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\n\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\n\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\n\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n10\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\n\\npreprint arXiv:1610.02357, 2016.\\n\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\n\\nnetwork grammars. In Proc. of NAACL, 2016.\\n\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\n\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n\\n[10] Alex Graves. Generating sequences with recurrent neural networks.\\n\\narXiv preprint\\n\\narXiv:1308.0850, 2013.\\n\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\n\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n\\n9(8):1735–1780, 1997.\\n\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\n\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\n\\nInformation Processing Systems, (NIPS), 2016.\\n\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\n\\non Learning Representations (ICLR), 2016.\\n\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nIn International Conference on Learning Representations, 2017.\\n\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\n\\narXiv:1703.10722, 2017.\\n\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\n\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\n\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n11\\n\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\n\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\n\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\n\\npreprint arXiv:1608.05859, 2016.\\n\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\n\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\n\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\n\\nAdvances in Neural Information Processing Systems, 2015.\\n\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n12\\n\\nAttention Visualizations\\n\\nthis\\n\\nthis\\n\\ndifficult\\n\\ndifficult\\n\\nprocess\\n\\nprocess\\n\\nmajority\\n\\nhave\\n\\nmore\\n\\nmore\\n\\nnew\\n\\nnew\\n\\nAmerican\\n\\nAmerican\\n\\nhave\\n\\nof\\n\\nIt\\n\\npassed\\n\\npassed\\n\\nregistration\\n\\nregistration\\n\\ngovernments\\n\\ngovernments\\n\\nvoting\\n\\nIt\\n\\nin\\n\\nin\\n\\nsince\\n\\nsince\\n\\nor\\n\\nor\\n\\nmajority\\n\\nof\\n\\nvoting\\n\\nlaws\\n\\n<pad>\\n\\n<EOS>\\n\\n<EOS>\\n\\nInput-Input Layer5\\n\\n.\\n\\n.\\n\\n<pad>\\n\\na\\n\\nthe\\n\\nthe\\n\\nspirit\\n\\nspirit\\n\\nthat\\n\\nthat\\n\\nmaking\\n\\n<pad>\\n\\n<pad>\\n\\n<pad>\\n\\n<pad>\\n\\n<pad>\\n\\n<pad>\\n\\n<pad>\\n\\n<pad>\\n\\n<pad>\\n\\n<pad>\\n\\n2009\\n\\n2009\\n\\nis\\n\\nis\\n\\nmaking\\n\\na\\n\\nlaws\\n\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\\n\\n13\\n\\napplication\\n\\nbe\\n\\nbe\\n\\nnever\\n\\njust\\n\\nThe\\n\\nLaw\\n\\nwill\\n\\nbut\\n\\nInput-Input Layer5\\n\\n,\\n\\n just\\n\\n just\\n\\nits\\n\\nshould\\n\\nperfect\\n\\n just\\n\\n <pad>\\n\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n14\\n\\napplication\\n\\nbe\\n\\nbe\\n\\nnever\\n\\njust\\n\\nThe\\n\\nLaw\\n\\nwill\\n\\nbut\\n\\nInput-Input Layer5\\n\\n,\\n\\n just\\n\\n just\\n\\nits\\n\\nshould\\n\\nperfect\\n\\n just\\n\\n <pad>\\n\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n\\n15')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check documents\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###clean the documents/page content if required\n",
    "\n",
    "\"\"\"Description \n",
    "    Clean the input text by removing unnecessary whitespace, non-alphanumeric characters, \n",
    "    and trimming leading/trailing spaces.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. Replaces multiple consecutive whitespace characters (spaces, newlines, tabs) \n",
    "       with a single space.\n",
    "    2. Removes all non-alphanumeric characters, retaining only letters (a-z, A-Z), \n",
    "       numbers (0-9), and spaces.\n",
    "    3. Strips leading and trailing spaces from the text.\n",
    "\"\"\"\n",
    "\n",
    "import re  # Import the regular expression module for text processing\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove extra whitespaces, newlines, and tabs by replacing all whitespace characters with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)  # This matches one or more whitespace characters (spaces, newlines, tabs) and replaces them with a single space\n",
    "    # Remove non-alphanumeric characters (except spaces) using a regular expression\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # This matches any character that is not a letter (a-z, A-Z), a number (0-9), or whitespace (\\s), and removes it\n",
    "    # Remove leading and trailing spaces from the text\n",
    "    text = text.strip()  # The .strip() method removes any whitespace at the start and end of the text\n",
    "    return text  # Return the cleaned text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Description : \n",
    "    Clean and process a list of documents, splitting the content into chunks and adding metadata.\n",
    "\n",
    "    This function performs the following steps for each document in the input list:\n",
    "    1. Cleans the document's content by removing unnecessary whitespace, non-alphanumeric characters,\n",
    "       and trimming leading/trailing spaces (via the `clean_text` function).\n",
    "    2. Adds metadata to each chunk, including the document's filename (extracted from the document's source).\n",
    "    3. Splits the cleaned content into smaller chunks using `RecursiveCharacterTextSplitter`, \n",
    "       ensuring each chunk is of a specified size (with an overlap between chunks).\n",
    "    \n",
    "    Args:\n",
    "    documents (list): A list of Document objects, each containing `page_content` and `metadata`.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of processed Document chunks, each with the cleaned content and associated metadata.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Import the text splitter\n",
    "\n",
    "# Clean the documents' page content and add filename as metadata\n",
    "def clean_and_process_documents(documents):\n",
    "    # Initialize the text splitter for chunking\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=3000,  # Set a chunk size limit\n",
    "        chunk_overlap=50,  # Set overlap between chunks\n",
    "        length_function=len,  # Define how chunk lengths are calculated\n",
    "        is_separator_regex=False,  # Whether the separator should be regex (False in this case)\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        # Clean the document's content\n",
    "        cleaned_content = clean_text(doc.page_content)\n",
    "\n",
    "        # Create metadata: use the document's source (filename) as metadata\n",
    "        metadata = {\"source\": (doc.metadata.get('source', 'Unknown')).split(\"/\")[-1]}\n",
    "    \n",
    "\n",
    "        # Split the document into chunks and append metadata\n",
    "        chunk = text_splitter.create_documents([cleaned_content], metadatas=[metadata])\n",
    "        chunks.extend(chunk)  # Add the chunks to the list\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Process the documents\n",
    "processed_documents = clean_and_process_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='information Review Recurrent Neural Networks A Comprehensive Review of Architectures Variants and Applications Ibomoiye Domor Mienye 1  Theo G Swart 1 and George Obaido 2 1 Institute for Intelligent Systems University of Johannesburg Johannesburg 2006 South Africa tgswartujacza 2 Center for HumanCompatible Artificial Intelligence CHAI Berkeley Institute for Data Science BIDS University of California Berkeley CA 94720 USA gobaidoberkeleyedu Correspondence ibomoiyemujacza  These authors contributed equally to this work Abstract Recurrent neural networks RNNs have significantly advanced the field of machine learn ing ML by enabling the effective processing of sequential data This paper provides a comprehensive review of RNNs and their applications highlighting advancements in architectures such as long shortterm memory LSTM networks gated recurrent units GRUs bidirectional LSTM BiLSTM echo state networks ESNs peephole LSTM and stacked LSTM The study examines the application of RNNs to different domains including natural language processing NLP speech recognition time series forecasting autonomous vehicles and anomaly detection Additionally the study discusses recent innovations such as the integration of attention mechanisms and the development of hybrid models that combine RNNs with convolutional neural networks CNNs and transformer architec tures This review aims to provide ML researchers and practitioners with a comprehensive overview of the current state and future directions of RNN research Keywords deep learning GRU LSTM machine learning NLP RNN Citation Mienye ID Swart TG 1 Introduction Obaido G Recurrent Neural Networks A Comprehensive Review of Architectures Variants and Applications Information 2024 15 517 httpsdoiorg103390info15090517 Academic Editor Mara N Moreno Deep learning DL has reshaped the field of artificial intelligence AI driving ad vancements in a wide array of applications from image recognition and natural language processing NLP to autonomous driving and medical diagnostics 15 This rapid growth is fueled by the increasing availability of big data advancements in computing power and the development of sophisticated algorithms 69 As DL models continue to evolve they are increasingly being deployed in critical sectors demonstrating their ability to outperform traditional machine learning ML techniques in handling complex tasks Garca Received 21 July 2024 Revised 22 August 2024 Accepted 23 August 2024 Published 25 August 2024 Recurrent neural networks RNNs are a class of deep learning models that are funda mentally designed to handle sequential data 1011 Unlike feedforward neural networks RNNs possess the unique feature of maintaining a memory of previous inputs by using their internal state memory to process sequences of inputs 12 This makes them ideally suited for applications such as natural language processing speech recognition and time series forecasting where context and the order of data points are crucial Copyright'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='the order of data points are crucial Copyright  2024 by the authors Licensee MDPI Basel Switzerland This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution CC BY license https creativecommonsorglicensesby The inception of RNNs dates back to the 1970s with foundational work by Werbos 13 which introduced the concept of backpropagation through time BPTT that laid the foun dation for training recurrent neural networks However RNNs struggled with practical applications due to the vanishing gradient problem where gradients either grow or shrink exponentially during backpropagation 14 Meanwhile the introduction of Long Short Term Memory LSTM networks by Hochreiter and Schmidhuber 15 was a turning point for RNNs allowing for the learning of dependencies over much longer periods Addi tionally gated recurrent units GRUs proposed by Cho et al 16 offered a simplified alternative to LSTM while maintaining comparable performance 40 Information 2024 15 517 httpsdoiorg103390info15090517 httpswwwmdpicomjournalinformation Information 2024 15 517 Over the years these RNN architectures have been applied in different fields achieving excellent performance 1719 Despite their advancements and adoption in various fields RNNs have continued to evolve Specifically the increasing complexity of data and tasks in recent years has driven continuous innovations in RNN architectures and variants These developments have expanded the application of RNNs from simple sequence prediction to complex tasks such as multimodal learning and realtime decisionmaking systems Recent studies and reviews have highlighted the significant progress made in the field of RNNs For example Lipton et al 20 provided an overview of the theoretical foundations and applications of RNNs while Yu et al 21 focused on the LSTM cell and different variants Additionally Tarwani et al 22 reviewed the application and role of RNNs in natural language processing However many of these reviews do not fully capture the latest advancements and applications given the rapid pace of innovation in this field Additionally there remains a gap in the literature that comprehensively covers the latest advancements in RNN architectures and their applications across a broader range of fields Therefore this paper aims to fill that gap by providing a comprehensive review of RNNs assessing their theoretical advancements and practical implementations as well as cuttingedge applications thus helping shape future research on neural networks The rest of this paper is organized as follows Section 2 reviews related works Section 3 covers the fundamentals of RNNs including basic architecture and components Section 4 explores advanced RNN variants such as LSTM and GRU Section 5 highlights innovations in RNN architectures and training methodologies Section 6 presents some publicly available datasets used for RNN studies Section 7 discusses various applications of RNNs in the'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='7 discusses various applications of RNNs in the literature Section 8 addresses challenges and future research directions Finally Section 9 concludes the study 2 Related Works RNNs have been applied in different applications achieving stateoftheart perfor mance especially in timeseries applications Early developments in RNNs including locally recurrent globally feedforward networks as reviewed by Tsoi and Back 23 and the blockdiagonal recurrent neural networks proposed by Mastorocostas and Theocharis 24 laid important groundwork for understanding complex sequence modeling Several reviews have been conducted on RNNs and their applications each contribut ing to the understanding and development of the field For instance Dutta et al 25 provided a comprehensive overview of the theoretical foundations of RNNs and their applications in sequence learning Their review highlighted the challenges associated with training RNNs particularly the vanishing gradient problem and discussed the ad vancements in LSTM and GRU architectures However the review primarily focused on the theoretical aspects and applications of RNNs and did not extensively cover the latest innovations and practical applications in emerging fields such as bioinformatics and autonomous systems Quradaa et al 26 presented a startoftheart review of RNNs covering the core architectures with a focus on applications in code clones Similarly the review by Tarwani et al 22 provided an indepth analysis of RNNs in the context of NLP While this review offered valuable insights into the advancements in NLP it lacked a broader perspective on other application domains and recent architec tural innovations Another significant review by Goodfellow et al 27 focused on the fundamentals of deep learning including RNNs and discussed their applications across various domains This review provided a solid foundation but did not delve deeply into the specific advancements in RNN architectures and their specialized applications Greff et al 28 conducted an extensive study comparing various LSTM variants to determine their effectiveness in different applications While this review provided a thorough comparison it primarily focused on LSTM architectures and did not address other RNN variants or the latest hybrid models In similar research AlSelwi et al 29 reviewed LSTM applications in the literature covering articles from the 20182023 time period Zaremba et al 30 reviewed the use of RNNs in language modeling highlighting 2 of 34 Information 2024 15 517 significant achievements and the ongoing challenges in this field Their work offered valuable insights into the application of RNNs in NLP but was limited to language modeling and did not explore other potential applications Bai et al 31 provided a critical review of RNNs and their variants comparing them with other sequence modeling techniques like CNNs and attentionbased models Che et al 32 focused on the application of RNNs in healthcare particularly for electronic'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='of RNNs in healthcare particularly for electronic health records EHRs analysis and disease prediction This review highlighted the potential of RNNs in medical applications Furthermore more recent studies have explored various new aspects and applications of RNNs For example Chung et al 33 explored the advancements in RNN architectures over the past decade focusing on improvements in training techniques optimization methods and new architectural innovations This review provided an extensive survey of recent developments and their implications for future research Badawy et al 34 provided a comprehensive overview of the use of RNNs in healthcare particularly for predictive analytics and patient monitoring They discussed the integration of RNNs with other ML techniques and the challenges in deploying these models in clinical settings Ismaeel et al 35 examined the application of RNNs in smart city technologies includ ing traffic prediction energy management and urban planning Their review discussed the potential and limitations of RNNs in these areas and suggested avenues for future research Meanwhile Mers et al 36 reviewed the applications of RNNs in pavement performance forecasting and conducted a comprehensive performance comparison of the various RNN models including simple RNNs LSTM GRU and hybrid LSTMfully connected neural networks LSTMFCNNs Chen et al 37 focused on the use of RNNs in environmental monitoring and climate modeling discussing their effectiveness in predicting environmental changes and managing natural resources They also highlighted the challenges in modeling complex environmental systems with RNNs Linardos et al 38 investigated the advancements in RNNs for natural disaster prediction and management highlighting the successes and challenges in using RNNs for early warning systems disaster response and recovery planning Zhang et al 39 discussed RNN applications in robotics particularly focusing on path planning motion control and humanrobot interaction They discussed the integration of RNNs with other DL techniques in robotics The different related studies are tabulated in Table 1 including their main contributions This research addresses the limitations in the existing literature by providing a more comprehensive review that includes the latest developments in RNN architectures such as hybrid models and neural architecture search as well as their applications across a wider range of domains Additionally this review contributes to a more holistic understanding of the current state and future directions of RNN research by integrating discussions on scalability robustness and interoperability Table 1 Summary of related reviews on RNNs Reference Year Description Zaremba et al 30 Chung et al 33 Goodfellow et al 27 Greff et al 28 Tarwani et al 22 Chen et al 37 Bai et al 31 2014 2014 2016 2016 2017 2018 2018 Insights into RNNs in language modeling Survey of advancements in RNN training optimiza tion and architectures Review on deep'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='optimiza tion and architectures Review on deep learning including RNNs Extensive comparison of LSTM variants Indepth analysis of RNNs in NLP Effectiveness of RNNs in environmental monitoring and climate modeling Comparison of RNNs with other sequence modeling techniques like CNNs and attention mechanisms Potential of RNNs in medical applications Che et al 32 2018 3 of 34 Information 2024 15 517 Table 1 Cont Reference Year Description Zhang et al 39 Dutta et al 25 Linardos et al 38 Badawy et al 34 Ismaeel et al 35 Mers et al 36 Quradaa et al 26 2020 2022 2022 2023 2023 2023 2024 RNN applications in robotics including path plan ning motion control and humanrobot interaction Overview of RNNs challenges in training and ad vancements in LSTM and GRU for sequence learning RNNs for early warning systems disaster response and recovery planning in natural disaster prediction Integration of RNNs with other ML techniques for pre dictive analytics and patient monitoring in healthcare Application of RNNs in smart city technologies in cluding traffic prediction energy management and urban planning Performance comparison of various RNN models in pavement performance forecasting Startoftheart review of RNNs covering core archi tectures with a focus on applications in code clones Review of LSTM applications from 2018 to 2023 AlSelwi et al 29 2024 3 Fundamentals of RNNs 31 Basic Architecture and Working Principle of Standard RNNs RNNs are designed to process sequential data by maintaining a hidden state that captures information about previous inputs 40 The basic architecture consists of an input layer a hidden layer and an output layer Unlike feedforward neural networks RNNs have recurrent connections as shown in Figure 1 allowing information to cycle within the networks At each time step t the RNN takes an input vector xt and updates its hidden state ht using the following equation ht  hWxhxt  Whhht1  bh where Wxh is the weight matrix between the input and hidden layer Whh is the weight matrix for the recurrent connection bh is the bias vector and h is the activation function typically the hyperbolic tangent function tanh or the rectified linear unit 4142 The output at each time step t is given by the following yt  yWhyht  by where Why is the weight matrix between the hidden and output layers by is the bias vector and y is the activation function for the output layer Figure 1 Basic RNN architecture 4 of 34 1 2 Information 2024 15 517 32 Activation Functions The core of RNN operations involves the recurrent computation of the hidden state which integrates the current input with the previous hidden state 43 This recurrent computation allows RNNs to exhibit dynamic temporal behavior The choice of activation function h plays a crucial role in the behavior of the network introducing nonlinearity that enables the network to learn and represent complex patterns in the data 4445 One commonly used activation function in RNNs is the hyperbolic tangent tanh The tanh'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='in RNNs is the hyperbolic tangent tanh The tanh function squashes the input values to the range of 1 1 making it zerocentered and suitable for modeling sequences with both positive and negative values 46 The tanh is represented mathematically as follows hz  tanhz  ez  ez ez  ez Another widely used activation function is the rectified linear unit ReLU The ReLU function outputs the input directly if it is positive otherwise it outputs zero 47 This simplicity helps mitigate the vanishing gradient problem to some extent by allowing gradients to flow through the network more effectively Furthermore the Leaky ReLU is a variant of the ReLU designed to address the dying ReLU problem where neurons can become inactive and stop learning 48 The leaky ReLU allows a small nonzero gradient when the input is negative thus keeping the neurons active during the training process Additionally the exponential linear unit ELU is another variant designed to bring the mean activation closer to zero which speeds up learning by reducing bias shifts 49 The ELU tends to improve learning characteristics over the ReLU by allowing the activations to take on negative values when the input is negative These activation functions are represented as follows hz  max0 z hz  cid40 if z  0 z otherwise z hz  cid40 if z  0 z ez  1 otherwise where  is a small constant typically 001 Meanwhile the sigmoid function squashes the input values to the range 0 1 It is similar to tanh but outputs values in a differ ent range making it useful for problems where the output needs to be interpreted as probabilities 5052 Similarly the softmax function is commonly used in the output layer of classification networks to convert raw scores into probabilities 53 It is particularly useful in multiclass classification problems The sigmoid and softmax functions are repre sented mathematically as follows hz  hzi  1 1  ez ezi j ezj where zi is the ith element of the input vector z Each of these activation functions has its advantages and is chosen based on the specific requirements of the task at hand Meanwhile the hidden state update in RNNs can be seen as a function ht  f xt ht1 which captures the dependencies between the input sequence and the recurrent connections The choice of h significantly affects how well the network learns these dependencies and generalizes to new data 5 of 34 3 4 5 6 7 8 Information 2024 15 517 33 The Vanishing and Exploding Gradient Problems Training RNNs presents significant challenges due to the vanishing and exploding gradient problems During the training process the BPTT algorithm is used to compute the gradients of the loss function with respect to the weights 54 As the gradients are propa gated backwards in time they can either diminish vanish or grow exponentially explode making it difficult for the network to learn longterm dependencies or causing instability during training Mathematically the hidden state at time step t can be expanded as follows ht  hWxhxt  WhhhWxhxt1'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='be expanded as follows ht  hWxhxt  WhhhWxhxt1  Whhht2  bh  bh When calculating the gradients we encounter terms involving the product of many Jacobian matrices ht htn  t1  ktn Jk where Jk is the Jacobian matrix of the hidden state at time step k If the eigenvalues of Jk are less than 1 the product of these matrices will tend to zero as n increases leading to vanishing gradients 5556 Conversely if the eigenvalues of Jk are greater than 1 the gradients can grow exponentially leading to exploding gradients which can cause the model parameters to become unstable and result in numerical overflow during training The vanishing gradient problem prevents the network from effectively learning longterm dependencies as the gradient signal becomes too weak to update the weights meaningfully for earlier layers On the other hand the exploding gradient problem can cause the model to converge too quickly to a poor local minimum or make the training process fail entirely due to excessively large updates To mitigate these problems various RNN variants have been developed such as LSTM and GRUs These architectures introduce gating mechanisms that regulate the flow of information and gradients through the network allowing for the better handling of longterm dependencies Additionally gradient clipping is a common technique used to prevent exploding gradients by capping the gradients at a maximum threshold during backpropagation ensuring that they do not grow uncontrollably 5758 34 Bidirectional RNNs Bidirectional RNNs BiRNNs enhance the architecture by processing the sequence in both forward and backward directions This allows the network to have access to future context as well as past context improving its performance in tasks for which understanding both the preceding and succeeding elements is crucial 4359 In BiRNNs two hidden  h t states are maintained one for the forward pass   h t and one for the backward pass   h t  hWxhxt  Whh  h t  hWxhxt  Whh  h t1  bh  h t1  bh The output yt is then computed by concatenating the forward and backward hidden states yt  yWhy  h t  h t  by where   denotes concatenation Furthermore BiRNNs are effective for tasks such as named entity recognition machine translation and speech recognition where context from both directions improves the models performance 6061 Through accessing information from both the past and future BiRNNs can provide a more comprehensive understanding of the input sequence For instance in language modeling understanding the surrounding words can significantly enhance the accuracy of predicting the next word 6263 In machine translation knowing the entire sentence allows the network to translate words more accurately considering the entire context Additionally BiRNNs are also used in 6 of 34 9 10 11 12 13 Information 2024 15 517 various time series applications such as stock price prediction and medical diagnosis where understanding the temporal dependencies in both directions is beneficial 64 However BiRNNs'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='both directions is beneficial 64 However BiRNNs require more computational resources than unidirectional RNNs due to the need to process the sequence twice forward and backwards 5965 35 Deep RNNs Deep RNNs extend the basic architecture by stacking multiple RNN layers on top of each other which allows the network to learn more complex representations 66 Each layers hidden state serves as the input to the subsequent layer enhancing the models capacity to capture hierarchical features For a deep RNN with L layers the hidden states at layer l and time step t are updated as follows h l t  hW l xh h l1 t  W l hh h l t1  b l h  0 t  xt represents the input at the first layer The output at the topmost layer is where h then computed using the same procedure as in basic RNNs yt  yWhyh L t  by Deep RNNs can model more complex sequences and capture longer dependencies than shallow RNNs 67 However they are also more prone to the vanishing gradient problem which can be mitigated by using advanced variants like LSTM or GRUs Deep RNNs have been successfully applied in various domains including natural language processing speech recognition and video analysis In NLP deep RNNs can model com plex linguistic structures and capture longrange dependencies improving tasks such as machine translation and text generation However training deep RNNs can be challenging due to the increased complexity and the risk of overfitting 6869 Techniques such as dropout layer normalization and residual connections are often employed to improve the training process and generalization of deep RNNs 7072 Dropout helps prevent overfit ting by randomly setting a fraction of the units to zero during training which encourages the network to learn more robust features while batch normalization helps stabilize and accelerate training by normalizing the inputs to each layer 73 Residual connections which add shortcut connections that bypass one or more layers help mitigate the vanishing gradient problem in very deep networks 74 4 Advanced Variants of RNNs RNN architectures can vary significantly with some featuring internal recurrence within neurons and others having external recurrence between layers These variations impact the networks ability to learn and process sequences influencing their application to specific tasks 41 Long ShortTerm Memory Networks LSTM networks were introduced by Hochreiter and Schmidhuber 15 to address the vanishing gradient problem inherent to basic RNNs The key innovation in LSTM is the use of gating mechanisms to control the flow of information through the network This allows LSTM networks to maintain and update their internal state over long periods making them effective for tasks requiring the modeling of longterm dependencies Each LSTM cell contains three gates the input gate forget gate and output gate which regulate the cell state ct and hidden state ht 75 These gates determine how much of the input to consider how much of the previous state to forget and how'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='how much of the previous state to forget and how much of the cell state to output The LSTM update equations are as follows it  Wxixt  Whiht1  bi ft  Wx f xt  Wh f ht1  b f  7 of 34 14 15 16 17 Information 2024 15 517 ot  Wxoxt  Whoht1  bo gt  tanhWxgxt  Whght1  bg ct  ft  ct1  it  gt ht  ot  tanhct where it is the input gate ft is the forget gate ot is the output gate gt is the cell input ct is the cell state ht is the hidden state  represents the sigmoid function tanh is the hyperbolic tangent function and  denotes elementwise multiplication 75 Figure 2 illustrates the internal architecture of an LSTM cell which effectively manages longterm dependencies in sequence data by employing three crucial gating mechanisms the input gate it forget gate ft and output gate ot Each of these gates plays a distinct role in controlling the flow of information through the cell The input gate controls how much of the new input xt is written to the cell state ct The forget gate decides how much of the previous cell state ct1 should be retained The output gate determines how much of the cell state ct is used to compute the hidden state ht The cell input gt is a candidate value that is added to the cell state after being modulated via the input gate The use of these gating mechanisms allows LSTM networks to selectively remember or forget information enabling them to handle longterm dependencies more effectively than traditional RNNs The internal recurrence within the LSTM cell is managed through the cell state ct which acts as a conveyor belt transferring relevant information across different time steps This recurrence mechanism allows the LSTM to maintain and update its memory over long sequences effectively capturing longterm dependencies Additionally the elementwise multiplication operations between the gates and their respective inputs ensure that the interactions between different components of the LSTM are smooth and efficient This enables the LSTM to perform complex transformations on the input data while maintaining the stability of the learning process Meanwhile LSTM networks utilize internal recurrence within each cell to manage longterm dependencies with the recurrence happening through the cell state as information is passed from one time step to the next 21 Other LSTM variants include bidirectional LSTM BiLSTM and stacked LSTM Figure 2 Architecture of the LSTM network 41 42 Bidirectional LSTM Bidirectional LSTM shown in Figure 3 extends the standard LSTM architecture by processing the sequence in both forward and backward directions similar to BiRNNs 76 This approach allows the network to capture context from both the past and the future enhancing its ability to understand dependencies in the sequence more comprehensively In BiLSTM two separate hidden states are maintained for each time step one for the  h t These hidden states are computed forward pass  as described in Equations 11 and 12 BiLSTM features external recurrence between layers as they'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='external recurrence between layers as they process the input sequence in both forward and backward directions maintaining separate hidden states for each direction  h t and one for the backward pass  8 of 34 18 19 20 21 Information 2024 15 517 Figure 3 Architecture of BiLSTM network 41 Stacked LSTM Stacked LSTM involves stacking multiple LSTM layers in which the output of one LSTM layer serves as the input to the next as shown in Figure 4 This deep architecture allows the network to capture more complex patterns and dependencies in the data by learning hierarchical representations at different levels of abstraction For a stacked LSTM with L layers the hidden states at layer l and time step t are updated as described in Equations 14 and 15 Stacked LSTM incorporates external recurrence by connecting multiple LSTM layers in which each layer passes its output as input to the next allowing the network to capture more complex temporal patterns Figure 4 A stacked LSTM 41 Stacking LSTM layers allows the network to learn increasingly complex features and representations The lower layers can capture local patterns and shortterm dependencies while the higher layers can capture more abstract features and longterm dependencies 21 This hierarchical learning is advantageous for tasks such as language modeling where different levels of syntactic and semantic information need to be captured or for video analysis where temporal dependencies at different time scales must be understood While stacked LSTM offers improved modeling capabilities they also come with increased computational complexity and a higher risk of overfitting 43 Gated Recurrent Units Gated recurrent units are another variant designed to address the vanishing gradient problem while simplifying the LSTM architecture Introduced by Cho et al 16 GRUs combine the forget and input gates into a single update gate and merge the cell state and hidden state reducing the number of gates and parameters and thus simplifying the model and making it computationally more efficient The GRU architecture consists of two gates the update gate zt and the reset gate rt 77 Figure 5 shows the GRU architecture The gates control the flow of information to ensure that relevant information is retained and irrelevant information is discarded Similar to LSTM GRUs rely on internal recurrence within each unit as they maintain and update the hidden state across time steps to capture temporal dependencies The updated equations for GRUs are as follows zt  Wxzxt  Whzht1  bz rt  Wxrxt  Whrht1  br h t  tanhWxhxt  rt  Whhht1  bh ht  1  zt  ht1  zt  h t 9 of 34 22 23 24 25 Information 2024 15 517 where zt is the update gate rt is the reset gate and h t is the candidate hidden state The update gate zt determines how much of the previous hidden state ht1 should be carried forward to the current hidden state ht while the reset gate rt controls how much of the previous hidden state to forget 75 The candidate hidden state h t represents the'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='75 The candidate hidden state h t represents the new content to be added to the current hidden state modulated via the reset gate Furthermore the simplified architecture of GRUs allows them to be computationally more efficient than LSTM while still addressing the vanishing gradient problem This efficiency makes GRUs wellsuited for tasks where computational resources are limited or when training needs to be faster GRUs have been successfully applied in various sequence modeling tasks Their ability to capture longterm dependencies with fewer parameters makes them a popular choice in many applications Additionally studies have shown that GRUs can achieve performance comparable to LSTM 7880 making them an attractive alternative for many use cases Figure 5 Architecture of the GRU network Comparison with LSTM GRUs have fewer parameters compared to LSTM due to the absence of a separate cell state and combined gating mechanisms 81 This often leads to faster training times and comparable performance to LSTM in many tasks However despite their advantages the choice between GRUs and LSTM often depends on the specific task and dataset Some tasks may benefit more from the additional complexity and gating mechanisms of LSTM while others may perform equally well with the simpler GRU architecture 44 Other Notable Variants While LSTM and GRUs are the most widely used RNN variants other architectures like peephole LSTM echo state networks and independently recurrent neural networks offer unique advantages for specific applications 441 Peephole LSTM Peephole LSTM introduced by Gers and Schmidhuber 82 enhances standard LSTM by allowing the gates to have access to the cell state through peephole connections This additional connection enables the LSTM to better regulate the gating mechanisms based on the current cell state improving timing decisions in applications such as speech recognition and financial time series prediction 83 In the following equations the input gate it forget gate ft and output gate ot are enhanced with peephole connections it  Wxixt  Whiht1  Wcict1  bi where it is the input gate and Wci is the peephole weight connecting the cell state ct1 to the input gate ft  Wx f xt  Wh f ht1  Wc f ct1  b f  10 of 34 26 27 Information 2024 15 517 where ft is the forget gate and Wc f is the peephole weight connecting the cell state ct1 to the forget gate ot  Wxoxt  Whoht1  Wcoct  bo where ot is the output gate and Wco is the peephole weight connecting the cell state ct to the output gate 442 Echo State Networks Echo state networks ESNs proposed by Jaeger 84 represent a class of RNNs in which the hidden layer also known as the reservoir is fixed and randomly connected while only the output layer is trained This architecture significantly simplifies the training process making ESNs particularly suitable for realtime signal processing time series prediction and adaptive control systems The state update and output computation in ESNs are achieved through the'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='computation in ESNs are achieved through the following equations ht  tanhWinxt  Wresht1 where ht is the hidden state Win is the input weight matrix and Wres is the fixed randomly initialized reservoir weight matrix When it is assumed that Wout is the trained output weight matrix the output of the network can be represented as follows yt  Woutht ESNs have gained significant attention due to their ability to handle complex tem poral dynamics with a relatively simple and efficient training process 8586 However traditional ESNs are limited due to the fixed nature of the reservoir which can restrict their adaptability and performance in more complex tasks To address these limitations several advancements have been proposed Deep EchoState Networks Recent research has extended the ESN architecture to deeper variants known as deep echostate networks DeepESNs In DeepESNs multi ple reservoir layers are stacked allowing the network to capture hierarchical temporal features across different timescales 87 Each layer in a DeepESN processes the output from the previous layers reservoir enabling the model to learn more abstract and complex representations of the input data The state update for a DeepESN can be generalized as follows t  tanhWl hl where l denotes the layer number hl in is the in put weight matrix for layer l and hl1 is the hidden state from the previous layer DeepESNs have demonstrated improved performance in tasks requiring the mod eling of complex temporal patterns such as speech recognition and financial time series forecasting 88 Ensemble Deep ESNs In ensemble deep ESNs multiple DeepESNs are trained in dependently and their outputs are combined to form the final prediction 89 This ensemble approach leverages the diversity of the reservoirs and the deep architecture to improve robustness and accuracy particularly in time series forecasting applica tions For instance Gao et al 90 demonstrated the effectiveness of Deep ESN en sembles in predicting significant wave heights where the ensemble approach helped mitigate the impact of reservoir initialization variability and improved the models generalization ability Input Processing with Signal Decomposition Another critical aspect of effectively utilizing RNNs and ESNs is the preprocessing of input signals Given the complex and often noisy nature of realworld time series data signal decomposition techniques such as the empirical wavelet transform EWT have been employed to enhance the input to ESNs 91 The EWT decomposes the input signal into different frequency Deep EchoState Networks Recent research has extended the ESN architecture to deeper variants known as deep echostate networks DeepESNs In DeepESNs multi ple reservoir layers are stacked allowing the network to capture hierarchical temporal features across different timescales 87 Each layer in a DeepESN processes the output from the previous layers reservoir enabling the model to learn more abstract and complex representations of the input data'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='and complex representations of the input data The state update for a DeepESN can be generalized as follows t  tanhWl hl where l denotes the layer number hl in is the in put weight matrix for layer l and hl1 is the hidden state from the previous layer DeepESNs have demonstrated improved performance in tasks requiring the mod eling of complex temporal patterns such as speech recognition and financial time series forecasting 88 Ensemble Deep ESNs In ensemble deep ESNs multiple DeepESNs are trained in dependently and their outputs are combined to form the final prediction 89 This ensemble approach leverages the diversity of the reservoirs and the deep architecture to improve robustness and accuracy particularly in time series forecasting applica tions For instance Gao et al 90 demonstrated the effectiveness of Deep ESN en sembles in predicting significant wave heights where the ensemble approach helped mitigate the impact of reservoir initialization variability and improved the models generalization ability Input Processing with Signal Decomposition Another critical aspect of effectively utilizing RNNs and ESNs is the preprocessing of input signals Given the complex and often noisy nature of realworld time series data signal decomposition techniques such as the empirical wavelet transform EWT have been employed to enhance the input to ESNs 91 The EWT decomposes the input signal into different frequency 11 of 34 28 29 30 Information 2024 15 517 12 of 34 components allowing the ESN to process each component separately and improve the models ability to capture underlying patterns The combination of the EWT with ESNs has shown promising results in various applications including time series forecasting where it helps reduce noise and enhance the predictive performance of the model 443 Independently Recurrent Neural Network independently recurrent neural networks IndRNNs proposed by Li et al 92 have used independent recurrent units to address the gradient vanish ing and exploding problems making it easier to train very deep RNNs This architecture is useful for long sequence tasks such as video sequence analysis and long text generation When it is assumed that ht is the hidden state Wxh is the input weight matrix and u is a vector of recurrent weights The state update equation for IndRNN is as follows Most recently ht  Wxhxt  u  ht1 32 The various RNN architectures are summarized in Table 2 Table 2 Comparative overview of RNN architectures RNN Type Key Features Gradient Stability Typical Applications Basic RNN Simple structure with shortterm memory High risk of vanishing gradients Simple sequence tasks like text generation LSTM Longterm memory with input forget and output gates Stable handles vanish ing gradients well Language speech recognition translation GRU Simplified LSTM with fewer gates Stable handles van ishing gradients effec tively Tasks training than LSTM requiring faster Bidirectional RNN Processes data in both forward and backward directions'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='data in both forward and backward directions for better context understanding Medium stability de pends on depth Speech recognition and sentiment analysis Deep RNN Multiple RNN layers are stacked to learn hi erarchical features Variable and the risk of vanishing gradients increases with depth Complex sequence model ing like video processing ESN Fixed hidden layer weights trained only at the output Not applicable as train ing bypasses typical gradient issues Time series prediction and system control Peephole LSTM Adds peephole connec tions to LSTM gates Stable and similar to LSTM Recognition of complex temporal patterns like mu sical notation IndRNN Allows training of deeper networks by maintaining indepen dence between time steps Reduces risk of vanish ing and exploding gra dients Very long sequences such as in video processing or long text generation 5 Innovations in RNN Architectures and Training Methodologies In recent years there have been significant innovations in RNN architectures and train ing methodologies aimed at enhancing performance and addressing existing limitations Information 2024 15 517 51 Hybrid Architectures Combining RNNs with other neural network architectures has led to hybrid models that leverage the strengths of each component For example integrating CNNs with RNNs has proven effective in video analysis where CNNs handle spatial features while RNNs capture temporal dynamics 9393 This approach allows the model to process both spatial and temporal information enhancing its ability to recognize patterns and make predictions Furthermore incorporating attention mechanisms into RNNs has also improved their ability to model longrange dependencies Attention mechanisms enable the network to focus on relevant parts of the input sequence which is useful in tasks such as machine translation and text summarization The attention mechanism can be described as follows at  softmaxut ct  T  i1 atihi where at is the attention weight ut is the score function and ct is the context vector 52 Neural Architecture Search Neural architecture search NAS has automated the design of RNN architectures enabling the discovery of more efficient and powerful models 9495 NAS techniques such as those pioneered by Zoph and Le 96 explore various combinations of layers activation functions and hyperparameters to find optimal configurations that outperform manually designed architectures The NAS process can be formulated as an optimization problem A  arg max AS AccuracyA where A represents an architecture S is the search space and A is the optimal architecture 53 Advanced Optimization Techniques Advanced optimization techniques have been developed to improve the training efficiency and stability of RNNs Gradient clipping is a technique used to prevent the gradients from becoming too large which can destabilize training 5897 g  g max1 g    where g is the gradient and  is the threshold value Furthermore adaptive learning rates such as those used in the Adam'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='learning rates such as those used in the Adam optimizer adjust the learning rate during training to accelerate convergence and improve performance 98 The Adam optimizer updates the parameters using the following mt  1mt1  1  1gt vt  2vt1  1  2g2 t  vt 1  t 2 mt 1  t 1 mt  vt    t  t1   mt vt    where mt and vt are the first and secondmoment estimates 1 and 2 are the decay rates  is the learning rate and  is a small constant 98 Also secondorder optimization methods such as the Hessianfree optimizer have also been explored to improve the convergence speed and stability of training deep networks 13 of 34 33 34 35 36 37 38 39 40 Information 2024 15 517 14 of 34 54 RNNs with Attention Mechanisms Integrating attention mechanisms into RNNs allows these networks to selectively focus on important parts of the input sequence addressing the limitations of traditional RNNs in modeling longterm dependencies 99101 This hybrid approach combines the strengths of RNNs and attention mechanisms enhancing their capability to handle complex sequence tasks Attentionenhanced RNNs have shown significant improvements in tasks such as speech recognition and text summarization For example Bahdanau et al 102 demonstrated the use of attention mechanisms in neural machine translation which al lowed RNNs to focus on relevant words in the source sentence improving translation accuracy Similarly Luong et al 103 proposed global and local attention mechanisms further enhancing the performance of RNNs in various sequencetosequence tasks 55 RNNs Integrated with Transformer Models Transformers introduced by Vaswani et al 104 in 2017 employ selfattention mecha nisms and have proven to be highly effective in capturing longrange dependencies Unlike RNNs transformers process sequences in parallel which can lead to better performance on long sequences The selfattention mechanism is defined as follows AttentionQ K V  softmax cid32 QKT  dk cid33 V 41 where Q K and V are the query key and value matrices respectively and dk is the dimension of the keys Considering that both transformer and RNN architecture have limitations studies have integrated both methods to obtain robust models as shown in the recent literature 104 Therefore researchers can develop more powerful and efficient models for a wide range of applications by leveraging the sequential processing capabilities of RNNs and the parallel attentionbased mechanisms of transformers This integrated approach addresses the limitations of each architecture and enhances the overall performance in sequence modeling tasks 6 Public Datasets for RNN Research This section provides an overview of publicly available datasets that are commonly used in the study and evaluation of RNNs These datasets cover a variety of applications ranging from natural language processing to time series forecasting reflecting the diverse capabilities of RNNs Each of these datasets provides a unique challenge for RNNs allowing researchers to explore the strengths and'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='allowing researchers to explore the strengths and limitations of different RNN architectures across various realworld tasks Table 3 summarizes the publicly available datasets for RNN research Table 3 Public datasets for studying RNNs Dataset Name Application Description Penn Treebank 105 Natural language processing A corpus of English sentences annotated for partofspeech tagging parsing and named en tity recognition widely used for language mod eling with RNNs IMDB Reviews 106 Sentiment analysis A dataset of movie reviews used for binary sen timent classification suitable for studying the effectiveness of RNNs in text sentiment classifi cation tasks MNIST Sequential 107 Image recognition A version of the MNIST dataset formatted as sequences for studying sequencetosequence learning with RNNs Information 2024 15 517 15 of 34 Table 3 Cont Dataset Name Application Description TIMIT Speech Corpus 108 Speech recognition An annotated speech database used for auto matic speech recognition systems Reuters21578 Text Categorization Collection 109 Text categorization A collection of newswire articles that is a com mon benchmark for text categorization and NLP tasks with RNNs UCI ML Reposi tory Time Series Data 110 Time series analysis Contains various time series datasets including stock prices and weather data ideal for forecast ing with RNNs CORe50 Dataset 111 Object Recognition Used for continuous object recognition ideal for RNN models dealing with video input se quences where object persistence and temporal context are important 7 Applications of RNNs in PeerReviewed Literature RNNs and their variants have been extensively studied and applied across various domains in the peerreviewed literature This section provides a comprehensive review of these applications 71 Natural Language Processing RNNs have transformed the field of NLP by enabling more sophisticated and context aware models Several studies have demonstrated the effectiveness of RNNs in various NLP tasks 711 Text Generation RNNs have been used extensively for textgeneration tasks Souri et al 112 demon strated the use of RNNs to generate coherent and contextually relevant Arabic text Their model was trained on a large corpus of text data allowing it to learn the probability distribu tion of word sequences which proved effective in generating humanlike text Meanwhile several researchers have proposed novel approaches to enhancing the performance of RNNs in text generation For instance Islam 113 introduced a sequencetosequence framework that improved the generation quality using LSTM This method allowed the network to handle longer sequences and maintain coherence over extended text Gajendran et al 114 demonstrated the effectiveness of RNNs in generating character level text Their work showed that BiLSTM could capture a wide range of patterns from characterlevel dependencies to higherlevel syntactic structures making them versatile for different text generation tasks including the generation of'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='text generation tasks including the generation of code literature and poetry More recently advancements in RNNbased text generation have focused on improving the diversity and coherence of generated text Hu et al 115 proposed the use of variational autoencoders VAEs combined with RNNs to enhance the creativity of text generation Their approach enabled the generation of diverse and contextually rich sentences by learning a latent space representation of the text Meanwhile Holtzman et al 116 introduced the concept of controlled text generation using RNNs which allowed users to influence the style and content of the generated text This method provided more flexibility and control over the text generation process making it useful for applications such as creative writing and personalized content generation Additionally with the advent of more sophisticated models like transformers RNNbased text generation has evolved to incorporate attention mechanisms Yin et al 117 proposed an approach combining RNN with an attention mechanism which allows the model to focus on relevant parts of the input sequence during the genera Information 2024 15 517 tion process This significantly improved the quality and coherence of the generated text by dynamically adjusting the focus of the model based on the context Hussein and Savas 118 employed LSTM for text generation Similarly Baskaran et al 119 employed LSTM for text generation achieving excellent performance These studies showed that LSTM networks are capable of generating texts that are contextually relevant and linguistically accurate Furthermore studies have continued to explore and enhance the capabilities of RNNs in text generation Keskar et al 120 introduced a largescale language model known as Conditional Transformer Language CTRL which can be conditioned on specific control codes to generate text in various styles and domains This work highlights the growing trend of combining RNNs with transformer architectures to leverage their strengths in sequence modeling and text generation Additionally Guo 121 explored the integra tion of reinforcement learning with RNNs for text generation The approach aimed to optimize the generation process by rewarding the model for producing highquality con textually appropriate text thereby improving both the coherence and relevance of the generated content In text generation tasks the LSTM networks have proven to be the most effective among RNN architectures The LSTMs ability to manage longterm dependencies through its gating mechanisms makes it wellsuited for generating coherent and contextually rel evant text over extended sequences Studies such as those by Souri et al 112 and Gajendran et al 114 highlight the versatility of LSTM in handling both wordlevel and characterlevel text generation tasks respectively While more recent models such as those incorporating transformers have gained popularity LSTMbased models continue to be preferred for scenarios requiring robust'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='to be preferred for scenarios requiring robust sequence modeling with fewer computa tional resources especially when dealing with smaller datasets where the complexity of transformers might not be necessary 712 Sentiment Analysis In sentiment analysis RNNs have been shown to outperform traditional models by capturing the context and details of sentiment expressed in text Yadav et al 122 used LSTMbased models to analyze customer reviews and social media posts achiev ing notable improvements in accuracy over conventional methods Building on this Abimbola et al 123 proposed a hybrid LSTMCNN model for documentlevel sentiment classification which first captures the sentiment of individual sentences and then aggre gates them to determine the overall sentiment of the document This hierarchical approach allows for a more detailed understanding of sentiment especially in long and complex texts Zulqarnain et al 124 utilized the concept of attention mechanisms and GRU to enhance sentiment analysis By allowing the model to focus on specific parts of the input text that are most indicative of sentiment attention mechanisms significantly improved the inter pretability and performance of sentiment analysis models This advancement enabled the models to highlight which words or phrases contribute the most to sentiment prediction Additionally several studies have explored the integration of RNNs with CNNs to leverage the strengths of both architectures For instance Pujari et al 125 combined CNNs and RNNs to capture both local features and longrange dependencies in text resulting in a hybrid model that achieved stateoftheart performance in sentiment classification tasks Meanwhile Wankhade et al 126 employed the fusion of CNN and BiLSTM with an attention mechanism leading to enhanced sentiment classification Furthermore Sangeetha and Kumaran 127 utilized BiLSTM to enhance the sentiment analysis capability by processing text in both forward and backward directions This approach captures the context from both past and future words providing a more comprehensive understanding of the sentiment expressed in the text In addition to these architectural innovations there has been a focus on improving the robustness of RNNbased sentiment analysis models For example He and McAuley 128 developed an adversarial training framework that enhances the models ability to handle noisy and adversarial text inputs thereby improving its generalization to realworld data 16 of 34 Information 2024 15 517 Also the use of transfer learning and pretrained language models such as BERT and GPT has been increasingly popular in sentiment analysis 129131 These models finetuned for sentiment classification tasks have demonstrated exceptional performance by leveraging largescale pretraining on diverse text corpora and then adapting to specific sentiment analysis datasets Furthermore BiLSTM can be considered the most effective variant of RNNs in senti ment analysis due to their ability to process'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='ment analysis due to their ability to process text in both forward and backward directions This bidirectional processing allows the model to capture the full context of a sentence making it effective in understanding intrinsic sentiment expressed in text Studies by Sangeetha and Kumaran 127 demonstrate the superiority of BiLSTM in achieving higher accuracy in sentiment classification tasks The ability of BiLSTM to integrate with attention mechanisms as shown in work by Wankhade et al 126 further enhances their perfor mance by allowing the model to focus on the most relevant parts of the text thus improving interpretability and classification accuracy 713 Machine Translation To address the challenge of translating long sentences Wu et al 132 introduced the concept of deep RNNs with multiple layers in both the encoder and decoder Their model known as Google Neural Machine Translation GNMT improved translation accuracy and fluency by capturing more complex patterns and dependencies within the text GNMT became a major achievement in neural machine translation setting a new benchmark for translation systems Sennrich et al 133 presented a method for incorporating subword units into RNNbased translation models This approach known as BytePair Encoding BPE enabled the translation models to handle rare and outofvocabulary words more effectively by splitting them into smaller more frequent subword units This method improved the robustness and generalization of the translation models With the advent of transformer models Vaswani et al 104 revolutionized the field of machine translation by introducing a fully attentionbased architecture that eliminated the need for recurrence entirely Transformers demonstrated superior performance in translation tasks by allowing for parallel processing of sequences and capturing longrange dependencies more efficiently Despite this shift RNNbased models with attention mecha nisms continued to be relevant particularly in scenarios where computational resources were limited or sequential processing was preferred For example Kang et al 134 com bined RNN with an attention mechanism to obtain a bilingual attentionbased machine translation model While Zulqarnain et al 124 utilized GRU in a multistage feature attention mechanism model Several studies have also combined RNNs with transformer models to utilize the strengths of both architectures For instance Yang et al 135 proposed a hybrid model that integrates RNNs into the transformer architecture to enhance its ability to capture sequential dependencies while maintaining the efficiency of parallel processing This hybrid approach achieved stateoftheart performance in several translation benchmarks Meanwhile more recent studies have explored the integration of pretrained language models like BERT and GPT into machine translation systems Song et al 136 demonstrated that incorporating BERT into the encoder of a translation model enhanced its understanding of the source language'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='enhanced its understanding of the source language leading to more accurate and fluent translations Table 4 summarizes the discussed applications of RNNs in natural language processing Hybrid models that combine the strengths of RNNs particularly LSTM with trans former architectures are considered the best approach to machine translation While transformers have set new benchmarks in translation accuracy due to their parallel pro cessing capabilities and efficient handling of longrange dependencies integrating RNNs with attention mechanisms as seen in studies by Yang et al 135 and Song et al 136 has shown that these hybrid models can outperform purely transformerbased approaches in certain scenarios This is especially true in resourceconstrained environments where 17 of 34 Information 2024 15 517 Application Domain Text generation Sentiment analysis Machine Translation 18 of 34 the sequential processing of RNNs enhanced by attention mechanisms can lead to more accurate and computationally efficient translations Table 4 Summary of applications of RNNs in natural language processing Reference Year Methods and Application Souri et al 112 Holtzman et al 116 Hu et al 115 2018 2019 2020 RNNs for generating coherent and contextually relevant Arabic text Controlled text generation using RNNs content control VAEs text generation for style and combined with RNNs to enhance creativity in Gajendran et al 114 Hussein and Savas 118 Baskaran et al 119 Islam 113 Yin et al 117 Guo 121 Keskar et al 120 2020 Characterlevel text generation using BiLSTM for various tasks 2024 2024 LSTM for text generation LSTM for text generation achieving excellent performance Sequencetosequence framework using LSTM for improved text generation quality Attention mechanisms with RNNs for improved text generation quality Integration of text generation Conditional Transformer Language CTRL for generating text in various styles 2019 2018 reinforcement learning with RNNs 2015 2019 for He and McAuley 128 2016 Adversarial training framework for robustness in sentiment analysis Pujari et al 125 2024 Hybrid CNNRNN model for sentiment classification Wankhade et al 126 Sangeetha and Kumaran 127 Yadav et al 122 2024 2023 2023 Fusion of CNN and BiLSTM with attention mechanism for senti ment classification BiLSTM for both directions LSTMbased models for sentiment analysis in customer reviews and social media posts sentiment analysis by processing text in Zulqarnain et al 124 Samir et al 129 Prottasha et al 130 Abimbola et al 123 Mujahid et al 131 2024 Attention mechanisms and GRU for enhanced sentiment analysis 2021 Use of pretrained models like BERT for sentiment analysis 2022 Transfer learning with BERT and GPT for sentiment analysis Hybrid LSTMCNN model classification Analyzing sentiment with pretrained models finetuned for specific tasks for documentlevel sentiment 2024 2023 Sennrich et al 133 Wu et al 132 Vaswani et al 104 2015 2016 2017 BytePair Encoding for handling rare words in'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='2017 BytePair Encoding for handling rare words in translation models Google Neural Machine Translation with deep RNNs for im proved accuracy Fully attentionbased transformer models for superior translation performance Yang et al 135 Song et al 136 Kang et al 134 2017 Hybrid model integrating RNNs into the transformer architecture Incorporating BERT into translation models for enhanced under standing and fluency Bilingual attentionbased machine translation model combining RNN with attention 2019 2023 Zulqarnain et al 124 2024 Multistage feature attention mechanism model using GRU 72 Speech Recognition RNNs have also made significant contributions to the field of speech recognition leading to more accurate and efficient systems Hinton et al 137 explored the use of deep neural networks including RNNs for speechtotext systems Their research showed that RNNs could capture the temporal dependencies in speech signals leading to significant improvements in transcription accuracy compared to previous methods Information 2024 15 517 Hannun et al 138 introduced DeepSpeech a stateoftheart speech recognition system based on RNNs DeepSpeech employed a deep LSTM network trained on a vast amount of labeled speech data thereby improving transcription accuracy This system was designed to handle noisy environments and diverse accents making it robust for various realworld applications Similarly Amodei et al 139 presented DeepSpeech2 which extended the capabilities of the original DeepSpeech model by incorporating bidirectional RNNs and a more extensive dataset DeepSpeech2 achieved notable performance improve ments demonstrating that RNNs could effectively handle variations in speech patterns and accents Meanwhile Chiu et al 140 proposed the use of RNNtransducer RNNT models for endtoend speech recognition RNNT models integrate both acoustic and language models into a single RNN framework allowing for more efficient and accurate transcription This integration reduced the complexity and latency of realtime speech recognition systems making them more practical for deployment in realworld applications Furthermore Zhang et al 141 proposed the use of convolutional recurrent neural networks CRNNs for speech recognition CRNNs combine the strengths of CNNs for feature extraction and RNNs for sequence modeling resulting in a hybrid architecture that is robust in both accuracy and computational efficiency Specifically this model was effective in handling long audio sequences and varying speech rates Recently Dong et al 142 introduced the SpeechTransformer a model that lever ages the selfattention mechanism to process audio sequences in parallel improving both speed and accuracy This model demonstrated that transformerbased architectures could effectively handle the sequential nature of speech data providing a competitive alterna tive to traditional RNNbased models Bhaskar and Thasleema 143 developed a speech recognition model using LSTM The model achieved visual speech'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='model using LSTM The model achieved visual speech recognition using fa cial expressions Other studies that explored the use of different RNN variants in speech recognition include 144147 In the field of speech recognition LSTM networks have been consistently recognized as the most effective RNN variant due to their ability to capture longrange dependencies in sequential data LSTM as utilized in systems like DeepSpeech by Hannun et al 138 has demonstrated superior performance in handling the temporal dependencies inherent in speech signals This capability is particularly crucial for maintaining context over long audio sequences which directly translates to improved transcription accuracy While newer models like the SpeechTransformer 142 leverage attention mechanisms for faster processing LSTM networks remain a cornerstone in speech recognition due to their proven robustness and ability to handle complex variations in speech patterns This makes them the preferred choice in scenarios where maintaining sequential order and context is critical despite the growing popularity of transformerbased architectures 73 Time Series Forecasting RNNs have been extensively used in time series prediction due to their ability to model temporal dependencies and trends in sequential data In financial forecasting Fischer and Krauss 148 conducted a comprehensive study using deep RNNs to predict stock returns Their results indicated that RNNs could outperform traditional ML models such as support vector machines and random forests in financial forecasting tasks The study demonstrated that deep RNNs could learn intricate patterns in stock price movements contributing to better forecasting accuracy With the advancement of deep learning techniques Nelson et al 149 proposed a model combining CNNs and RNNs for stock price prediction The CNN component extracted local features from historical price data while the RNN component captured the temporal dependencies This hybrid model showed significant improvements in prediction performance suggesting that integrating different neural network architectures could enhance financial forecasting Also attention mechanisms have been integrated into RNNs to improve financial forecasting 19 of 34 Information 2024 15 517 Luo et al 150 used an attentionbased CNNBiLSTM model that focused on relevant time steps in the input sequence enhancing the models ability to capture important patterns in financial data This approach allowed for more accurate predictions of stock prices and market trends by dynamically weighting the significance of past observations Furthermore Bao et al 151 employed a novel deep learning framework combining LSTM with stacked autoencoders for financial time series forecasting Their model utilized stacked autoencoders to reduce the dimensionality of input data and LSTM to model temporal dependencies This method improved the models ability to predict future stock prices by effectively capturing both feature representations'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='capturing both feature representations and sequential patterns Feng et al 152 explored the use of transfer learning for financial forecasting They proposed a model that pretrained an RNN on a large corpus of financial data and fine tuned it on specific stock datasets This approach employed the knowledge gained from broader market data to improve predictions on individual stocks which demonstrates the potential of transfer learning in financial forecasting Meanwhile the application of reinforcement learning in conjunction with RNNs has gained attention in financial forecasting Rundo 153 combined RL with LSTM to develop a trading strategy that maximizes returns Their model learned optimal trading actions through interactions with the market environment resulting in a robust and adaptive financial forecasting system Beyond financial applications RNNs have shown effectiveness in other domains such as weather forecasting and renewable energy predictions where modeling temporal dependencies is critical for accurate forecasts In weather forecasting Devi et al 154 developed an RNNbased model specifically for weather prediction which demonstrated superior performance in both shortterm and longterm forecasting compared to traditional statistical methods This model effectively captures the sequential dependencies in meteo rological data such as temperature humidity and atmospheric pressure enabling more accurate and reliable forecasts Additionally Anshuka et al 155 showed the effective ness of using LSTM networks in predicting extreme weather events by learning complex temporal patterns in historical weather data Furthermore Lin et al 100 proposed a model that integrates the attention mechanism with LSTM which further improved the ability of RNNs especially as the attention mechanism ensures the model focuses on critical features within the large dataset thereby enhancing the accuracy of predictions in complex weather scenarios In the field of renewable energy RNNs have been extensively applied to forecast energy generation from renewable sources such as wind and solar power Marulanda et al 156 utilized an LSTMbased model for shortterm wind power forecasting which showed signifi cant improvements in prediction accuracy by capturing the nonlinear and timedependent characteristics of wind speed data Similarly Chen et al 157 developed an advanced DL approach combining a bidirectional GRU with temporal convolutional networks TCNs for energy time series forecasting This hybrid model was particularly effective in capturing both shortterm fluctuations and longterm trends leading to more reliable predictions Moreover RNNs have also been used to forecast energy demand in smart grids where their ability to model temporal dependencies helps in optimizing the integration of renewable energy sources into the grid and improving overall energy management 158159 Furthermore RNNs have been applied to predict consumer demand patterns for goods and services allowing'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='demand patterns for goods and services allowing businesses to optimize their supply chain management and reduce costs For instance Yildiz et al 160 proposed a hybrid RNN model that combines LSTM with CNN to accurately predict electricity demand in urban areas showing significant improvements over traditional forecasting techniques Meanwhile ESNs have also shown promise in weather forecasting and renewable energy predictions due to their ability to handle nonlinear and chaotic time series data For instance Anshuka et al 155 applied ESNs to model and predict extreme weather events demonstrating the networks ability to capture complex temporal patterns from historical weather data Similarly Marulanda et al 156 used an ESNbased approach for shortterm wind power forecasting which effectively captured the nonlinear dynamics of 20 of 34 Information 2024 15 517 Application Domain Speech recognition Time series forecasting wind speed and improved prediction accuracy compared to conventional methods These studies highlight the versatility and robustness of ESNs in handling diverse time series forecasting tasks across different domains Additionally Gao et al 90 proposed a dynamic ensemble deep ESN specifically designed for wave height forecasting This model adjusts reservoir weights dynamically allowing it to model the complex and nonlinear patterns often found in time series more effectively than traditional methods Additionally Bhambu et al 161 introduced a re current ensemble deep random vector functional link neural network for financial time series forecasting This model integrates the strengths of both ESNs and recurrent net works providing superior performance in predicting financial market volatility and trends Table 5 provides a summary of the RNN applications in both speech recognition and time series forecasting Among the various RNN architectures LSTM networks stand out as the most effective for time series forecasting especially in financial applications 162 LSTMs gating mechanisms allow it to maintain and utilize longterm dependencies which are crucial for accurately predicting future trends based on historical data The ability of LSTM to capture complex temporal patterns makes it particularly wellsuited for financial markets where longrange dependencies and intricate patterns in data are common Meanwhile when LSTM is combined with other techniques such as CNNs for feature extraction or attention mechanisms for focusing on critical time steps the models forecasting performance improves even further This combination of adaptability robustness and precision demonstrates why LSTM is frequently considered the best RNN variant for time seriesforecasting tasks Table 5 Summary of RNNs in speech recognition and time series forecasting Reference Year Methods and Application Hinton et al 137 2012 Deep neural networks including RNNs for speechtotext systems Hannun et al 138 2014 DeepSpeech LSTMbased speech recognition system Amodei et al 139 2016'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='speech recognition system Amodei et al 139 2016 DeepSpeech2 Enhanced LSTMbased speech recognition with bidirectional RNNs Zhang et al 141 Chiu et al 140 2017 Convolutional RNN for robust speech recognition 2018 RNNtransducer models for endtoend speech recognition Dong et al 142 Bhaskar and Thasleema 143 Daouad et al 144 Nasr et al 146 Kumar et al 147 Dhanjal et al 145 SpeechTransformer Leveraging selfattention for better pro cessing of audio sequences LSTM for visual speech recognition using facial expressions 2018 2023 2023 Various RNN variants for automatic speech recognition 2023 2023 Endtoend speech recognition using RNNs Performance evaluation of RNNs in speech recognition tasks Comprehensive study of different RNN models for speech recog nition 2024 Nelson et al 149 2017 Hybrid CNNRNN model for stock price prediction Bao et al 151 Fischer and Krauss 148 Feng et al 152 Rundo 153 Devi et al 154 Anshuka et al 155 2017 2018 2019 2019 2024 2022 Combining LSTM with stacked autoencoders for financial time series forecasting Deep RNNs for predicting stock returns outperforming tradi tional ML models Transfer learning with RNNs for stock prediction Combining reinforcement learning with LSTM for trading strat egy development RNNbased model for weather prediction and capturing se quential dependencies in meteorological data LSTM networks for predicting extreme weather events by learn ing complex temporal patterns 21 of 34 Information 2024 15 517 Application Domain Table 5 Cont Reference Year Methods and Application Lin et al 100 Marulanda et al 156 Chen et al 157 Hasanat et al 158 Asiri et al 159 Yildiz et al 160 2022 2023 2024 2024 2024 2024 Integrating attention mechanisms with LSTM for enhanced weather fore casting accuracy LSTM model for shortterm wind power forecasting and improving pre diction accuracy Bidirectional GRU with TCNs for energy time series forecasting RNNs for forecasting energy demand in smart grids and optimizing renewable energy integration Shortterm renewable energy predictions using RNNbased models Hybrid model of LSTM with CNN for accurate electricity demand predic tion Luo et al 150 Gao et al 90 2024 Attentionbased CNNBiLSTM model for improved financial forecasting 2023 Dynamic ensemble deep ESN for wave height forecasting Bhambu et al 161 2024 Recurrent ensemble deep random vector functional link neural network for financial time series forecasting 74 Signal Processing RNNs particularly ESNs have seen significant applications in various signalprocessing tasks due to their efficient training and robust performance in handling timedependent data One key area of application is physiological signal processing Mastoi et al 163 de veloped an ESNbased approach for the realtime monitoring and prediction of heart rate variability Their approach outperformed traditional methods in terms of accu racy and computational efficiency demonstrating ESNs potential in realtime health monitoring systems ESNs have also been extensively used in'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='systems ESNs have also been extensively used in speechprocessing tasks Valin et al 164 proposed an ESN architecture specifically designed for speech signal enhancement This model demonstrated improved noise reduction and speech intelligibility in noisy envi ronments which is critical for applications such as hearing aids and speech recognition systems The models ability to handle temporal dependencies in speech signals made it particularly effective in enhancing audio quality under challenging conditions Additionally ESNs have been applied in the preprocessing and analysis of non stationary and noisy time series data Gao et al 91 integrated the empirical wavelet transform EWT with ESNs to enhance performance in time series forecasting This hybrid approach demonstrated that combining the EWTs ability to decompose complex signals with the robust modeling capabilities of ESNs leads to superior performance particularly in scenarios where data is noisy or exhibits nonstationary behavior This integration demonstrates the adaptability of ESNs to a wide range of signal processing challenges reinforcing their utility in domains requiring accurate and efficient time series analysis 75 Bioinformatics In bioinformatics RNNs have been used to analyze biological sequences such as DNA RNA and proteins Li et al 165 employed RNNs for gene prediction and protein structure prediction demonstrating the ability of RNNs to capture dependencies within biological sequences and providing insights into genetic information and biological pro cesses Zhang et al 166 used bidirectional LSTM in predicting DNAbinding protein sequences Their model called DeepSite leveraged the sequential nature of biological data achieving higher accuracy in identifying binding sites compared to traditional methods This application demonstrated the potential of RNNs to enhance our understanding of proteinDNA interactions In the field of proteomics RNNs have been used for protein structure prediction and function annotation Xu et al 167 developed an RNNbased model to predict protein secondary structures showing that RNNs could effectively captures the sequential depen 22 of 34 Information 2024 15 517 dencies in amino acid sequences This application provided significant advancements in protein structure prediction which is essential for drug discovery and disease research More recently researchers have explored the integration of RNNs with other neural network architectures for bioinformatics applications For example Yadav et al 168 combined BiLSTM with CNNs to analyze protein sequences Their model extracted local features and captured longrange dependencies with BiLSTM resulting in improved per formance in protein classification tasks Additionally the use of ensemble deep learning has enhanced the performance of RNNs in bioinformatics Aybey et al 169 introduced an ensemble model for predicting proteinprotein interactions using RNNs GRUs and CNNs The model improves the accuracy of interaction'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='The model improves the accuracy of interaction predictions This approach highlighted the potential of ensemble deep learning to enhance the interpretability and performance of RNNs in bioinformatics In bioinformatics RNNs specifically LSTM networks and GRUs have established themselves as the best models for analyzing biological sequences due to their ability to process long sequences and maintain information over long distances crucial for under standing complex biological structures and functions Bidirectional LSTM used by Zhang et al 166 in predicting DNAbinding protein sequences is particularly effective as it processes sequences in both forward and backward directions providing a better context and significantly improving prediction accuracy over unidirectional approaches This capability makes it preferable for tasks where understanding the full context of a sequence is essential such as gene prediction protein folding and other complex bioinformatics applications involving sequential data 76 Autonomous Vehicles RNNs play an important role in autonomous vehicles by processing sequential data from sensors to make driving decisions Li et al 170 used RNNs for path planning object detection and trajectory prediction enabling autonomous vehicles to navigate complex environments and make realtime decisions Following this foundational work researchers have continued to explore and enhance the use of RNNs in autonomous driving For instance Lee et al 171 developed a deep learning framework that integrates LSTM with CNN for endtoend driving Their model utilized CNN to extract spatial features from camera images and LSTM to capture temporal dependencies which improved the accuracy and robustness of driving decisions in dynamic environments Codevilla et al 172 introduced a conditional imitation learning approach that com bined RNNs with imitation learning for autonomous driving The model learned from human driving demonstrations and used RNNs to predict future actions based on past observations This approach allowed the vehicle to adapt to various driving conditions and make safer decisions in complex scenarios Additionally researchers have explored the use of LSTM for trajectory prediction in autonomous vehicles Altch and de La Fortelle 173 proposed an LSTMbased model that predicts the future trajectories of surrounding vehi cles This model leverages the sequential nature of traffic data to anticipate the movements of other road users enabling more accurate and proactive path planning for autonomous vehicles Meanwhile attention mechanisms have been integrated into RNN models to enhance their performance in autonomous driving tasks Li et al 174 introduced an attentionbased LSTM model that focuses on relevant parts of the data improving the detection and tracking of video objects Researchers have also explored the use of RNNs for behavior prediction in autonomous driving Li et al 175 proposed a model that combines RNNs with CNN to predict the intentions'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='combines RNNs with CNN to predict the intentions of other drivers Their approach used sequential data to learn the behavioral patterns of surrounding vehicles enabling the autonomous vehicle to anticipate potential hazards and respond accordingly In addition researchers have investigated the use of RNNs for decisionmaking in autonomous vehicles Liu and Diao 176 introduced a deep reinforcement learning framework that incorporates GRU for decisionmaking in complex 23 of 34 Information 2024 15 517 traffic scenarios Their model used RNNs to process sequential observations and make realtime decisions achieving stateoftheart performance in various driving tasks Furthermore the integration of LSTM with CNNs seems to represent the best approach in autonomous vehicle applications as demonstrated by Lee et al 171 This combination leverages LSTMs ability to understand temporal dynamics and CNNs strength in spatial feature extraction making it robust for realtime applications like driving where both spatial and temporal understandings are crucial for decisionmaking The hybrid nature of these models allows for a better understanding and processing of the vast amounts of data from various sensors ensuring better performance in navigation and realtime decision making in dynamic environments 77 Anomaly Detection RNNs are used in anomaly detection across different fields such as cybersecurity industrial monitoring and healthcare Altindal et al 177 demonstrated the use of LSTM networks for anomaly detection in time series data showing that RNNs could effectively model normal behavior patterns and identify deviations indicative of anomalies Sim ilarly Matar et al 178 proposed a model for anomaly detection in multivariate time series Their model utilized BiLSTM to learn temporal dependencies and to detect devia tions from normal patterns This approach was effective in industrial applications where monitoring the health of machinery and predicting failures is critical In cybersecurity Kumaresan et al 179 applied RNNs to detect anomalies in network traffic Their model analyzed sequential data to identify unusual patterns that could indicate security breaches or malicious activities The use of RNNs allowed for realtime detection and response to potential threats enhancing the security of network systems Furthermore Li et al 180 explored the application of RNNs for anomaly detection in manufacturing processes They developed a model combining RNNs with transfer learning to capture both temporal dependencies and feature representations This method improved the detection of anomalies in complex industrial processes contributing to the optimization of production efficiency and quality control In healthcare researchers have utilized RNNs for detecting anomalies in physiological signals For instance Mini et al 181 employed RNNs to detect abnormal patterns in electrocardiogram ECG signals Their model accurately identified deviations indicative of cardiac arrhythmias'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='deviations indicative of cardiac arrhythmias demonstrating the potential of RNNs to assist in the early diagnosis and monitoring of heart conditions Moreover advances in unsupervised learning have further enhanced the capabilities of RNNs in anomaly detection Zhou and Paffenroth 182 introduced a robust deep au toencoder model that leverages RNNs for unsupervised anomaly detection This approach effectively captured the underlying structure of the data identifying anomalies without requiring labeled training data Ren et al 183 proposed an attentionbased RNN model that focuses on relevant time steps in the data improving the accuracy and interpretabil ity of anomaly detection This approach allowed for the more precise identification of anomalies by dynamically weighting the importance of different parts of the sequence Ad ditionally hybrid models combining RNNs with other neural network architectures have also been employed in anomaly detection Munir et al 184 developed a hybrid model that integrates CNNs and RNNs to detect anomalies in multivariate time series data The CNN component extracted local features while the RNN component captured temporal dependencies resulting in improved performance in various anomaly detection tasks The BiLSTM model stands out as the best RNN architecture in anomaly detection especially in multivariate time series data where understanding the influence of past and future input contexts is crucial Matar et al 178 demonstrated that BiLSTM effectively capture temporal dependencies in both directions which is critical in anomaly detection scenarios where anomalies may be contextually linked to events in both the past and the future This bidirectional processing capability allows for more robust detection of anomalies across various applications from industrial monitoring to cybersecurity making it the most suitable RNN model for these tasks 24 of 34 Information 2024 15 517 A summary of RNN applications in bioinformatics autonomous vehicles and anomaly detection is shown in Table 6 Table 6 Summary of RNNs in signal processing bioinformatics autonomous vehicles and anomaly detection Application Domain Reference Year Methods and Application Signal processing Mastoi et al 163 Valin et al 164 Gao et al 91 2019 2021 2021 ESNs for realtime heart rate variability monitoring ESNs for speech signal enhancement in noisy environments EWT integrated with ESNs for enhanced time series forecasting Bioinformatics Li et al 165 2019 RNNs for gene prediction and proteinstructure prediction Zhang et al 166 2020 Bidirectional LSTM for predicting DNAbinding protein sequences Xu et al 167 Yadav et al 168 Aybey et al 169 2021 RNNbased model for predicting protein secondary structures 2019 Combining BiLSTM with CNNs for protein sequence analysis 2023 Ensemble model for predicting proteinprotein interactions Autonomous vehicles Altch and de La Fortelle 173 Codevilla et al 172 Li et al 170 2017 LSTM for predicting the future trajectories of'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='LSTM for predicting the future trajectories of vehicles 2018 RNNs with imitation learning for autonomous driving 2020 RNNs for path planning and object detection Lee et al 171 2020 Integrating LSTM with CNN for endtoend autonomous driving Li et al 174 Liu and Diao 176 2024 Attentionbased LSTM for video object tracking 2024 GRU with deep reinforcement learning for decisionmaking Anomaly detection Zhou and Paffenroth 182 2017 RNNs autoencoders in unsupervised anomaly detection with deep Munir et al 184 Ren et al 183 2018 Hybrid CNNRNN model for anomaly detection in time series 2019 Attentionbased RNN model for anomaly detection Li et al 180 2023 RNNs with Transfer learning for anomaly detection in manufacturing Mini et al 181 Matar et al 178 Kumaresan et al 179 Altindal et al 177 2023 RNNs for detecting anomalies in ECG signals 2023 2024 RNNs for detecting network traffic anomalies 2024 BiLSTM for anomaly detection in multivariate time series LSTM for anomaly detection in time series data 8 Challenges and Future Research Directions Despite significant advancements several unresolved problems are encountered when applying RNNs Addressing these issues is crucial for further improving the performance and usage of RNNs 81 Scalability and Efficiency Training RNNs on large datasets with long sequences remains computationally inten sive and timeconsuming 185187 Although techniques like gradient checkpointing and hardware accelerators have provided improvements the sequential nature of RNNs contin ues to limit their scalability compared to parallelizable architectures like transformers 188 Future research could focus on developing more efficient training algorithms and exploring asynchronous and parallel training methods to distribute the computational load more effectively Additionally hybrid architectures that combine RNNs with other models such as integrating RNNs with attention mechanisms or convolutional layers could provide new solutions These hybrid models have the potential to reduce training times and improve scalability while maintaining the performance advantages of RNNs 104 82 Interpretability and Explainability RNNs are often perceived as blackbox models due to their complex internal dy namics making it challenging to interpret their decisions 189190 Although attention mechanisms and post hoc explanation techniques like Local Interpretable ModelAgnostic Explanations LIMEs and Shapley Addictive Explanations SHAPs have been proposed 25 of 34 Information 2024 15 517 to improve interpretability these methods can still be improved to further provide more comprehensive explanations 191 Therefore future research should aim to develop inher ently interpretable RNN architectures and hierarchical models that offer structured insights into the models decisionmaking process Additionally integrating domain knowledge into RNN models can help align their behavior with human reasoning enhancing both interpretability and performance in specialized'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='interpretability and performance in specialized applications 83 Bias and Fairness RNNs can inadvertently learn and propagate biases present in the training data lead ing to unfair predictions While various bias detection and mitigation techniques have been developed such as fairnessaware algorithms and adversarial training these methods need further refinement to ensure fairness across diverse applications and datasets 192194 Research should continue to focus on developing robust bias detection techniques and fair training algorithms that explicitly incorporate fairness constraints Additionally trans parency and accountability frameworks including external audits and impact assessments are essential for ensuring that RNNs are developed and deployed responsibly 84 Data Dependency and Quality RNNs require large amounts of highquality labeled sequential data for effective training 195 In many realworld scenarios such data may be scarce noisy or incomplete Although data augmentation transfer learning and semisupervised learning techniques have been explored these methods require further refinement to handle diverse data challenges more effectively Future research should focus on enhancing these techniques to improve the robustness of RNNs when trained on limited or imperfect data Additionally developing new methods for utilizing unlabeled data and integrating domainspecific knowledge can further improve the performance of RNNs in datascarce environments 85 Overfitting and Generalization RNNs particularly deep architectures are prone to overfitting especially when trained on small datasets 196 Ensuring that RNN models generalize well to unseen data without overfitting remains a significant challenge While regularization techniques like dropout and L2 regularization are commonly used more robust methods for improving general ization are needed Future research can explore advanced regularization techniques such as adversarial training and ensemble methods to enhance the generalization capabilities of RNNs Additionally applying data augmentation and transfer learning can help RNN models learn more robust features improving their ability to generalize to new data 9 Conclusions RNNs have demonstrated a remarkable ability to model sequential data making them indispensable in numerous ML applications such as natural language processing speech recognition time series prediction bioinformatics and autonomous systems This paper provided a comprehensive overview of RNNs and their variants covering fundamental architectures like basic RNNs LSTM networks and GRUs as well as advanced variants including bidirectional RNNs peephole LSTM ESNs and IndRNNs This study has provided a detailed and comprehensive review of RNNs as well as their architectures applications and challenges The paper will be a valuable resource for researchers and practitioners in the field of machine learning helping to guide future developments and applications of RNNs Author Contributions'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='and applications of RNNs Author Contributions Conceptualization IDM TGS and GO methodology IDM validation IDM TGS and GO investigation IDM TGS and GO resources TGS writingoriginal draft preparation IDM and GO writingreview and editing IDM TGS and GO visual ization IDM supervision TGS All authors have read and agreed to the published version of the manuscript 26 of 34 Information 2024 15 517 Funding This research received no external funding Institutional Review Board Statement Not applicable Informed Consent Statement Not applicable Data Availability Statement Not applicable Conflicts of Interest The authors declare no conflicts of interest Abbreviations The following abbreviations are used in this manuscript Artificial intelligence AI ANN Artificial neural network BiLSTM Bidirectional long shortterm memory CNN DL GRU LSTM ML NAS NLP RNN RL SHAPs TPU VAE Convolutional neural network Deep learning Gated recurrent unit Long shortterm memory Machine learning Neural architecture search Natural language processing Recurrent neural network Reinforcement learning Shapley Additive Explanations Tensor processing unit Variational autoencoder References 1 2 3 4 OHalloran T Obaido G Otegbade B Mienye ID A deep learning approach for Maize Lethal Necrosis and Maize Streak Virus disease detection Mach Learn Appl 2024 16 100556 CrossRef Peng Y He L Hu D Liu Y Yang L Shang S Decoupling Deep Learning for Enhanced Image Recognition Interpretability ACM Trans Multimed Comput Commun Appl 2024 CrossRef Khan W Daud A Khan K Muhammad S Haq R Exploring the frontiers of deep learning and natural language processing A comprehensive overview of key challenges and emerging trends Nat Lang Process J 2023 4 100026 CrossRef Obaido G Achilonu O Ogbuokiri B Amadi CS Habeebullahi L Ohalloran T Chukwu CW Mienye E Aliyu M Fasawe O et al An Improved Framework for Detecting Thyroid Disease Using FilterBased Feature Selection and Stacking Ensemble IEEE Access 2024 12 8909889112 CrossRef 5 Mienye ID Obaido G Aruleba K Dada OA Enhanced Prediction of Chronic Kidney Disease using Feature Selection and Boosted Classifiers In Proceedings of the International Conference on Intelligent Systems Design and Applications Virtual 1315 December 2021 pp 527537 AlJumaili AHA Muniyandi RC Hasan MK Paw JKS Singh MJ Big data analytics using cloud computing based frameworks for power management systems Status constraints and future recommendations Sensors 2023 23 2952 CrossRef Gill SS Wu H Patros P Ottaviani C Arora P Pujol VC Haunschild D Parlikad AK Cetinkaya O Lutfiyya H et al Modern computing Vision and challenges Telemat Inform Rep 2024 13 100116 CrossRef 6 7 8 Mienye ID Jere N A Survey of Decision Trees Concepts Algorithms and Applications IEEE Access 2024 12 8671686727 CrossRef Aruleba RT Adekiya TA Ayawei N Obaido G Aruleba K Mienye ID Aruleba I Ogbuokiri B COVID19 diagnosis A review of rapid antigen RTPCR and artificial intelligence methods Bioengineering 2022 9 153 CrossRef 8 Mienye ID Jere N A'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='2022 9 153 CrossRef 8 Mienye ID Jere N A Survey of Decision Trees Concepts Algorithms and Applications IEEE Access 2024 12 8671686727 CrossRef Aruleba RT Adekiya TA Ayawei N Obaido G Aruleba K Mienye ID Aruleba I Ogbuokiri B COVID19 diagnosis A review of rapid antigen RTPCR and artificial intelligence methods Bioengineering 2022 9 153 CrossRef 10 Alhajeri MS Ren YM Ou F Abdullah F Christofides PD Model predictive control of nonlinear processes using transfer learningbased recurrent neural networks Chem Eng Res Des 2024 205 112 CrossRef Shahinzadeh H Mahmoudi A Asilian A Sadrarhami H Hemmati M Saberi Y Deep Learning A Overview of Theory and Architectures In Proceedings of the 2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing AISP Babol Iran 2122 February 2024 pp 111 10 Alhajeri MS Ren YM Ou F Abdullah F Christofides PD Model predictive control of nonlinear processes using transfer learningbased recurrent neural networks Chem Eng Res Des 2024 205 112 CrossRef Shahinzadeh H Mahmoudi A Asilian A Sadrarhami H Hemmati M Saberi Y Deep Learning A Overview of Theory and Architectures In Proceedings of the 2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing AISP Babol Iran 2122 February 2024 pp 111 12 Baruah RD Organero MM Explicit Context Integrated Recurrent Neural Network for applications in smart environments Expert Syst Appl 2024 255 124752 CrossRef 13 Werbos P Backpropagation through time What it does and how to do it Proc IEEE 1990 78 15501560 CrossRef 14 Lalapura VS Amudha J Satheesh HS Recurrent neural networks for edge intelligence A survey ACM Comput Surv CSUR 2021 54 138 CrossRef 27 of 34 Information 2024 15 517 15 Hochreiter S Schmidhuber J Long shortterm memory Neural Comput 1997 9 17351780 CrossRef PubMed 16 Cho K Van Merrinboer B Gulcehre C Bahdanau D Bougares F Schwenk H Bengio Y Learning phrase representations using RNN encoderdecoder for statistical machine translation arXiv 2014 arXiv14061078 17 Liu F Li J Wang L PILSTM Physicsinformed long shortterm memory network for structural response modeling Eng Struct 2023 292 116500 CrossRef 18 Ni Q Ji J Feng K Zhang Y Lin D Zheng J Datadriven bearing health management using a novel multiscale fused feature and gated recurrent unit Reliab Eng Syst Saf 2024 242 109753 CrossRef 19 Niu Z Zhong G Yue G Wang LN Yu H Ling X Dong J Recurrent attention unit A new gated recurrent unit for longterm memory of important parts in sequential data Neurocomputing 2023 517 19 CrossRef 20 Lipton ZC Berkowitz J Elkan C A critical review of recurrent neural networks for sequence learning arXiv150600019 21 Yu Y Si X Hu C Zhang J A review of recurrent neural networks LSTM cells and network architectures Neural Comput 2019 31 12351270 CrossRef 22 Tarwani KM Edem S Survey on recurrent neural network in natural language processing Int J Eng Trends Technol 2017 48 301304 CrossRef 23 Tsoi AC Back AD Locally recurrent globally feedforward networks A'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='Locally recurrent globally feedforward networks A critical review of architectures IEEE Trans Neural Netw 1994 5 229239 CrossRef PubMed 24 Mastorocostas PA Theocharis JB A stable learning algorithm for blockdiagonal recurrent neural networks Application to the analysis of lung sounds IEEE Trans Syst Man Cybern Part B Cybern 2006 36 242254 CrossRef PubMed 25 Dutta KK Poornima S Sharma R Nair D Ploeger PG Applications of Recurrent Neural Network Overview and Case Studies In Recurrent Neural Networks CRC Press Boca Raton FL USA 2022 pp 2341 26 Quradaa FH Shahzad S Almoqbily RS A systematic literature review on the applications of recurrent neural networks in code clone research PLoS ONE 2024 19 e0296858 CrossRef 27 Goodfellow I Bengio Y Courville A Deep Learning MIT Press Cambridge MA USA 2016 28 Greff K Srivastava RK Koutnk J Steunebrink BR Schmidhuber J LSTM A search space odyssey IEEE Trans Neural Netw Learn Syst 2016 28 22222232 CrossRef PubMed 29 AlSelwi SM Hassan MF Abdulkadir SJ Muneer A Sumiea EH Alqushaibi A Ragab MG RNNLSTM From J King SaudUnivComput Inf Sci 2024 36 102068 applications to modeling techniques and beyondSystematic review CrossRef 30 Zaremba W Sutskever I Vinyals O Recurrent neural network regularization arXiv 2014 arXiv14092329 31 Bai S Kolter JZ Koltun V An empirical evaluation of generic convolutional and recurrent networks for sequence modeling arXiv 2018 arXiv180301271 32 Che Z Purushotham S Cho K Sontag D Liu Y Recurrent neural networks for multivariate time series with missing values Sci Rep 2018 8 6085 CrossRef 33 Chung J Gulcehre C Cho K Bengio Y Empirical evaluation of gated recurrent neural networks on sequence modeling arXiv 2014 arXiv14123555 34 Badawy M Ramadan N Hefny HA Healthcare predictive analytics using machine learning and deep learning techniques A survey J Electr Syst Inf Technol 2023 10 40 CrossRef Ismaeel AG Janardhanan K Sankar M Natarajan Y Mahmood SN Alani S Shather AH Traffic pattern classification in smart cities using deep recurrent neural network Sustainability 2023 15 14522 CrossRef 34 Badawy M Ramadan N Hefny HA Healthcare predictive analytics using machine learning and deep learning techniques A survey J Electr Syst Inf Technol 2023 10 40 CrossRef Ismaeel AG Janardhanan K Sankar M Natarajan Y Mahmood SN Alani S Shather AH Traffic pattern classification in smart cities using deep recurrent neural network Sustainability 2023 15 14522 CrossRef 36 Mers M Yang Z Hsieh YA Tsai Y Recurrent neural networks for pavement performance forecasting Review and model performance comparison Transp Res Rec 2023 2677 610624 CrossRef 37 Chen Y Cheng Q Cheng Y Yang H Yu H Applications of recurrent neural networks in environmental factor forecasting A review Neural Comput 2018 30 28552881 CrossRef PubMed 38 Linardos V Drakaki M Tzionas P Karnavas YL Machine learning in disaster management Recent developments in methods and applications Mach Learn Knowl Extr 2022 4 446473 CrossRef 39 Zhang J Liu H Chang Q Wang L'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='4 446473 CrossRef 39 Zhang J Liu H Chang Q Wang L Gao RX Recurrent neural network for motion trajectory prediction in humanrobot collaborative assembly CIRP Ann 2020 69 912 CrossRef 40 Tsantekidis A Passalis N Tefas A Recurrent Neural Networks In Deep Learning for Robot Perception and Cognition Elsevier Amsterdam The Netherlands 2022 pp 101115 41 Mienye ID Jere N Deep Learning for Credit Card Fraud Detection A Review of Algorithms Challenges and Solutions IEEE Access 2024 12 9689396910 CrossRef 42 Mienye ID Sun Y A machine learning method with hybrid feature selection for improved credit card fraud detection Appl Sci 2023 13 7254 CrossRef 43 Rezk NM Purnaprajna M Nordstrm T UlAbdin Z Recurrent neural networks An embedded computing perspective IEEE Access 2020 8 5796757996 CrossRef 44 Yu Y Adu K Tashi N Anokye P Wang X Ayidzoe MA Rmaf Relumemristorlike activation function for deep learning IEEE Access 2020 8 7272772741 CrossRef 28 of 34 Information 2024 15 517 45 Mienye ID Ainah PK Emmanuel ID Esenogho E Sparse Noise Minimization in Image Classification using Genetic Algorithm and DenseNet In Proceedings of the 2021 Conference on Information Communications Technology and Society ICTAS Durban South Africa 1011 March 2021 pp 103108 46 Ciaburro G Venkateswaran B Neural Networks with R SMART Models Using CNN RNN Deep Learning and Artificial Intelligence Principles Packt Publishing Ltd Birmingham UK 2017 47 Nwankpa C Ijomah W Gachagan A Marshall S Activation functions Comparison of trends in practice and research for deep learning arXiv 2018 arXiv181103378 Szandaa T Review and comparison of commonly used activation functions for deep neural networks BioInspired Neurocomp 2021 203224 47 Nwankpa C Ijomah W Gachagan A Marshall S Activation functions Comparison of trends in practice and research for deep learning arXiv 2018 arXiv181103378 Szandaa T Review and comparison of commonly used activation functions for deep neural networks BioInspired Neurocomp 2021 203224 49 Clevert DA Unterthiner T Hochreiter S Fast and accurate deep network learning by exponential linear units elus arXiv 2015 arXiv151107289 50 Dubey SR Singh SK Chaudhuri BB Activation functions in deep learning A comprehensive survey and benchmark Neurocomputing 2022 503 92108 CrossRef 51 Obaido G Mienye ID Egbelowo OF Emmanuel ID Ogunleye A Ogbuokiri B Mienye P Aruleba K Supervised machine learning in drug discovery and development Algorithms applications challenges and prospects Mach Learn Appl 2024 17 100576 CrossRef 52 Mienye ID Sun Y Effective Feature Selection for Improved Prediction of Heart Disease In Proceedings of the PanAfrican Artificial Intelligence and Smart Systems Conference Durban South Africa 46 December 2021 pp 94107 53 Martins A Astudillo R From Softmax to Sparsemax A Sparse Model of Attention and MultiLabel Classification Proceedings of the International Conference on Machine Learning New York NY USA 2022 June 2016 pp 16141623 54 Bianchi FM Maiorino E Kampffmeyer MC Rizzi'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='54 Bianchi FM Maiorino E Kampffmeyer MC Rizzi A Jenssen R Bianchi FM Maiorino E Kampffmeyer MC Rizzi A Jenssen R Properties and Training in Recurrent Neural Networks In Recurrent Neural Networks for ShortTerm Load Forecasting An Overview and Comparative Analysis Springer BerlinHeidelberg Germany 2017 pp 921 55 Mohajerin N Waslander SL State Initialization for Recurrent Neural Network Modeling of TimeSeries Data In Proceedings of the 2017 International Joint Conference on Neural Networks IJCNN Anchorage AK USA 1419 May 2017 pp 23302337 Forgione M Muni A Piga D Gallieri M On the adaptation of recurrent neural networks for system identification Automatica 2023 155 111092 CrossRef 56 57 Zhang J He T Sra S Jadbabaie A Why gradient clipping accelerates training A theoretical justification for adaptivity arXiv 2019 arXiv190511881 58 Qian J Wu Y Zhuang B Wang S Xiao J Understanding Gradient Clipping in Incremental Gradient Methods In Proceedings of the International Conference on Artificial Intelligence and Statistics Virtual 1315 April 2021 pp 15041512 Fei H Tan F Bidirectional grid long shortterm memory bigridlstm A method to address contextsensitivity and vanishing gradient Algorithms 2018 11 172 CrossRef 58 Qian J Wu Y Zhuang B Wang S Xiao J Understanding Gradient Clipping in Incremental Gradient Methods In Proceedings of the International Conference on Artificial Intelligence and Statistics Virtual 1315 April 2021 pp 15041512 Fei H Tan F Bidirectional grid long shortterm memory bigridlstm A method to address contextsensitivity and vanishing gradient Algorithms 2018 11 172 CrossRef 60 Dong X Chowdhury S Qian L Li X Guan Y Yang J Yu Q Deep learning for named entity recognition on Chinese electronic medical records Combining deep transfer learning with multitask bidirectional LSTM RNN PLoS ONE 2019 14 e0216046 CrossRef PubMed 61 Chorowski JK Bahdanau D Serdyuk D Cho K Bengio Y Attentionbased models for speech recognition Adv Neural Inf Process Syst 2015 28 62 Zhou M Duan N Liu S Shum HY Progress in neural NLP Modeling learning and reasoning Engineering 2020 6 275290 CrossRef 63 Naseem U Razzak I Khan SK Prasad M A comprehensive survey on word representation models From classical to stateoftheart word representation language models Trans Asian LowResour Lang Inf Process 2021 20 135 CrossRef 64 Adil M Wu JZ Chakrabortty RK Alahmadi A Ansari MF Ryan MJ Attentionbased STLBiLSTM network to forecast tourist arrival Processes 2021 9 1759 CrossRef 65 Min S Park S Kim S Choi HS Lee B Yoon S Pretraining of deep bidirectional protein sequence representations with structural information IEEE Access 2021 9 123912123926 CrossRef Jain A Zamir AR Savarese S Saxena A Structuralrnn Deep Learning on SpatioTemporal Graphs In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Las Vegas NV USA 2730 June 2016 pp 53085317 67 Pascanu R Gulcehre C Cho K Bengio Y How to construct deep recurrent neural networks arXiv 2013 arXiv13126026 68 66 Shi H Xu M'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='arXiv 2013 arXiv13126026 68 66 Shi H Xu M Li R Deep learning for household load forecastingA novel pooling deep RNN IEEE Trans Smart Grid 2017 9 52715280 CrossRef 69 Gal Y Ghahramani Z A theoretically grounded application of dropout in recurrent neural networks Adv Neural Inf Process Syst 2016 29 70 Moradi R Berangi R Minaei B A survey of regularization strategies for deep models Artif Intell Rev 2020 53 39473986 CrossRef Salehin I Kang DK A review on dropout regularization approaches for deep neural networks within the scholarly domain Electronics 2023 12 3106 CrossRef 70 Moradi R Berangi R Minaei B A survey of regularization strategies for deep models Artif Intell Rev 2020 53 39473986 CrossRef Salehin I Kang DK A review on dropout regularization approaches for deep neural networks within the scholarly domain Electronics 2023 12 3106 CrossRef 72 Cai S Shu Y Chen G Ooi BC Wang W Zhang M Effective and efficient dropout for deep convolutional neural networks arXiv 2019 arXiv190403392 29 of 34 Information 2024 15 517 73 Garbin C Zhu X Marques O Dropout vs batch normalization An empirical study of their impact to deep learning Multimed Tools Appl 2020 79 1277712815 CrossRef 74 Borawar L Kaur R ResNet Solving Vanishing Gradient in Deep Networks In Proceedings of the International Conference on Recent Trends in Computing ICRTC 2022 Delhi India 34 June 2022 Springer BerlinHeidelberg Germany 2023 pp 235247 IEEE Access 2023 75 Mienye ID Sun Y A deep learning ensemble with data resampling for credit card fraud detection 11 3062830638 CrossRef 76 Kiperwasser E Goldberg Y Simple and accurate dependency parsing using bidirectional LSTM feature representations Trans Assoc Comput Linguist 2016 4 313327 CrossRef 77 Zhang W Li H Tang L Gu X Wang L Wang L Displacement prediction of Jiuxianping landslide using gated recurrent unit GRU networks Acta Geotech 2022 17 13671382 CrossRef 78 Cahuantzi R Chen X Gttel S A Comparison of LSTM and GRU Networks for Learning Symbolic Sequences In Proceedings of the Science and Information Conference Nanchang China 24 June 2023 Springer BerlinHeidelberg Germany 2023 pp 771785 Shewalkar A Nyavanandi D Ludwig SA Performance evaluation of deep neural networks applied to speech recognition RNN LSTM and GRU J Artif Intell Soft Comput Res 2019 9 235245 CrossRef 79 80 Vatanchi SM Etemadfard H Maghrebi MF Shad R A comparative study on forecasting of longterm daily streamflow using ANN ANFIS BiLSTM and CNNGRULSTM Water Resour Manag 2023 37 47694785 CrossRef 81 Mateus BC Mendes M Farinha JT Assis R Cardoso AM Comparing LSTM and GRU models to predict the condition of a pulp paper press Energies 2021 14 6958 CrossRef 82 Gers FA Schmidhuber J Recurrent Nets That Time and Count In Proceedings of the IEEEINNSENNS International Joint Conference on Neural Networks IJCNN 2000 Neural Computing New Challenges and Perspectives for the New Millennium Como Italy 2427 July 2000 Volume 3 pp 189194 83 Gers FA Schraudolph NN Schmidhuber J Learning precise'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='FA Schraudolph NN Schmidhuber J Learning precise timing with LSTM recurrent networks J Mach Learn Res 2002 84 85 86 3 115143 Jaeger H Adaptive nonlinear system identification with echo state networks Adv Neural Inf Process Syst 2002 15 593600 Ishaq M Kwon S A CNNAssisted deep echo state network using multiple TimeScale dynamic learning reservoirs for generating ShortTerm solar energy forecasting Sustain Energy Technol Assessments 2022 52 102275 Sun C Song M Cai D Zhang B Hong S Li H A systematic review of echo state networks from design to application IEEE Trans Artif Intell 2022 5 2337 CrossRef 87 Gallicchio C Micheli A Deep echo state network deepesn A brief survey arXiv 2017 arXiv171204323 88 Gallicchio C Micheli A Richness of Deep Echo State Network Dynamics In Proceedings of the Advances in Computational Intelligence 15th International WorkConference on Artificial Neural Networks IWANN 2019 Gran Canaria Spain 1214 June 2019 Proceedings Part I 15 Springer BerlinHeidelberg Germany 2019 pp 480491 89 Hu R Tang ZR Song X Luo J Wu EQ Chang S Ensemble echo network with deep architecture for timeseries modeling Neural Comput Appl 2021 33 49975010 CrossRef 90 Gao R Li R Hu M Suganthan PN Yuen KF Dynamic ensemble deep echo state network for significant wave height forecasting Appl Energy 2023 329 120261 CrossRef 91 Gao R Du L Duru O Yuen KF Time series forecasting based on echo state network and empirical wavelet transformation Appl Soft Comput 2021 102 107111 CrossRef 92 Li S Li W Cook C Zhu C Gao Y Independently Recurrent Neural Network indrnn Building a Longer and Deeper rnn In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Salt Lake City UT USA 1823 June 2018 pp 54575466 93 Yang J Qu J Mi Q Li Q A CNNLSTM model for tailings dam risk prediction IEEE Access 2020 8 206491206502 CrossRef 94 Ren P Xiao Y Chang X Huang PY Li Z Chen X Wang X A comprehensive survey of neural architecture search Challenges and solutions ACM Comput Surv CSUR 2021 54 134 CrossRef 95 Mellor J Turner J Storkey A Crowley EJ Neural Architecture Search without Training In Proceedings of the International Conference on Machine Learning Virtual 1824 July 2021 pp 75887598 96 Zoph B Le QV Neural architecture search with reinforcement learning arXiv 2016 arXiv161101578 97 Chen X Wu SZ Hong M Understanding gradient clipping in private sgd A geometric perspective Adv Neural Inf Process Syst 2020 33 1377313782 98 Zhang Z Improved Adam Optimizer for Deep Neural Networks In Proceedings of the 2018 IEEEACM 26th International Symposium on Quality of Service IWQoS Banff AB Canada 46 June 2018 pp 12 99 De Santana Correia A Colombini EL Attention please A survey of neural attention models in deep learning Artif Intell Rev 2022 55 60376124 CrossRef 100 Lin J Ma J Zhu J Cui Y Shortterm load forecasting based on LSTM networks considering attention mechanism Int J Electr Power Energy Syst 2022 137 107818 CrossRef 101 Chaudhari S Mithal V Polatkan G Ramanath R An'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='101 Chaudhari S Mithal V Polatkan G Ramanath R An attentive survey of attention models ACM Trans Intell Syst Technol TIST 2021 12 132 CrossRef 102 Bahdanau D Cho K Bengio Y Neural machine translation by jointly learning to align and translate arXiv 2014 arXiv14090473 30 of 34 Information 2024 15 517 103 Luong MT Pham H Manning CD Effective approaches to attentionbased neural machine translation arXiv150804025 104 Vaswani A Shazeer N Parmar N Uszkoreit J Jones L Gomez AN Kaiser  Polosukhin I Attention is all you need Adv Neural Inf Process Syst 2017 30 105 Marcus MP Marcinkiewicz MA Santorini B Building a large annotated corpus of English The Penn Treebank Comput Linguist 1993 19 313330 106 Maas AL Daly RE Pham PT Huang D Ng AY Potts C Learning Word Vectors for Sentiment Analysis In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics Human Language Technologies Portland OR USA 1924 June 2011 pp 142150 107 LeCun Y Bottou L Bengio Y Haffner P Gradientbased learning applied to document recognition Proc IEEE 1998 86 22782324 CrossRef 108 Garofolo JS Lamel LF Fisher WM Fiscus JG Pallett DS TIMIT acousticphonetic continuous speech corpus Linguist Data Consort 1993 93 27403 109 Lewis D Reuters21578 Text Categorization Test Collection Distribution 10 ATT LabsResearch Atlanta GA USA 1997 110 Dua D Graff C UCI Machine Learning Repository School of Information and Computer Science University of California Irvine CA USA 2017 111 Lomonaco V Maltoni D Core50 A New Dataset and Benchmark for Continuous Object Recognition In Proceedings of the Conference on Robot Learning PMLR Mountain View CA USA 1315 November 2017 pp 1726 112 Souri A El Maazouzi Z Al Achhab M El Mohajir BE Arabic Text Generation using Recurrent Neural Networks In Proceedings of the Big Data Cloud and Applications Third International Conference BDCA 2018 Kenitra Morocco 45 April 2018 Revised Selected Papers 3 Springer BerlinHeidelberg Germany 2018 pp 523533 113 Islam MS Mousumi SSS Abujar S Hossain SA Sequencetosequence Bangla sentence generation with LSTM recurrent neural networks Procedia Comput Sci 2019 152 5158 CrossRef 114 Gajendran S Manjula D Sugumaran V Character level and word level embedding with bidirectional LSTMDynamic recurrent neural network for biomedical named entity recognition from literature J Biomed Inform 2020 112 103609 CrossRef 115 Hu H Liao M Mao W Liu W Zhang C Jing Y Variational AutoEncoder for Text Generation In Proceedings of the 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference ITOEC Chongqing China 1214 June 2020 pp 595598 116 Holtzman A Buys J Du L Forbes M Choi Y The curious case of neural text degeneration arXiv 2019 arXiv190409751 117 Yin W Schtze H Attentive convolution Equipping cnns with rnnstyle attention mechanisms Trans Assoc Comput Linguist 2018 6 687702 CrossRef 118 Hussein MAH Savas S LSTMBased Text Generation A Study on Historical Datasets arXiv 2024 arXiv240307087 119 Baskaran S'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='Datasets arXiv 2024 arXiv240307087 119 Baskaran S Alagarsamy S S S Shivam S Text Generation using Long ShortTerm Memory In Proceedings of the 2024 Third International Conference on Intelligent Techniques in Control Optimization and Signal Processing INCOS Krishnankoil India 1416 March 2024 pp 16 CrossRef 120 Keskar NS McCann B Varshney LR Xiong C Socher R Ctrl A conditional transformer language model for controllable generation arXiv 2019 arXiv190905858 121 Guo H Generating text with deep reinforcement learning arXiv 2015 arXiv151009202 122 Yadav V Verma P Katiyar V Long short term memory LSTM model for sentiment analysis in social data for ecommerce products reviews in Hindi languages Int J Inf Technol 2023 15 759772 CrossRef 123 Abimbola B de La Cal Marin E Tan Q Enhancing Legal Sentiment Analysis A Convolutional Neural NetworkLong ShortTerm Memory DocumentLevel Model Mach Learn Knowl Extr 2024 6 877897 CrossRef 124 Zulqarnain M Ghazali R Aamir M Hassim YMM An efficient twostate GRU based on feature attention mechanism for sentiment analysis Multimed Tools Appl 2024 83 30853110 CrossRef 125 Pujari P Padalia A Shah T Devadkar K Hybrid CNN and RNN for Twitter Sentiment Analysis International Conference on Smart Computing and Communication Springer BerlinHeidelberg Germany 2024 pp 297310 126 Wankhade M Annavarapu CSR Abraham A CBMAFM CNNBiLSTM multiattention fusion mechanism for sentiment classification Multimed Tools Appl 2024 83 5175551786 CrossRef 127 Sangeetha J Kumaran U A hybrid optimization algorithm using BiLSTM structure for sentiment analysis Meas Sensors 2023 25 100619 CrossRef 128 He R McAuley J Ups and Downs Modeling the Visual Evolution of Fashion Trends with OneClass Collaborative Filtering In Proceedings of the 25th International Conference on World Wide Web Montreal QC Canada 1115 April 2016 pp 507517 129 Samir A Elkaffas SM Madbouly MM Twitter Sentiment Analysis using BERT In Proceedings of the 2021 31st International Conference on Computer Theory and Applications ICCTA Kochi Kerala India 1719 August 2021 pp 182186 130 Prottasha NJ Sami AA Kowsher M Murad SA Bairagi AK Masud M Baz M Transfer learning for sentiment analysis using BERT based supervised finetuning Sensors 2022 22 4157 CrossRef 131 Mujahid M Rustam F Shafique R Chunduri V Villar MG Ballester JB Diez IdlT Ashraf I Analyzing sentiments regarding ChatGPT using novel BERT A machine learning approach Information 2023 14 474 CrossRef 31 of 34 Information 2024 15 517 132 Wu Y Schuster M Chen Z Le QV Norouzi M Macherey W Krikun M Cao Y Gao Q Macherey K et al Googles neural machine translation system Bridging the gap between human and machine translation arXiv 2016 arXiv160908144 133 Sennrich R Haddow B Birch A Neural machine translation of rare words with subword units arXiv 2015 arXiv150807909 134 Kang L He S Wang M Long F Su J Bilingual attention based neural machine translation Appl Intell 2023 53 43024315 CrossRef 135 Yang Z Dai Z Salakhutdinov R Cohen WW Breaking the'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='Z Dai Z Salakhutdinov R Cohen WW Breaking the softmax bottleneck A highrank RNN language model arXiv 2017 arXiv171103953 136 Song K Tan X Qin T Lu J Liu TY Mass Masked sequence to sequence pretraining for language generation arXiv 2019 arXiv190502450 137 Hinton G Deng L Yu D Dahl GE Mohamed Ar Jaitly N Senior A Vanhoucke V Nguyen P Sainath TN et al Deep neural networks for acoustic modeling in speech recognition The shared views of four research groups IEEE Signal Process Mag 2012 29 8297 CrossRef 138 Hannun A Case C Casper J Catanzaro B Diamos G Elsen E Prenger R Satheesh S Sengupta S Coates A et al Deep speech Scaling up endtoend speech recognition arXiv 2014 arXiv14125567 139 Amodei D Ananthanarayanan S Anubhai R Bai J Battenberg E Case C Casper J Catanzaro B Cheng Q Chen G et al Deep Speech 2 EndtoEnd Speech Recognition in English and Mandarin In Proceedings of the International Conference on Machine Learning New York NY USA 2022 June 2016 pp 173182 140 Chiu CC Sainath TN Wu Y Prabhavalkar R Nguyen P Chen Z Kannan A Weiss RJ Rao K Gonina E et al State oftheArt Speech Recognition with SequencetoSequence Models In Proceedings of the 2018 IEEE International Conference on Acoustics Speech and Signal Processing ICASSP Calgary Canada 1520 April 2018 pp 47744778 141 Zhang Y Chan W Jaitly N Very Deep Convolutional Networks for EndtoEnd Speech Recognition In Proceedings of the 2017 IEEE International Conference on Acoustics Speech and Signal Processing ICASSP New Orleans LA USA 59 March 2017 pp 48454849 142 Dong L Xu S Xu B SpeechTransformer A NoRecurrence SequencetoSequence Model for Speech Recognition In Proceedings of the 2018 IEEE International Conference on Acoustics Speech and Signal Processing ICASSP Calgary AB Canada 1520 April 2018 pp 58845888 143 Bhaskar S Thasleema T LSTM model for visual speech recognition through facial expressions Multimed Tools Appl 2023 82 54555472 CrossRef 144 Daouad M Allah FA Dadi EW An automatic speech recognition system for isolated Amazigh word using 1D  2D CNNLSTM architecture Int J Speech Technol 2023 26 775787 145 Dhanjal AS Singh W A comprehensive survey on automatic speech recognition using neural networks Multimed Tools Appl 2024 83 2336723412 CrossRef 146 Nasr S Duwairi R Quwaider M Endtoend speech recognition for arabic dialects Arab J Sci Eng 2023 48 1061710633 CrossRef 147 Kumar D Aziz S Performance Evaluation of Recurrent Neural NetworksLSTM and GRU for Automatic Speech Recognition In Proceedings of the 2023 International Conference on Computer Electronics  Electrical Engineering  Their Applications IC2E3 Srinagar Garhwal India 89 June 2023 pp 16 148 Fischer T Krauss C Deep learning with long shortterm memory networks for financial market predictions Eur J Oper Res 2018 270 654669 CrossRef 149 Nelson DM Pereira AC De Oliveira RA Stock Markets Price Movement Prediction with LSTM Neural Networks In Proceedings of the 2017 International Joint Conference on Neural Networks IJCNN Anchorage AK USA 1419 May'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='Neural Networks IJCNN Anchorage AK USA 1419 May 2017 pp 14191426 150 Luo A Zhong L Wang J Wang Y Li S Tai W Shortterm stock correlation forecasting based on CNNBiLSTM enhanced by attention mechanism IEEE Access 2024 12 2961729632 CrossRef 151 Bao W Yue J Rao Y A deep learning framework for financial time series using stacked autoencoders and longshort term memory PLoS ONE 2017 12 e0180944 CrossRef PubMed 152 Feng F Chen H He X Ding J Sun M Chua TS Enhancing Stock Movement Prediction with Adversarial Training In Proceedings of the TwentyEighth International Joint Conference on Artificial Intelligence IJCAI19 Macao China 1016 August 2019 Volume 19 pp 58435849 153 Rundo F Deep LSTM with reinforcement learning layer for financial trend prediction in FX high frequency trading systems Appl Sci 2019 9 4460 CrossRef 154 Devi T Deepa N Gayathri N Rakesh Kumar S AIBased Weather Forecasting System for Smart Agriculture System Using a Recurrent Neural Networks RNN Algorithm Sustain Manag Electron Waste 2024 97112 155 Anshuka A Chandra R Buzacott AJ Sanderson D van Ogtrop FF Spatio temporal hydrological extreme forecasting framework using LSTM deep learning model Stoch Environ Res Risk Assess 2022 36 34673485 CrossRef 156 Marulanda G Cifuentes J Bello A Reneses J A hybrid model based on LSTM neural networks with attention mechanism for shortterm wind power forecasting Wind Eng 2023 0309524X231191163 CrossRef 157 Chen W An N Jiang M Jia L An improved deep temporal convolutional network for new energy stock index prediction Inf Sci 2024 682 121244 CrossRef 32 of 34 Information 2024 15 517 158 Hasanat SM Younis R Alahmari S Ejaz MT Haris M Yousaf H Watara S Ullah K Ullah Z Enhancing Load Forecasting Accuracy in Smart Grids A Novel Parallel Multichannel Network Approach Using 1D CNN and BiLSTM Models Int J Energy Res 2024 2024 2403847 CrossRef 159 Asiri MM Aldehim G Alotaibi F Alnfiai MM Assiri M Mahmud A Shortterm load forecasting in smart grids using hybrid deep learning IEEE Access 2024 12 2350423513 CrossRef 160 Yldz Do gan G Aksoy A ztrk N A Hybrid Deep Learning Model to Estimate the Future Electricity Demand of Sustainable Cities Sustainability 2024 16 6503 CrossRef 161 Bhambu A Gao R Suganthan PN Recurrent ensemble random vector functional link neural network for financial time series forecasting Appl Soft Comput 2024 161 111759 CrossRef 162 Mienye E Jere N Obaido G Mienye ID Aruleba K Deep Learning in Finance A Survey of Applications and Techniques Preprints 2024 CrossRef 163 Mastoi QUA Wah TY Gopal Raj R Reservoir computing based echo state networks for ventricular heart beat classification Appl Sci 2019 9 702 CrossRef 164 Valin JM Tenneti S Helwani K Isik U Krishnaswamy A LowComplexity RealTime Joint Neural Echo Control and Speech Enhancement Based on Percepnet In Proceedings of the ICASSP 20212021 IEEE International Conference on Acoustics Speech and Signal Processing ICASSP Toronto ON Canada 611 June 2021 pp 71337137 165 Li Y Huang C Ding L Li Z Pan'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='2021 pp 71337137 165 Li Y Huang C Ding L Li Z Pan Y Gao X Deep learning in bioinformatics Introduction application and perspective in the big data era Methods 2019 166 421 CrossRef 166 Zhang Y Qiao S Ji S Li Y DeepSite Bidirectional LSTM and CNN models for predicting DNAprotein binding Int J Mach Learn Cybern 2020 11 841851 CrossRef 167 Xu J Mcpartlon M Li J Improved protein structure prediction by deep learning irrespective of coevolution information Nat Mach Intell 2021 3 601609 CrossRef 168 Yadav S Ekbal A Saha S Kumar A Bhattacharyya P Feature assisted stacked attentive shortest dependency path based BiLSTM model for proteinprotein interaction KnowlBased Syst 2019 166 1829 CrossRef 169 Aybey E Gms  SENSDeep An ensemble deep learning method for proteinprotein interaction sites prediction Interdiscip Sci Comput Life Sci 2023 15 5587 CrossRef PubMed 170 Li Z Du X Cao Y DATRNN Trajectory Prediction with Diverse Attention In Proceedings of the 2020 19th IEEE International Conference on Machine Learning and Applications ICMLA Miami FL USA 1417 December 2020 pp 15121518 171 Lee Mj Ha Yg Autonomous Driving Control Using EndtoEnd Deep Learning In Proceedings of the 2020 IEEE International Conference on Big Data and Smart Computing BigComp Busan Republic of Korea 1922 February 2020 pp 470473 CrossRef 172 Codevilla F Mller M Lpez A Koltun V Dosovitskiy A EndtoEnd Driving via Conditional Imitation Learning In Proceedings of the 2018 IEEE International Conference on Robotics and Automation ICRA Brisbane Australia 2125 May 2018 pp 46934700 173 Altch F de La Fortelle A An LSTM Network for Highway Trajectory Prediction In Proceedings of the 2017 IEEE 20th International Conference on Intelligent Transportation Systems ITSC Abu Dhabi United Arab Emirates 2528 October 2017 pp 353359 174 Li P Zhang Y Yuan L Xiao H Lin B Xu X Efficient longshort temporal attention network for unsupervised video object segmentation Pattern Recognit 2024 146 110078 CrossRef 175 Li R Shu X Li C Driving Behavior Prediction Based on Combined Neural Network Model IEEE Trans Comput Soc Syst 2024 11 44884496 CrossRef 176 Liu Y Diao S An automatic driving trajectory planning approach in complex traffic scenarios based on integrated driver style inference and deep reinforcement learning PLoS ONE 2024 19 e0297192 CrossRef 177 Altindal MC Nivlet P Tabib M Rasheed A Kristiansen TG Khosravanian R Anomaly detection in multivariate time series of drilling data Geoenergy Sci Eng 2024 237 212778 CrossRef 178 Matar M Xia T Huguenard K Huston D Wshah S MultiHead Attention Based bilstm for Anomaly Detection in Multivariate TimeSeries of wsn In Proceedings of the 2023 IEEE 5th International Conference on Artificial Intelligence Circuits and Systems AICAS Hangzhou China 1113 June 2023 pp 15 179 Kumaresan SJ Senthilkumar C Kongkham D Beenarani B Nirmala P Investigating the Effectiveness of Recurrent Neural Networks for Network Anomaly Detection In Proceedings of the 2024 International Conference on'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='of the 2024 International Conference on Intelligent and Innovative Technologies in Computing Electrical and Electronics IITCEE Bangalore India 2425 January 2024 pp 15 180 Li E Bedi S Melek W Anomaly detection in threeaxis CNC machines using LSTM networks and transfer learning Int J Adv Manuf Technol 2023 127 51855198 CrossRef 181 Minic A Jovanovic L Bacanin N Stoean C Zivkovic M Spalevic P Petrovic A Dobrojevic M Stoean R Applying recurrent neural networks for anomaly detection in electrocardiogram sensor data Sensors 2023 23 9878 CrossRef 182 Zhou C Paffenroth RC Anomaly Detection with Robust Deep Autoencoders In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining Halifax NS Canada 1317 August 2017 pp 665674 33 of 34 Information 2024 15 517 183 Ren H Xu B Wang Y Yi C Huang C Kou X Xing T Yang M Tong J Zhang Q TimeSeries Anomaly Detection Service at Microsoft In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery  Data Mining Anchorage AK USA 48 August 2019 pp 30093017 184 Munir M Siddiqui SA Dengel A Ahmed S DeepAnT A deep learning approach for unsupervised anomaly detection in time series IEEE Access 2018 7 19912005 CrossRef 185 Hewamalage H Bergmeir C Bandara K Recurrent neural networks for time series forecasting Current status and future directions Int J Forecast 2021 37 388427 CrossRef 186 Ahmed SF Alam MSB Hassan M Rozbu MR Ishtiak T Rafa N Mofijur M Shawkat Ali A Gandomi AH Deep learning modelling techniques Current progress applications advantages and challenges Artif Intell Rev 2023 56 1352113617 CrossRef 187 Li X Qin T Yang J Liu TY LightRNN Memory and computationefficient recurrent neural networks Adv Neural Inf Process Syst 2016 29 188 Katharopoulos A Vyas A Pappas N Fleuret F Transformers Are rnns Fast Autoregressive Transformers with Linear Attention In Proceedings of the International Conference on Machine Learning Virtual 1218 July 2020 pp 51565165 189 Shao W Li B Yu W Xu J Wang H When Is It Likely to Fail Performance Monitor for BlackBox Trajectory Prediction Model IEEE Trans Autom Sci Eng 2024 4 765772 CrossRef 190 Jacobs WR Kadirkamanathan V Anderson SR Interpretable deep learning for nonlinear system identification using frequency response functions with ensemble uncertainty quantification IEEE Access 2024 12 1105211065 CrossRef 191 Mamalakis M Mamalakis A Agartz I MrchJohnsen LE Murray G Suckling J Lio P Solving the enigma Deriving optimal explanations of deep networks arXiv 2024 arXiv240510008 192 Shah M Sureja N A Comprehensive Review of Bias in Deep Learning Models Methods Impacts and Future Directions Arch Comput Methods Eng 2024 113 CrossRef 193 Goethals S Calders T Martens D Beyond AccuracyFairness Stop evaluating bias mitigation methods solely on betweengroup metrics arXiv 2024 arXiv240113391 194 Weerts H Pfisterer F Feurer M Eggensperger K Bergman E Awad N Vanschoren J Pechenizkiy M Bischl B Hutter F Can fairness be automated Guidelines'),\n",
       " Document(metadata={'source': 'Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf'}, page_content='B Hutter F Can fairness be automated Guidelines and opportunities for fairnessaware AutoML J Artif Intell Res 2024 79 639677 CrossRef 195 Bai Y Geng X Mangalam K Bar A Yuille AL Darrell T Malik J Efros AA Sequential Modeling Enables Scalable Learning for Large Vision Models In Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition Seattle WA USA 1721 June 2024 pp 2286122872 196 Taye MM Understanding of machine learning with deep learning Architectures workflow applications and future directions Computers 2023 12 91 CrossRef DisclaimerPublishers Note The statements opinions and data contained in all publications are solely those of the individual authors and contributors and not of MDPI andor the editors MDPI andor the editors disclaim responsibility for any injury to people or property resulting from any ideas methods instructions or products referred to in the content 34 of 34'),\n",
       " Document(metadata={'source': 'GRU.pdf'}, page_content='GateVariants of Gated Recurrent Unit GRU Neural Networks Rahul Dey and Fathi M Salem Circuits Systems and Neural Networks CSANN LAB Department of Electrical and Computer Engineering Michigan State University East Lansing MI 488241226 USA deyrahulmsuedu  salemfmsuedu Abstract  The paper evaluates three variants of the Gated Recurrent Unit GRU in recurrent neural networks RNN by reducing parameters in the update and reset gates We evaluate the three variant GRU models on MNIST and IMDB datasets and show that these GRURNN variant models perform as well as the the original GRU RNN model while reducing computational expense performance sentiment classification from a given review paragraph II BACKGROUND RNN LSTM AND GRU In principal RNN are more suitable relationships among sequential data types The socalled simple RNN has a recurrent hidden state as in for capturing I INTRODUCTION Gated Recurrent Neural Network RNN have shown success in several applications involving sequential or temporal data 113 For example they have been applied extensively in speech recognition natural language processing machine translation etc 2 5 Long ShortTerm Memory LSTM RNN and the recently introduced Gated Recurrent Unit GRU RNN have been successfully shown to perform well with long sequence applications 25 812 Their success is primarily due to the gating network signals that control how the present input and previous memory are used to update the current activation and produce the current state These gates have their own sets of weights that are adaptively updated in the learning phase ie the training and evaluation process While these models empower successful in learning parameterization through their gate networks Consequently there is an added computational expense visvis the simple RNN model 2 5 6 It is noted that the LSTM RNN employs 3 distinct gate networks while the GRU RNN reduce the gate networks to two In 14 it is proposed to reduce the external gates to the minimum of one with preliminary evaluation of sustained performance in RNN they introduce an increase In this paper we focus on the GRU RNN and explore three new gatevariants with reduced parameterization We comparatively evaluate the performance of the original and the variant GRU RNN on two public datasets Using the MNIST dataset one generates two sequences 2 5 6 14 One sequence is obtained from each 28x28 image sample as pixel wise long sequence of length 28x28784 basically scanning from the upper left to the bottom right of the image Also one generates a rowwise short sequence of length 28 with each element being a vector of dimension 28 14 15 The third sequence type employs the IMDB movie review dataset where one defines the length of the sequence in order to achieve high cid2  cid4cid5cid6cid7cid2  cid9cid21  cid12cid13 cid51cid13 where cid7cid15 is the external mdimensional input vector at time cid2  cid15 the ndimensional hidden state g is the pointwise activation function such as the logistic'),\n",
       " Document(metadata={'source': 'GRU.pdf'}, page_content='activation function such as the logistic function the hyperbolic tangent function or the rectified Linear Unit ReLU 2 6 and cid6 cid9 cid17cid18cid19 cid12 are the appropriately sized parameters two weights and bias Specifically in this case cid6 is an cid18  cid21 matrix cid9 is an cid18  cid18 matrix and cid12 is an cid18  1 matrix or vector Bengio at al 1 showed that it is difficult to capture longterm dependencies using such simple RNN because the stochastic gradients tend to either vanish or explode with long sequences Two particular models the Long ShortTerm Memory LSTM unit RNN 3 4 and Gated Recurrent Unit GRU RNN 2 have been proposed to solve the vanishing or exploding gradient problems We will present these two models in sufficient details for our purposes below A Long ShortTerm Memory LSTM RNN The LSTM RNN architecture uses the computation of the simple RNN of Eqn 1 as an intermediate candidate for the internal memory cell state say cid22cid15 and add it in a element wise weightedsum to the previous value of the internal memory state say cid22cid15cid24cid25 to produce the current value of the memory cell state cid22cid15 This is expressed succinctly in the following discrete dynamic equations cid22cid15  cid27cid15cid22cid15cid24cid25  cid29cid15  cid22cid15 cid52cid13 cid22cid15  cid4cid5cid6cid31cid7cid15  cid9cid31cid15cid24cid25  cid12cid31cid13 cid53cid13 cid15  cid15gcid5cid22cid15cid13 cid54cid13 In Eqns 3 and 4 the activation nonlinearity cid4 is typically the hyperbolic tangent function but more recently may be implemented as a rectified Linear Unit reLU The weighted sum is implemented in Eqn 2 via elementwise Hadamard multiplication denoted by  to gating signals The gating control signals cid29cid15 cid27cid15 cid17cid18cid19 cid15 denote respectively the input forget and output gating signals at time cid2  These control gating signals are in fact replica of the basic equation 3 with their own parameters and replacing cid4 by the logistic function The logistic function limits the gating signals to within 0 and 1 The specific mathematical form of the gating signals are thus expressed as the vector equations cid29cid15  cid5cid6cid7cid15  cid9cid15cid24cid25  cid12cid13 cid27cid15  cid6cid7cid15  cid9cid15cid24cid25  cid12 cid15  cid5cid6cid7cid15  cid9cid15cid24cid25  cid12cid13 where  is the logistic nonlinearity and the parameters for each gate consist of two matrices and a bias vector Thus the total number of parameters represented as matrices and bias vectors for the 3 gates and the memory cell structure are respectively cid6 cid9 cid12 cid6 cid9 cid12 cid6 cid9 cid12 cid6cid31 cid9cid31 cid17cid18cid19 cid12cid31 These parameters are all updated at each training step and stored It is immediately noted that the number of parameters in the LSTM model is increased 4folds from the simple RNN model in Eqn 1 Assume that the cell state is ndimensional Note that the activation and all the gates have the same dimensions Assume'),\n",
       " Document(metadata={'source': 'GRU.pdf'}, page_content='and all the gates have the same dimensions Assume also that the input signal is m dimensional Then the total parameters in the LSTM RNN are equal to 4n2  nm n B Gated Recurrent Unit GRU RNN The GRU RNN reduce the gating signals to two from the LSTM RNN model The two gates are called an update gate cid15 and a reset gate cid15 The GRU RNN model is presented in the form cid15  cid51  cid15cid13cid15cid24cid25  cid15cid15 cid55cid13 cid15  cid4cid5cid6cid7cid15  cid9cid5cid15cid15cid24cid25cid13  cid12cid13 cid56cid13 with the two gates presented as cid15  cid5cid60cid7cid15  cid90cid15cid24cid25  cid120cid13 cid57cid13 cid15  cid5cid62cid7cid15  cid92cid15cid24cid25  cid122cid13 cid58cid13 One observes that the GRU RNN Eqns 56 is similar to the LSTM RNN Eqns 23 however with less external gating signal in the interpolation Eqn 5 This saves one gating signal and the associated parameters We defer further information to reference 2 and the references therein In essence the GRU RNN has 3folds increase in parameters in comparison to the simple RNN of Eqn 1 Specifically the total number of parameters in the GRU RNN equals 3n2  nm n In various studies eg in 2 and the references therein it has been noted that GRU RNN is comparable to or even outperforms the LSTM in most cases Moreover there are other reduced gated RNNs eg the Minimal Gated Unit MGU RNN where only one gate equation is used and it is reported that this MGU RNN performance is comparable to the LSTM RNN and the GRU RNN see 14 for details In this paper we focus on the GRU RNN model and evaluate new variants Specifically we retain the architecture of Eqns 56 unchanged and focus on variation in the structure of the gating signals in Eqns 7 and 8 We apply the variations identically to the two gates for uniformity and simplicity III THE VARIANT GRU ARCHITECTURES The gating mechanism in the GRU and LSTM RNN is a replica of the simple RNN in terms of parametrization The weights corresponding to these gates are also updated using the backpropagation through time BTT stochastic gradient descent as it seeks to minimize a losscost function 3 4 Thus each parameter update will information pertaining to the state of the overall network Thus all information regarding the current input and the previous hidden states are reflected in the latest state variable There is a redundancy in the signals driving the gating signals The key driving signal should be the internal state of the network Moreover involve components of the internal state of the system 16 17 In this study we consider three distinct variants of the gating equations applied uniformly to both gates involve the adaptive parameter updates all Variant 1 called GRU1 where each gate is computed using only the previous hidden state and the bias cid15  cid5cid90cid15cid24cid25  cid120cid13 cid59  cid17cid13 cid15  cid5cid92cid15cid24cid25  cid122cid13 cid59  cid12cid13 Thus the total number of parameters is now reduced in comparison to the GRU RNN by 2'),\n",
       " Document(metadata={'source': 'GRU.pdf'}, page_content='is now reduced in comparison to the GRU RNN by 2 nm Variant 2 called GRU2 where each gate is computed using only the previous hidden state cid15  cid5cid90cid15cid24cid25cid13 cid510  cid17cid13 cid15  cid5cid92cid15cid24cid25cid13 cid510  cid12cid13 Thus the total number of parameters is reduced in comparison to the GRU RNN by 2 nmn Variant 3 called GRU3 where each gate is computed using only the bias cid15  cid5cid120cid13 cid511  cid17cid13 cid15  cid5cid122cid13 cid511  cid12cid13 Thus the total number of parameters is reduced in comparison to the GRU RNN by 2 nmn2 We have performed an empirical study of the performance of each of these variants as compared to the GRU RNN on first sequences generated from the MNIST dataset and then on the IMDB movie review dataset In the subsequent figures and tables we refer to the base GRU RNN model as GRU0 and the three variants as GRU1 GRU2 and GRU3 respectively Our architecture consists of a single layer of one of the variants of GRU units driven by the input sequence and the activation function cid4 set as ReLU Initial experiments using cid4  cid2cid17cid18 have produced similar results For the MNIST dataset we generate the rowwise sequences as in 15 The networks have been generated in Python using the Keras library 15 with Theano as a backend library As Keras has a GRU layer class we modified this class to classes for GRU1 GRU2 and GRU3 All of these classes used the ReLU activation function The RNN layer of units is followed by a softmax layer in the case of the MNIST dataset or a traditional logistic activation layer in the case of the IMDB dataset to predict the output category The Root Mean Square Propagation RMSprop is used as the choice of optimizer that is known to adapt the learning rate for each of the parameters To speed up training we also decay the learning rate exponentially with the cost in each epoch the pixelwise and 6  678cid319cid15 cid512cid13 where 67 represents a base constant learning rate and cid22cid2 is the cost computed in the previous epoch The details of our models are delineated in Table I Table I Network model characteristics Model Hidden Units Gate Activation Activation Cost Epochs Optimizer Dropout Batch Size MNIST Pixelwise 100 Sigmoid ReLU Categorical Crossentropy 100 RMSProp 20 32 MNIST Rowwise 100 Sigmoid ReLU Categorical Crossentropy 50 RMSProp 20 32 IMDB 128 sigmoid ReLU Binary Cross entropy 100 RMSProp 20 32 IV RESULTS AND DISCUSSION A Application to MNIST Dataset  pixelwise sequences The MNIST dataset 15 consists of 60000 training images and 10000 test images each of size 28x28 of handwritten digits We evaluated our three variants against the original GRU model on the MNIST dataset by generating the sequential input in one case pixelwise one pixel at a time and in the second case rowwise one row at a time The pixelwise sequence generated from each image are 1element signal of length 784 while the 28element rowwise produces a sequence of length 28 For each case we'),\n",
       " Document(metadata={'source': 'GRU.pdf'}, page_content='produces a sequence of length 28 For each case we performed different iterations by varying the constant base learning rate 67 The results of our experiments are depicted in Fig 1 2 and 3 with summary in Table II below Table II MNIST pixelwise sequences performance summary of different architectures using 4 constant base learning rates 67 in 100 epochs Architecture GRU0 GRU1 GRU2 GRU3 Lr  1e3 Train Test 9919 9859 9859 9804 9888 9837 9852 9890 9810 9861 1044  Lr  5e4 Train Test   1e4 5e5 Train Test 30600  30400   30200 6097 5960 10400  Params   Fig 1 Training left and Testing right Accuracy of GRU0 GRU1 and GRU2 on MNIST pixelwise generated sequences at eta0001 Fig 2 Training Accuracy of GRU0 GRU1 GRU2 and GRU3 on MNIST generated sequences at eta5e4 Fig 3 Performance of GRU3 on MNIST generated sequences for 3 constant base learning rates 67 From Table II and Fig 1 and 2 GRU1 and GRU2 perform almost as well as GRU0 on MNIST pixelwise generated sequence inputs While GRU3 does not perform as well for this constant base learning rate Figure 3 shows that reducing the constant base learning rate to 00001 and below has enabled GRU3 to increase its test accuracy performance to 596 after 100 epochs and with a positive slope indicating that it would increase further after more epochs Note that in this experiment GRU3 has about 33 of the number of adaptively computed parameters compared to GRU0 Thus there exists a potential tradeoff between the higher accuracy performance and the decrease in the number of parameters In our experiments using 100 epochs the GRU3 architecture never attains saturation Further experiments using more epochs andor more units would shed more light on the comparative evaluation of this tradeoff between performance and parameterreduction B Application to MNIST Dataset  rowwise sequences While pixelwise sequences represent relatively long sequences rowwise generated sequences can test short sequences of length 28 with vector elements The accuracy profile performance vs epochs of the MNIST dataset with rowwise input of all four GRU RNN variants are depicted in Fig 4 Fig 5 and Fig 6 using several constant base learning rates Accuracy performance results are then summarized in Table III below Table III MNIST rowwise generated sequences Accuracy  performance of different variants using several constant base learning rates over 50 epochs Architecture GRU0 GRU1 GRU2 GRU3 Lr  1e2 Train Test 9699 9849 9814 9885 9302 9666 38700 9724 9855 9746 9893 9154 9658 33100 9695 9871 9733 9893 9120 9623 32900 9719 9885 9704 9739 8033 8796 13100 Lr  1e3 Train Test Lr  1e4 Train Test  Params Fig 4 Training and testing accuracy on MNIST rowwise generated sequences at a constant base learning rate of 1e2 Fig 5 Training and testing accuracy on MNIST rowwise generated sequences at a constant base learning rate of 1e3 Fig 6 Training and testing accuracy on MNIST rowwise generated sequences at a constant base learning rate of 1e4 From Table III and Fig 4 Fig5'),\n",
       " Document(metadata={'source': 'GRU.pdf'}, page_content='rate of 1e4 From Table III and Fig 4 Fig5 and Fig 6 all the four variants GRU0 GRU1 GRU2 and GRU3 appear to exhibit comparable accuracy performance over three constant base learning rates GRU3 exhibits lower performance at the base learning rate of 1e4 where after 50 epochs is still lagging From Fig 6 however it appears that the profile has not yet levelled off and has a positive slope More epochs are likely to increase performance to comparable levels with the other variants It is noted that in this experiment GRU3 can achieve comparable performance with roughly one third of the number of adaptively computed parameters Computational expense savings may play a role in favoring one variant over the others in targeted applications andor available resources C Application to the IMDB Dataset natural sequence The IMDB dataset is composed of 25000 test data and 25000 training data consisting of movie reviews and their binary sentiment classification Each review is represented by a maximum of 80 most frequently occurring words in a vocabulary of 20000 words 7 We have trained the dataset on all 4 GRU variants using the two constant base learning rates of 1e3 and 1e4 over 100 epochs In the training we employ 128dimensional GRU RNN variants and have adopted a batch size of 32 We have observed that using the constant base learning rate of 1e3 performance fluctuates visibly see Fig 7 whereas performance is uniformly progressing over profilecurves as shown in Fig 8 Table IV summarizes the results of accuracy performance which show comparable performance among GRU0 GRU1 GRU2 and GRU3 Table IV also lists the number of parameters in each Fig 7 Test and validation accuracy on IMDB dataset using a base learning rate of 1e3 Fig 8 Training and testing accuracy on IMDB dataset using a base learning rate of 1e4 Table IV IMDB dataset Accuracy  performance of different architectures using two base learning rates over 100 epochs Architecture GRU0 GRU1 GRU2 GRU3 Train 953 945 945 923 Lr  1e3 Test 837 841 842 832 Train 874 870 869 868 Lr  1e4 Test 848 848 846 845  Params 98688 65920 65664 33152 The IMDB data experiments provide the most striking results It can be clearly seen that all the 3 GRU variants perform comparably to the GRU RNN while using less number of parameters The learning pace of GRU3 was also similar to those of the other variants at the constant base learning rate of 1e4 From Table IV it is noted that more saving in computational load is achieved by all variant GRU RNN as the input is represented as a large 128dimensional vector V CONCLUSION The experiments on the variants GRU1 GRU2 and GRU3 verse the GRU RNN have demonstrated that their accuracy performance is comparable on three example sequence lengths Two sequences generated from the MNIST dataset and one from the IMDB dataset The main driving signal of the gates appear to be the recurrent state as it contains essential information about other signals Moreover the use of the stochastic gradient'),\n",
       " Document(metadata={'source': 'GRU.pdf'}, page_content='Moreover the use of the stochastic gradient descent implicitly carries information about the network state This may explain the relative success in using the bias alone in the gate signals as its adaptive update carries information about the state of the network The GRU variants reduce their performance has been comparable to the original GRU RNN While GRU1 and GRU2 have indistinguishable performance from the GRU RNN GRU3 frequently lags in performance especially for relatively long sequences and may require more execution time to achieve comparable performance this redundancy and thus We remark that the goal of this work to comparatively evaluate the performance of GRU1 GRU2 and GRU3 which possess less gate parameters and thus less computational expense than the original GRU RNN By performing more experimental evaluations using constant or varying learning rates and training for longer number of epochs one can validate the performance on broader domain We remark that the three GRU RNN variants need to be further comparatively evaluated on diverse datasets for a broader empirical performance evidence REFERENCES 1 Bengio Y Simard P and Frasconi P Learning Longterm Dependencies with Gradient Descent is Difficult IEEE TransNeural Networks 52157166 1994 H Simpson Dumb Robots 3rd ed Springfield UOS Press 2004 pp69 2 Chung J Gulcehre C Cho K and Bengio Y Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling arXiv preprint arXiv14123555 2014 Gers F A Schraudolph N N and Schmidhuber J Learning Precise Timing with LSTM Recurrent Networks Journal of Machine Learning Research 3115143 2002 J G Lu Title of paper with only the first word capitalized J Name Stand Abbrev in press 3 Hochreiter S and Schmidhuber J Long ShortTerm Memory Neural 4 Computation 9817351780 1997 Jozefowicz R Zaremba W and Sutskever I An Empirical Exploration of Recurrent Network Architectures In Proc Intl Conf on Machine Learning pp 23422350 2015 5 Le Q V Jaitly N and Hinton G E A Simple Way to Initialize Recurrent Networks of Rectified Linear Units arXiv preprint arXiv150400941 2015 6 Maas A L Daly R E Pham P T Huang D Ng A Y and Potts C Learning Word Vectors for Sentiment Analysis In Proc 49th Annual Meeting of the ACL pp 142150 2011 7 Mikolov T Joulin A Chopra S Mathieu M and Ranzato M Learning Longer Memory in Recurrent Neural Networks In Intl Conf Learning Represenations 2015 8 Zaremba W Sutskever I and Vinyals O Recurrent Neural Network Regularization arXiv preprint arXiv14092329 2014 9 BoulangerLewandowski Nicolas Bengio Yoshua and Vincent Pascal Modeling temporal dependencies in highdimensional sequences Application to polyphonic music generation and transcription arXiv preprint arXiv12066392 2012 10 Gers Felix A Schmidhuber Jurgen and Cummins Fred Learning to lstm Neural computation forget Continual prediction with 121024512471 2000 11 Mikolov Tomas Joulin Armand Chopra Sumit Mathieu Michael and Ranzato MarcAurelio Learning longer memory in recurrent'),\n",
       " Document(metadata={'source': 'GRU.pdf'}, page_content='MarcAurelio Learning longer memory in recurrent neural networks arXiv preprint arXiv14127753 2014 12 Pascanu Razvan Mikolov Tomas and Bengio Yoshua On the training recurrent neural networks arXiv preprint difficulty of arXiv12115063 2012 13 Zhou G B Wu J Zhang C L and Zhou Z H Minimal Gated Unit for Recurrent Neural Networks arXiv preprint arXiv160309420v1 csNE 31 Mar 2016 14 httpsgithubcomfcholletkerasblobmasterexamplesimdblstmpy 15 F M Salem Reduced Parameterization in Gated Recurrent Neural Networks Memorandum 7112016 MSU Nov 2016 16 F M Salem A Basic Recurrent Neural Network Model arXiv Preprint arXiv 161209022 Dec 2016'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Review Article A Comprehensive Overview and Comparative Analysis on Deep Learning Models Farhad Mortezapour Shiria Raihani Mohameda  Thinagaran Perumala  Norwati Mustaphaa and aFaculty of Computer Science and Information Technology University Putra Malaysia UPM Serdang 43400 Malaysia KEYWORDS ABSTRACT Deep Learning Convolutional Neural Network CNN Long ShortTerm Memory LSTM Gated Recurrent Unit GRU Temporal Convolutional Network TCN Transformer KolmogorovArnold networks KAN Deep Reinforcement Learning DRL Deep Transfer Learning DTL Autoencoder Generative Adversarial Network GAN Deep Belief Network DBN Deep learning DL has emerged as a powerful subset of machine learning ML and artificial intelligence AI outperforming traditional ML methods especially in handling unstructured and large datasets Its impact spans across various domains including speech recognition healthcare autonomous vehicles cybersecurity predictive analytics and more However the complexity and dynamic nature of realworld problems present challenges in designing effective deep learning models Consequently several deep learning models have been developed to address different problems and applications In this article we conduct a comprehensive survey of various deep learning models including Convolutional Neural Network CNN Recurrent Neural Network RNN Temporal Convolutional Networks TCN Transformer KolmogorovArnold networks KAN Generative Models Deep Reinforcement Learning DRL and Deep Transfer Learning We examine the structure applications benefits and limitations of each model Furthermore we perform an analysis using three publicly available datasets IMDB ARAS and Fruit360 We compared the performance of six renowned deep learning models CNN RNN Long ShortTerm Memory LSTM Bidirectional LSTM Gated Recurrent Unit GRU and Bidirectional GRU alongside two newer models TCN and Transformer using the IMDB and ARAS datasets Additionally we evaluated the performance of eight CNNbased models including VGG Visual Geometry Group Inception ResNet Residual Network InceptionResNet Xception Extreme Inception MobileNet DenseNet Dense Convolutional Network and NASNet Neural Architecture Search Network for image classification tasks using the Fruit360 dataset 1 Introduction Artificial intelligence AI aims to emulate humanlevel intelligence in machines In computer science AI refers to the study of intelligent agents which are objects capable of perceiving their environment and taking actions to maximize their chances of achieving specific goals 1 Machine learning ML is a field that focuses on the development and application of methods capable of learning from datasets 2 ML finds extensive use in various domains such as speech recognition A Comprehensive Overview and Comparative Analysis on Deep Learning Models computer vision text analysis video games medical sciences and cybersecurity In recent years deep learning DL techniques a subset of machine learning ML have outperformed traditional ML'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='learning ML have outperformed traditional ML approaches across numerous tasks driven by several critical advancements 3 The proliferation of large datasets has been pivotal in enabling models to learn intricate patterns and relationships thereby significantly enhancing their performance 4 Concurrently advancements in hardware acceleration technologies notably Graphics Processing Units GPUs and FieldProgrammable Gate Arrays FPGAs 5 have markedly reduced model training times by facilitating rapid computations and parallel processing capabilities These advancements have substantially accelerated the training process2 Moreover enhancements in algorithmic techniques for optimization and training have further augmented the speed and efficiency of deep learning models leading to quicker convergence and superior generalization capabilities 4 Deep learning techniques have demonstrated remarkable success across a wide range of applications including computer vision CV natural language processing NLP and speech recognition These applications underscore the transformative impact of DL in various domains where it continues to set new performance benchmarks 6 7 Deep learning models draw inspiration from the structure and functionality of the human nervous system and brain These models employ input hidden and output layers to organize processing units Within each layer the nodes or units are interconnected with those in the layer below and each connection is assigned to a weight value The units sum the inputs after multiplying them by their corresponding weights 8 Fig 1 illustrates the relationship between AI ML and DL highlighting that machine learning and deep learning are subfields of artificial intelligence The objective of this research is to provide a comprehensive overview of various deep learning models and compare their performance across different applications In Section 2 we introduce a fundamental definition of deep learning Section 3 covers supervised deep learning models including MultiLayer Perceptron MLP Convolutional Neural Networks CNN Recurrent Neural Networks RNN Temporal Convolutional Networks TCN and KolmogorovArnold Networks KAN Section 4 reviews generative models such as Autoencoders Generative Adversarial Networks GANs and Deep Belief Networks DBNs Section 5 presents a comprehensive survey of Transformer architecture Deep Reinforcement Learning DRL is discussed in Section 6 while Section 7 addresses Deep Transfer Learning DTL The principles of hybrid deep learning are explored in Section 8 followed by a discussion of deep learning applications in Section 9 Section 10 surveys the challenges in deep learning and potential alternative solutions In Section 11 we conduct experiments and analyze the performance of different deep learning models using three datasets Research directions and future aspects are covered in Section 12 Finally Section 13 concludes the paper Artificial Machine Deep Intelligent Learning Learning Figure 1 Relationship'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Learning Learning Figure 1 Relationship between artificial intelligence AI machine learning ML and deep learning DL 2 F M Shiri et al 2 Deep Learning Deep learning DL involves the process of learning hierarchical representations of data by utilizing architectures with multiple hidden layers With the advancement of highperformance computing facilities deep learning techniques using deep neural networks have gained increasing popularity 9 In a deep learning algorithm data is passed through multiple layers with each layer progressively extracting features and transmitting information to the subsequent layer The initial layers extract lowlevel characteristics which are then combined by later layers to form a comprehensive representation 6 In traditional machine learning techniques the classification task typically involves a sequential process that includes preprocessing feature extraction meticulous feature selection learning and classification The effectiveness of machine learning methods heavily relies on accurate feature selection as biased feature selection can lead to incorrect class classification In contrast deep learning models enable simultaneous learning and classification eliminating the need for separate steps This capability makes deep learning particularly advantageous for automating feature learning across diverse tasks 10 Fig 2 visually illustrates the distinction between deep learning and traditional machine learning in terms of feature extraction and learning In the era of deep learning a wide array of methods and architectures have been developed These models can be broadly categorized into two main groups discriminative supervised and generative unsupervised approaches Among the discriminative models two prominent groups are Convolutional Neural Networks CNNs and Recurrent Neural Networks RNNs Additionally generative approaches encompass various models such as Generative Adversarial Networks GANs and AutoEncoders AEs 11 In the following sections we provide a comprehensive survey of different types of deep learning models Figure 2 Visual illustration of the distinction between deep learning and traditional machine learning in terms of feature extraction and learning 10 3 Supervised Deep Learning Models In supervised learning and classification tasks this family of deep learning algorithms is used to perform discriminative functions These supervised deep architectures typically model the posterior distributions of classes based on observable data enabling effective pattern classification Common supervised models include MultiLayer Perceptron MLP Convolutional Neural Networks CNN Recurrent Neural Networks RNN Temporal Convolutional Networks TCN KolmogorovArnold Networks KAN and their variations A brief overview of these methods follows 31 Multi Layers Perceptron MLP The MultiLayer Perceptron MLP model is a type of feedforward artificial neural network 3 A Comprehensive Overview and Comparative Analysis on Deep Learning Models ANN'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Comparative Analysis on Deep Learning Models ANN that serves as a foundation architecture for deep learning or deep neural networks DNNs 11 It operates as a supervised learning approach The MLP consists of three layers the input layer the output layer and one or more hidden layers 12 It is a fully connected network meaning each neuron in one layer is connected to all neurons in the subsequent layer In an MLP the input layer receives the input data and performs feature normalization The hidden layers which can vary in number process the input signals The output layer makes decisions or predictions based on the processed information 13 Fig 3 a depicts a singleneuron perceptron model where the activation function  Eq 1 is a nonlinear function used to map the summation function    to the output value       1 In Eq 1 the terms    and  represent the input vector weighting vector bias and output value respectively 14 Fig 3 b illustrates the structure of the multilayer perceptron MLP model a b Figure 3 a Singleneuron perceptron model b Structure of the MLP 14 32 Convolutional Neural Networks CNN Convolutional Neural Networks CNNs are a powerful class of deep learning models widely applied in various tasks including object detection speech recognition computer vision image classification and bioinformatics 15 They have also demonstrated success in time series prediction tasks 16 CNNs are feedforward neural networks that leverage convolutional structures to extract features from data 17 CNN has a twostage architecture that combines a classifier and a feature extractor to provide automatic feature extraction and endtoend training with the least amount of preprocessing necessary 18 Unlike traditional methods CNNs automatically learn and recognize features from the data without the need for manual feature extraction by humans 19 The design of CNNs is inspired by visual perception 17 The major components of CNNs include the convolutional layer pooling layer fully connected layer and activation function 20 21 Fig 4 presents the pipeline of the convolutional neural network highlighting how each layer contributes to the efficient processing and successful progression of input data through the network convolution layer Pooling layer Fully connected layer Class 1 Input Data Class N Figure 4 The pipeline of a Convolutional Neural Network 4 F M Shiri et al Figure 5 Schematic diagram of the convolution process 22 Convolutional Layer The convolutional layer is a pivotal component of CNN Through multiple convolutional layers the convolution operation extracts distinct features from the input In image classification lower layers tend to capture basic features such as texture lines and edges while higher layers extract more abstract features The convolutional layer comprises learnable convolution kernels which are weight matrices typically of equal length width and an odd number eg 3x3 5x5 or 7x7 These kernels are convolved with the input feature maps sliding over the'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='with the input feature maps sliding over the regions of the feature map and executing convolution operations 22 Fig 5 illustrates the schematic diagram of the convolution process Pooling Layer Typically following the convolutional layer the pooling layer reduces the number of connections in the network by performing downsampling and dimensionality reduction on the input data 23 Its primary purpose is to alleviate the computational burden and address overfitting issues 24 Moreover the pooling layer enables CNN to recognize objects even when their shapes are distorted or viewed from different angles by incorporating various dimensions of an image through pooling 25 The pooling operation produces output feature maps that are more robust against distortion and errors in individual neurons 26 There are various pooling methods including Max Pooling Average Pooling Spatial Pyramid Pooling Mixed Pooling MultiScale OrderLess and Stochastic Pooling 2730 Fig 6 depicts an example of Max Pooling where a window slides across the input and the contents of the window are processed by a pooling function 31 Figure 6 Computing the output values of a 3  3 max pooling operation on a 5  5 input 5 A Comprehensive Overview and Comparative Analysis on Deep Learning Models X1 w1 X2 w2 Output    WN          f z XN Figure 7 The general structure of activation functions Fully Connected FC Layer The FC layer is typically located at the end of a CNN architecture In this layer every neuron is connected to all neurons in the preceding layer adhering to the principles of a conventional multilayer perceptron neural network The FC layer receives input from the last pooling or convolutional layer which is a vector created by flattening the feature maps The FC layer serves as the classifier in the CNN enabling the network to make predictions 10 Activation Functions Activation functions are fundamental components in convolutional neural networks CNNs indispensable for introducing nonlinearity into the network This non linearity is crucial for CNNs ability to model complex patterns and relationships within the data allowing it to perform tasks beyond simple linear classification or regression Without nonlinear activation functions a CNN would be limited to linear operations significantly constraining its capacity to accurately represent the intricate nonlinear behaviors typical of many realworld phenomena 32 Fig 7 typically illustrates how these activation functions modulate input signals to produce output highlighting the nonlinear transformations applied to the input data across different regions of the function curve In this figure  represents the input feature while  denotes the weight associated with the connection between the input feature  and neuron  The figure shows that neuron  receives  features simultaneously The output from neuron  is labeled by  and its internal state or bias is indicated by  The activation function depicted as   could be any one of several types such as'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='as   could be any one of several types such as the Rectified Linear Unit ReLU hyperbolic tangent Tanh Sigmoid function or others 33 34 These various activation functions are shown in Fig 8 with emphasis on their distinct characteristics and profiles These activation functions are essential for convolutional neural networks CNNs to be more effective in a variety of applications by allowing them to recognize intricate patterns and provide accurate predictions Sigmoid and Tanh functions are frequently referred to as saturating nonlinearities due to the way they act when inputs are very large or small As per the reference the Sigmoid function approaches values of 0 or 1 whereas the Tanh function leans towards 1 or 117 Different alternative nonlinearities have been suggested for reducing problems associated with these saturating effects including Rectified Linear Unit ReLU 35 Leaky ReLU 36 Parametric Rectified Linear Units PReLU 37 Randomized Leaky ReLU RReLU 38 Sshaped ReLU SReLU 39 and Exponential Linear Units ELUs 40 Gaussian Error Linear Units GELUs 41 6 F M Shiri et al Sigmoid Hyperbolic Tangent ReLU Leaky ReLU ELU GELU Figure 8 Diagram of different activation functions ReLU Rectified Linear Unit is one of the most often used activation functions in modern CNNs because of how well it solves the vanishing gradient issue during training The definition of ReLU in mathematics is as Eq 2 where the input to the neuron is represented by  34   max0        0 0    0 2 This feature helps CNN learn complicated features more efficiently by effectively turning off any negative input values while maintaining positive values It also keeps neurons from being saturated during training As an alternative the definition of the Sigmoid function is represented by Eq 3 where  stands for the input of the neuron 1  3 Although the sigmoid distinctive Sshape and capacity to condense real numbers into a range between 0 and 1 make it useful for binary classification its propensity to saturate can hinder training by causing the vanishing gradient problem in deep neural networks   Convolutional Neural Networks CNNs are extensively used in various fields including natural language processing image segmentation image analysis video analysis and more Several CNN variations have been developed such as AlexNet 42 VGG Visual Geometry Group 43 Inception 44 45 ResNet Residual Networks 46 47 WideResNet 48 FractalNet 49 SqueezeNet 50 InceptionResNet 51 Xception Extreme Inception 52 MobileNet 53 54 DenseNet Dense Convolutional Network 55 SENet SqueezeandExcitation Network 56 Efficientnet 57 58 among others These variants are applied in different application areas based on their learning capabilities and performance 7 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 33 Recurrent Neural Networks RNN Recurrent Neural Networks RNNs are a class of deep learning models that possess internal memory enabling them to capture sequential dependencies Unlike traditional'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='sequential dependencies Unlike traditional neural networks that treat inputs as independent entities RNNs consider the temporal order of inputs making them suitable for tasks involving sequential information 59 By employing a loop RNNs apply the same operation to each element in a series with the current computation depending on both the current input and the previous computations 60 The ability of RNNs to utilize contextual information is particularly valuable in tasks such as natural language processing video classification and speech recognition For example in language modeling understanding the preceding words in a sentence is crucial for predicting the next word RNNs excel at capturing such dependencies due to their recurrent nature6163 However a limitation of simple RNN is their shortterm memory which restricts their ability to retain information over long sequences 64 To overcome this more advanced RNN variants have been developed including Long ShortTerm Memory LSTM 65 bidirectional LSTM 66 Gated Recurrent Unit GRU 67 bidirectional GRU 68 Bayesian RNN 69 and others Figure 9 Simple RNN internal operation 70 Fig 9 depicts a simple recurrent neural network where the internal memory  is computed using Eq 4 70        4 In this equation  represents the activation function typically Tanh  and  are adjustable weight matrices for the hidden state   is the bias and  denotes the input vector RNNs have proven to be powerful models for processing sequential data leveraging their ability to capture dependencies over time Various types of RNN models such as LSTM bidirectional LSTM GRU and bidirectional GRU have been developed to address specific challenges in different applications 331 Long ShortTerm Memory LSTM Long ShortTerm Memory LSTM is an advanced variant of Recurrent Neural Networks RNN that addresses the issue of capturing longterm dependencies LSTM was initially introduced by 65 in 1997 and further improved by 71 in 2013 gaining significant popularity in the deep learning community Compared to standard RNN LSTM models have proven to be more effective at retaining and utilizing information over longer sequences In an LSTM network the current input at a specific time step and the output from the previous time step are fed into the LSTM unit which then generates an output that is passed to the next time step The final hidden layer of the last time step sometimes along with all hidden layers is commonly employed for classification purposes 72 The overall architecture of an LSTM network is depicted in Fig 10 a LSTM consists of three gates input gate forget gate and output gate Each gate performs a specific function in controlling the flow of information The input gate decides how to update the internal state based on the current input and the previous internal state The forget gate determines how much of the previous internal state should be forgotten Finally the output gate regulates the influence of the internal state on the system 60 73 8 F M'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='of the internal state on the system 60 73 8 F M Shiri et al a b Figure 10 a The highlevel architecture of LSTM b The inner structure of LSTM unit 60 Fig 10 b illustrates the update mechanism within the inner structure of an LSTM The update for the LSTM unit is expressed by Eq 5     1     1      1     5     1         1      where  and  represent the activation functions of the system state and internal state typically utilizing the hyperbolic tangent function The gating operation denoted as g is a feedforward neural network with a sigmoid activation function ensuring output values within the range of 0 1 which are interpreted as a set of weights The subscripts   and  correspond to the input gate output gate and forget gate respectively 1    While standard LSTM has demonstrated promising performance in various tasks it may struggle to comprehend input structures that are more complex than a sequential format To address this limitation a treestructured LSTM network known as SLSTM was proposed by 74 S LSTM consists of memory blocks comprising an input gate two forget gates a cell gate and an output gate While SLSTM exhibits superior performance in challenging sequential modeling problems it comes with higher computational complexity compared to standard LSTM 75 332 Bidirectional LSTM Bidirectional Long ShortTerm Memory BiLSTM is an extension of the LSTM architecture that addresses the limitation of standard LSTM models by considering both past and future context in sequence modeling tasks While traditional LSTM models process input data only in the forward direction BiLSTM overcomes this limitation by training the model in two directions forward and backward 76 A BiLSTM consists of two parallel LSTM layers one processes the input sequence in the forward direction while the other processes it in the backward direction The forward LSTM layer reads the input data from left to right as indicated by the green arrow in Fig 11 Simultaneously the backward LSTM layer reads the input data from right to left as represented by the red arrow 77 This bidirectional processing enables the model to capture information from both past and future contexts allowing for a more comprehensive understanding of temporal dependencies within the sequence 9 A Comprehensive Overview and Comparative Analysis on Deep Learning Models Figure 11 The architecture of a Bidirectional LSTM model 76 During the training phase the forward and backward LSTM layers independently extract features and update their internal states based on the input sequence The output of each LSTM layer at each time step is a prediction score These prediction scores are then combined using a weighted sum to generate the final output result 77 By incorporating information from both directions BiLSTM models can capture a broader context and improve the models ability to model temporal dependencies in sequential data BiLSTM has been widely applied in various sequence modeling tasks such as natural language'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='sequence modeling tasks such as natural language processing speech recognition and sentiment analysis It has shown promising results in capturing complex patterns and dependencies in sequential data making it a popular choice for tasks that require an understanding of both past and future context 333 Gated Recurrent Unit GRU The Gated Recurrent Unit GRU is another variant of the RNN architecture that addresses the shortterm memory issue and offers a simpler structure compared to LSTM 59 GRU combines the input gate and forget gate of LSTM into a single update gate resulting in a more streamlined design Unlike LSTM GRU does not include a separate cell state A GRU unit consists of three main components an update gate a reset gate and the current memory content These gates enable the GRU to selectively update and utilize information from previous time steps allowing it to capture longterm dependencies in sequences 78 Fig 12 illustrates the structure of a GRU unit 79 The update gate Eq 6 determines how much of the past information should be retained and combined with the current input at a specific time step It is computed based on the concatenation of the previous hidden state 1 and the current input  followed by a linear transformation and a sigmoid activation function   1    6 The reset gate Eq 7 decides how much of the past information should be forgotten It is computed in a similar manner to the update gate using the concatenation of the previous hidden state and the current input   1    7 The current memory content Eq 8 is calculated based on the reset gate and the concatenation of the transformed previous hidden state and the current input The result is passed through a hyperbolic tangent activation function to produce the candidate activation   1  8 10 F M Shiri et al Figure 12 The structure of a GRU unit 79 Finally the final memory state  is determined by a combination of the previous hidden state and the candidate activation Eq 9 The update gate determines the balance between the previous hidden state and the candidate activation Additionally an output gate  can be introduced to control the information flow from the current memory content to the output Eq 10 The output gate is computed using the current memory state  and is typically followed by an activation function such as the sigmoid function   1  1   9      10 where the weight matrix of the output layer is  and the bias vector of the output layer is  GRU offers a simpler alternative to LSTM with fewer tensor operations allowing for faster training However the choice between GRU and LSTM depends on the specific use case and problem at hand Both architectures have their advantages and disadvantages and their performance may vary depending on the nature of the task 59 334 Bidirectional GRU The Bidirectional Gated Recurrent Unit BiGRU 80 improves upon the conventional GRU architecture through the integration of contexts from the past and future in sequential modeling tasks In contrast to'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='in sequential modeling tasks In contrast to the conventional GRU which exclusively processes input sequences forward the BiGRU manages sequences in both forward and backward directions In order to do this two parallel GRU layers are used one of which processes the input data forward and the other in reverse Fig 13 shows the BiGRUs structural layout Figure 13 The structure of a BiGRU model 81 11 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 34 Temporal Convolutional Networks TCN Temporal Convolutional Networks TCN represent a significant advancement in neural network architectures specifically tailored for handling sequential data particularly time series Originating as an extension of the onedimensional Convolutional Neural Network CNN TCN was first introduced by 82 in 2017 for the task of action segmentation in video data and its application was further generalized to other types of sequential data by 83 in 2018 TCN retains the powerful feature extraction capabilities inherent to CNN while being highly efficient in processing and analyzing time series data The purpose of training TCN is to forecast the next  values of the input time series Assume that we have a sequence of inputs 0 1     We would like to predict at each time step some corresponding output 0 1     whose values are equal to the input shifted forward  time steps The primary limitation is that it can only use the inputs that have already been observed 0 1     when forecasting the output  for a given time step  84 TCN is characterized by two fundamental properties 1 The convolutions within the network are causal ensuring that the output at any given time step depends solely on the current and past inputs without any influence from future inputs 2 Similar to Recurrent Neural Networks RNNs TCN can process sequences of arbitrary length and produce output sequences of identical length The three primary components of a typical TCN are residual connections dilated convolution and causal convolution 83 85 86 Fig 14 illustrates the schematic architecture of a TCN model Figure 14 Schematic diagram of the TCN model architecture 87 Causal Convolution TCN architecture is built upon two foundational principles To adhere to the first principle the initial layer of a TCN is a onedimensional fully convolutional network wherein each hidden layer maintains the same length as the input layer achieved through zeropadding This padding ensures that each successive layer remains the same length as the preceding one To satisfy the second principle TCN employs causal convolutions A causal convolution is a specialized one dimensional convolutional network where only elements from time  and earlier are convolved to produce the output at time  Fig 15 demonstrates the structure of a causal convolutional network Dilated Convolution TCN aims to effectively capture longrange dependencies in sequential data A simple causal convolution can only consider a history that scales linearly with'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='only consider a history that scales linearly with the depth of the network This limitation would necessitate the use of large filters or an exceptionally deep network structure which could hinder performance particularly for tasks requiring a longer history 12 F M Shiri et al Figure 15 The structure of the causal convolutional network 85 The depth of the network could lead to issues such as vanishing gradients ultimately degrading network performance or causing it to plateau To address these challenges TCN employs dilated convolutions 88 which exponentially expand the receptive field allowing the network to process large time series efficiently without a proportional increase in computational complexity The architecture of a dilated convolutional network is depicted in Fig 16 By inserting gaps between the weights of the convolutional kernel dilated convolutions effectively increase the networks receptive field while maintaining computational efficiency The mathematical formulation of a dilated convolution is given by Eq 11 1           11 0 where  is the dilation rate  is the size of the filter and      accounts for the direction of the past Dilation is the same as adding a fixed step in between each pair of neighboring filter taps When   1 dilated convolution becomes a regular convolution As  increases the output at the higher layers reflects a broader range of inputs improving performance on longrange dependencies in time series Residual Connections To construct a more expressive TCN model it is essential to use small filter sizes and stack multiple layers However stacking dilated and causal convolutional layers increases the depth of the network potentially leading to problems such as gradient decay or vanishing gradients during training To mitigate these issues TCN incorporates residual connections into the output layer Residual connections facilitate the flow of data across layers by adding a shortcut path allowing the network to learn residual functions which are modifications to the identity mapping rather than learning a full transformation This approach has been shown to be highly effective in very deep networks Figure 16 Dilated convolutional structure 85 13 A Comprehensive Overview and Comparative Analysis on Deep Learning Models A residual block 46 has a branch that lead to a set of transformations F whose outputs are appended to blocks input x as shown in Eq 12       12 This method enables the network to focus on learning residual functions rather than the entire mapping The TCN residual block typically consists of two layers of dilated causal convolutions followed by a nonlinear activation function such as Rectified Linear Unit ReLU The convolutional filters within the TCN are normalized using weight normalization 89 and dropout 90 is applied to each dilated convolution layer for regularization where an entire channel is zeroed out at each training step In contrast to a conventional ResNet where the input is directly added to the'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='ResNet where the input is directly added to the output of the residual function TCN adjusts for differing inputoutput widths by performing an additional 1  1 convolution to ensure that the elementwise addition  operates on tensors of matching dimensions 36 KolmogorovArnold Network KAN KolmogorovArnold Networks KANs represent a promising alternative to traditional Multi Layer Perceptrons MLPs by leveraging the KolmogorovArnold theorem a sophisticated mathematical framework that enhances the capacity of neural networks to process complex data structures KANs were first introduced in 2024 by 91 with the goal of incorporating advanced mathematical theories into deep learning architectures to improve their performance on intricate tasks While MLPs are inspired by the universal approximation theorem KANs are motivated by the KolmogorovArnold representation theorem 92 which states that any multivariate continuous function  over a bounded domain can be expressed as a finite composition of simpler one dimensional continuous functions 21  1           13 1 1 where  is a mapping 01   and  is a mapping    KAN maintains a fully connected structure like MLP but with a key distinction while MLP assign fixed activation functions to nodes neurons KAN assigns learnable activation functions to edges weights Consequently KAN does not employ traditional linear weight matrices instead each weight parameter is replaced by a learnable onedimensional function parameterized as a spline Unlike MLP which apply nonlinear activation functions at each node KAN nodes only sum the incoming data relying on the rich learnable spline functions to introduce nonlinearity Although this approach might initially seem computationally expensive KAN often result in significantly smaller computation graphs compared to MLP Fig 17 illustrates the structure of a KAN Figure 17 The structure of KolmogorovArnold Network KAN 91 14 F M Shiri et al The KolmogorovArnold Network KAN can be expressed specifically as follows   1  2      1  1 14 The transformation of each layer   operates on the input  to generate 1 the input for the following layer as follows 1     11 12 21 22   11 12  1  2    1      15 Where each activation function  is a spline offering a rich flexible response surface to inputs from the model         16  Several variants of KANs have emerged to tackle specific challenges in various applications  Convolutional KAN CKAN 93 CKAN is a pioneering alternative to standard CNN which have significantly advanced the field of computer vision Convolutional KAN integrate the nonlinear activation functions of KAN into the convolutional layers leading to a substantial reduction in the number of parameters and offering a novel approach to optimizing neural network architectures  Temporal KAN TKAN 94 Temporal KolmogorovArnold Networks combines the principles of KAN and Long ShortTerm Memory LSTM networks to create an advanced architecture for time series analysis Comprising layers of Recurrent'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='series analysis Comprising layers of Recurrent KolmogorovArnold Networks RKANs with embedded memory management TKAN excels in multistep time series forecasting The TKAN architecture offers tremendous promise for improvement in domains needing onestepahead forecasting by solving the shortcomings of existing models in handling complicated sequential patterns 95 96  Multivariate Time Series KAN MTKAN 97 MTKAN is specifically designed to handle multivariate time series data The primary objective of MTKAN is to enhance forecasting accuracy by modeling the intricate interactions between multiple variables MTKAN utilizes splineparametrized univariate functions to capture temporal relationships while incorporating methods to model crossvariable interactions  Fractional KAN fKAN 98 fKAN is an enhancement of the KAN architecture that integrates the unique properties of fractionalorthogonal Jacobi functions into the networks basis functions This method guarantees effective learning and improved accuracy by utilizing the special mathematical characteristics of fractional Jacobi functions such as straightforward derivative equations nonpolynomial behavior and activity for positive and negative input values  Wavelet KAN WavKAN 99 The purpose of this innovative neural network design is to improve interpretability and performance by incorporating wavelet functions into the KolmogorovArnold Networks KAN framework WavKAN is an excellent way to capture complicated data patterns by utilizing wavelets multiresolution analysis capabilities It offers a reliable solution to the drawbacks of both recently suggested KANs and classic multilayer perceptrons MLPs  Graph KAN 100 This innovative model applies KAN principles to graphstructured data replacing the MLP and activation functions typically used in Graph Neural Networks GNNs with KAN This substitution enables more effective feature extraction from graphlike data structures 15 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 4 Generative Unsupervised Deep Learning Models Supervised machine learning is widely used in artificial intelligence AI while unsupervised learning remains an active area of research with numerous unresolved questions However recent advancements in deep learning and generative modeling have injected new possibilities into unsupervised learning A rapidly evolving domain within computer vision research is generative models GMs These models leverage training data originating from an unknown datagenerating distribution to produce novel samples that adhere to the same distribution The ultimate goal of generative models is to generate data samples that closely resemble real data distribution 101 Various generative models have been developed and applied in different contexts such as AutoEncoder 102 Generative Adversarial Network GAN 103 Restricted Boltzmann Machine RBM 104 and Deep Belief Network DBN 105 41 Autoencoder The concept of an autoencoder originated as a neural network'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='of an autoencoder originated as a neural network designed to reconstruct its input data Its fundamental objective is to learn a meaningful representation of the data in an unsupervised manner which can have various applications including clustering 102 An autoencoder is a neural network that aims to replicate its input at its output It consists of an internal hidden layer that defines a code representing the input data The autoencoder network is comprised of two main components an encoder function denoted as    and a decoder function that generates a reconstruction denoted as    106 The function  transforms a data point  from the data space to the feature space while the function  transforms  from the feature space back to the data space to reconstruct the original data point   In modern autoencoders these functions    and    are considered as stochastic functions represented as   and   respectively where  denotes the reconstruction of  107 Fig 18 illustrates an autoencoder model Autoencoder models find utility in various unsupervised learning tasks such as generative modeling 108 dimensionality reduction 109 feature extraction 110 anomaly or outlier detection 111 and denoising 112 Reconstructed Original data data Compressed Representation Figure 18 The structure of autoencoders In general autoencoder models can be categorized into two major groups Regularized Autoencoders which are valuable for learning representations for subsequent classification tasks and Variational Autoencoders 113 which can function as generative models Examples of include Sparse Autoencoder SAE 114 Contractive regularized autoencoder models Autoencoder CAE 115 and Denoising Autoencoder DAE 116 Variational Autoencoder VAE is a generative model that employs probabilistic distributions such as the mean and variance of a Gaussian distribution for data generation 102 VAE provide a principled framework for learning deep latentvariable models and their associated inference models The VAE consists of two coupled but independently parameterized models the encoder or recognition model and the decoder or generative model During expectation maximization 16 F M Shiri et al learning iterations the generative model receives an approximate posterior estimation of its latent random variables from the recognition model which it uses to update its parameters Conversely the generative model acts as a scaffold for the recognition model enabling it to learn meaningful representations of the data such as potential class labels In terms of Bayes rule the recognition model is roughly the inverse of the generative model 117 42 Generative Adversarial Network GAN A notable neural network architecture for generative modeling capable of producing realistic and novel samples on demand is the Generative Adversarial Network GAN initially proposed by Ian Goodfellow in 2014 103 A GAN consists of two key components a generative model and a discriminative model The generative model aims to generate data'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='model The generative model aims to generate data that resemble real ones while the discriminative model aims to differentiate between real and synthetic data Both models are typically implemented using multilayer perceptrons 118 Fig 19 depicts the framework of a GAN where a twoplayer adversarial game is played between a generator G and a discriminator D The generators updating gradients are determined by the discriminator through an adaptive objective 119 Figure 19 The framework of a GAN As previously mentioned GANs operate based on principles derived from neural networks utilizing a training set as input to generate new data that resembles the training set In the case of GANs trained on image data they can generate new images exhibiting humanlike characteristics The following outlines the stepbystep operation of a GAN 120 1 The generator created by a discriminative network generates content based on the real data distribution 2 The system undergoes training to increase the discriminators ability to distinguish between synthesized and real candidates allowing the generator to better fool the discriminator 3 The discriminator initially trains using a dataset as the training data 4 Training sample datasets are repeatedly presented until the desired accuracy is achieved 5 The generator is trained to process random input and generate candidates that deceive the discriminator 17 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 6 Backpropagation is employed to update both the discriminator and the generator with the former improving its ability to identify real images and the latter becoming more adept at producing realistic synthetic images 7 Convolutional Neural Networks CNNs are commonly used as discriminators while deconvolutional neural networks are utilized as generative networks Generative Adversarial Networks GANs have introduced numerous applications across various domains including image blending 121 3D object generation 122 face aging 123 medicine 124 125 steganography 126 image manipulation 127 text transfer 128 language and speech synthesis 129 traffic control 130 and video generation 131 Furthermore several models have been developed based on the Generative Adversarial Network GAN framework to address specific tasks These models include Laplacian GAN Lap GAN 132 Coupled GAN CoGAN 118 Markovian GAN 133 Unrolled GAN 134 Wasserstein GAN WGAN 135 and Boundary Equilibrium GAN BEGAN 136 CycleGAN 137 DiscoGAN 138 Relativistic GAN 139 StyleGAN 140 Evolutionary GAN EGAN 119 Bayesian Conditional GAN 141 Graph Embedding GAN GEGAN 130 43 Deep Belief Network DBN The Deep Belief Network DBN is a type of deep generative model utilized primarily in unsupervised learning to uncover patterns within large datasets Consisting of multiple layers of hidden units DBNs are adept at identifying intricate patterns and extracting features from data Unlike discriminative models DBNs exhibit a higher resistance to overfitting making them'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='a higher resistance to overfitting making them well suited for feature extraction from unlabeled data 142 The stack of Restricted Boltzmann Machines RBMs which operate in an unsupervised learning framework is a fundamental part of DBN Every RBM in a DBN is made up of a hidden layer that contains latent representations and a visible layer that represents observable data features 143 RBMs are trained layer by layer first each RBM is trained independently and then all of the RBMs are finetuned together as a whole within the DBN During the forward pass the activations represent the probability of an output given a weighted input In the backward pass the activations estimate the probability of inputs given the weighted outputs Through iterative training of RBMs within a DBN these processes converge to form joint probability distributions of activations and inputs allowing the network to effectively capture the underlying data structure 144 145 Fig 20 illustrates the schematic structure of a Deep Belief Network DBN Figure 20 structure of a DBN model 143 18 F M Shiri et al 5 Transformer Architecture The Transformer architecture was originally introduced by Vaswani et al 146 in 2017 for machine translation and has since become a foundational model in deep learning especially for natural language processing NLP The transformer functions as a selfattention encoderdecoder structure The encoder consists of a stack of identical layers and each layer consists of two sublayers A multihead selfattention mechanism is the first layer while the other layer is a positionwise fully connected feedforward network Also A normalizing layer 147 and residual connections 46 connect the multiheaded selfattention modules inputs and output After that a decoder uses the representation that the encoder produced to create an output sequence A stack of identical layers makes up the decoder as well The decoder adds a third sublayer to each encoder layer in addition to the primary two and this sublayer handles multihead attention over the encoder stacks output Like the encoder residual connections and a normalizing layer are used surrounding each of the sublayers The encoder and decoders overall Transformer design is depicted in Fig 21 left and right halves respectively 148 149 Traditional RNNbased Seq2Seq models could be replaced with attention layers Using various projection matrices the query key and value vectors in the selfattention layer are all produced from the same sequence 150 RNN training takes a very long period because it is sequential and iterative Transformer training on the other hand is parallel and enables all features to be learned concurrently significantly improving computational efficiency and cutting down on the amount of time needed for model training 151 MultiHead Attention In the Transformer model a multiheaded selfattention mechanism is employed to enhance the models ability to capture dependencies between elements in a sequence The core principle of the'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='elements in a sequence The core principle of the attention mechanism is that every token in the sequence can aggregate information from other tokens allowing the model to understand contextual relationships more effectively This is achieved by mapping a query a set of keyvalue pairs and an output each represented as vectors to form an attention function The output is computed as a weighted sum of the values where the weights are determined by the compatibility function between the query and its corresponding key 146 Figure 21 The architecture of the Transformer model 146 19 A Comprehensive Overview and Comparative Analysis on Deep Learning Models Multihead attention is equivalent to the blended of  distinct scaled dotproduct attention selfattention It can effectively process the three vectors Q K and V in parallel to obtain the final result by combining and calculating The formula is visible in Eq 17      1   2      Where the projections are parameter matrices            17            The main component of the transformer scaled dotproduct attention selfattention uses the weight of each sensor event in the input vector which is represented by            18 The initial step in scaled dotproduct attention is to convert the input data into an embedding vector and the three vectors of query vector Q key vector K and value vector V are then extracted from the embedding vectors Next a score is determined for every vector score is equal to    Score normalization dividing by  is used for gradient stability Next the score is processed using the softmax activation function The weighted score  for every input vector is obtained by taking the softmax dot product value  The final result is produced after summing Scaled dotproduct attention and multihead attention are displayed in Fig 22 152 Positionwise FeedForward Networks Each encoder and decoder layer have a fully connected feedforward network in addition to attention sublayers This feedforward network is applied to each position independently and in the same way This is made up of two linear transformations connected by a ReLU activation   0 1  12  2 19 Positional Encoding Since the Transformer model does not rely on recurrence or convolution it requires a way to capture the relative or absolute positions of tokens within a sequence to effectively utilize the sequences order To address this positional encoding is introduced at the input level of both the encoder and decoder stacks These positional encodings are added to the input embeddings as they share the same dimensionality  This combination enables the model to incorporate positional information allowing it to better understand the sequential nature of the data 146 a b Figure 22 a Scaled DotProduct Attention b MultiHead Attention 20 F M Shiri et al Positional encodings in transformer architecture were achieved by using sine and cosine functions of various frequencies  2  100002 21  100002 20 where  is the position and  is the dimension Every'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='is the position and  is the dimension Every dimension of the positional encoding has a sinusoidal relationship The wavelengths range from 2  10000  2 in a geometric development This function was selected because it would make it simple for the model to learn how to attend to relative positions since for any fixed offset   can be expressed as a linear function of  51 Transformer Variants The Transformer architecture has proven to be highly versatile with numerous variants developed to address specific challenges across different domains Typically Transformers are pre trained on large datasets using unsupervised methods to learn general representations which are then finetuned on specific tasks using supervised learning This hybrid approach leverages the strengths of both learning paradigms Some notable Transformer variants include  Bidirectional Encoder Representations from Transformers BERT 153 A multilayer bidirectional Transformer encoder for unsupervised pretraining in natural language understanding NLU tasks  Generative pretraining Transformer GPT 154 155 A type of Transformer model developed by OpenAI that excels in natural language processing NLP tasks through unsupervised pretraining followed by supervised finetuning  TransformerXL 156 It is proposed for language modeling to permit learning reliance beyond a set temporal coherence TransformerXL TransformerExtra Long comprises a unique relative positional encoding method and a segmentlevel recurrence mechanism This approach not only makes it possible to record longerterm dependencies but also fixes the issue of context fragmentation length without compromising  XLNet 157 It is a generalized autoregressive AR pretraining technique that combines the benefits of autoencoding AE and autoregressive AR techniques with a permutation language modeling aim XLNets neural architecture which integrates TransformerXL and the twostream attention mechanism is built to function effortlessly with the autoregressive AR objective  Fast Transformer 158 It introduces multiquery attention as an alternative to multihead attention This approach reduces memory bandwidth requirements leading to increased processing speed  Multimodal Transformer MulT 159 It is designed for analyzing human multimodal language At the heart of MulT is the crossmodal attention mechanism which provides a latent crossmodal adaptation that fuses multimodal information by directly attending to lowlevel features in other modalities  Vision Transformer ViT 160 An innovative approach based on Transformer structure for visual tasks like image classification  Pyramid Vision Transformer PVT 161 An Transformer framework for complex prediction tasks like semantic segmentation and object recognition  Swin Transformer 162 A hierarchical transformer that uses shifted windows to construct its representation A wide variety of vision tasks including semantic segmentation object detection and image classification may be performed with Swin Transformer'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='may be performed with Swin Transformer  TokenstoToken Vision Transformer T2TViT 163 A vision transformer that can be 21 A Comprehensive Overview and Comparative Analysis on Deep Learning Models trained from scratch on ImageNet T2TViT overcomes ViTs drawbacks by accurately modeling the structural information of images and enhancing feature richness  Transformer in Transformer TNT 164 A vision transformer for visual recognition Both local and global representations are extracted by the TNT architecture through the use of an inner transformer and an outer transformer  PyramidTNT 165 A improved TNT model which used pyramid architecture and convolutional stem in order to greatly enhance the original TNT model  Switch Transformers 166 It is suggested as a straightforward and computationally effective method of increasing a Transformer models parameter count  ConvNeXt 167 A redesigned transformer architecture that makes use of the transformer attention mechanism and incorporates convolutional layers into the encoder and decoder modules to extract spatially localized data  Evolutionary Algorithm Transformer EATFormer 168 An improved vision transformer influenced by an evolutionary algorithm 6 Deep Reinforcement Learning Reinforcement learning RL is a machine learning approach that deals with sequential decisionmaking aiming to map situations to actions in a way that maximizes the associated reward Unlike supervised learning where explicit instructions are given after each system action in the RL framework the learner known as an agent is not provided with explicit guidance on which actions to take at each timestep  The RL agent must explore through trial and error to determine which actions yield the highest rewards 169 Furthermore unlike supervised learning where the correct output is obtained and the model is updated based on the loss or error RL uses gradients without a differentiable loss function to teach a model to explore randomly and learn to make optimal decisions 170 Fig 23 depicts the agentenvironment interaction in reinforcement learning RL The standard theoretical framework for RL is based on a Markov Decision Process MDP which extends the concept of a Markov process and is used to model decisionmaking based on states actions and rewards 171 Deep reinforcement learning combines the decisionmaking capabilities of reinforcement learning with the perception function of deep learning It is considered a form of real AI as it aligns more closely with human thinking Fig 24 illustrates the basic structure of deep reinforcement learning where deep learning processes sensory inputs from the environment and provides the current state data The reinforcement learning process then links the current state to the appropriate action and evaluates values based on anticipated rewards 172 173 22 F M Shiri et al Figure 23 AgentEnvironment interaction in RL Figure 24 Basic structure of Deep Reinforcement Learning DRL 172 One of the most renowned deep'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Learning DRL 172 One of the most renowned deep reinforcement learning models is the Deep Qlearning Network DQN 174 which directly learns policies from highdimensional inputs using Convolutional Neural Network CNN Other common models in deep reinforcement learning include Double DQN 175 Dueling DQN 176 and Monte Carlo Tree Search MCTS 177 Deep reinforcement learning DRL models find applications in various domains such as video game playing 178 179 robotic manipulation 180 181 image segmentation 182 183 video analysis 184 185 energy management 186 187 and more 7 Deep Transfer Learning Deep neural networks have significantly improved performance across various machine learning tasks and applications However achieving these remarkable performance gains often requires large amounts of labeled data for supervised learning as it relies on capturing the latent patterns within the data 188 Unfortunately in certain specialized domains the availability of sufficient training data is a major challenge Constructing a largescale highquality annotated dataset is costly and timeconsuming 189 To address the issue of limited training data transfer learning TL has emerged as a crucial tool in machine learning The concept of transfer learning finds its roots in educational psychology where the theory of generalization suggests that transferring knowledge from one context to another is facilitated by generalizing experiences To achieve successful transfer there needs to be a connection between the two learning tasks For example someone who has learned to play the violin is likely to learn the piano more quickly due to the shared characteristics between musical instruments 190 Fig 25 depicts the learning process of transfer learning Deep transfer learning DTL makes use of the learning experience to reduce the time and effort needed to train large networks as well as the time and effort needed to create the weights for an entire network from scratch 191 With the growing popularity of deep neural networks in various fields numerous deep transfer learning techniques have been proposed Deep transfer learning can be categorized into four main types based on the techniques employed 189 instancesbased deep transfer learning mapping based featurebased deep transfer learning networkbased modelbased deep transfer learning 23 A Comprehensive Overview and Comparative Analysis on Deep Learning Models and adversarialbased deep transfer learning Figure 25 Learning process of transfer learning Instancesbased deep transfer learning involves selecting a subset of instances from the source domain and assigning appropriate weight values to these selected instances to supplement the training set in the target domain Algorithms such as TaskTrAdaBoost 192 and TrAdaBoostR2 193 are wellknown approaches based on this strategy Mappingbased deep transfer learning focuses on mapping instances from both the source and target domains into a new data space where instances from the two domains'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='data space where instances from the two domains exhibit similarity and are suitable for training a unified deep neural network Successful methods based on this approach include Extend MMD Maximum Mean Discrepancy 194 and MKMMD Multiple Kernel variant of MMD 195 Networkbased modelbased deep transfer learning involves reusing a segment of a pre trained network from the source domain including its architecture and connection parameters and applying it to a deep neural network in the target domain These modelbased approaches are highly effective for domain adaptation between source and target data by adjusting the network model making them the most widely adopted strategies in deep transfer learning DTL Remarkably these methods can even adapt target data that is significantly different from the source data 196 Networkbased modelbased approaches in deep transfer learning typically involve pre training freezing finetuning and adding new layers Pretrained models consist of layers from a deep learning network DL model that have been trained using source data Two key methods for training a model with target data are freezing and finetuning These methods involve using some or all layers of a predefined model When layers are frozen they retain fixed parametersweights from the pretrained model In contrast finetuning involves initializing parameters and weights with pretrained values instead of starting with random values either for the entire network or specific layers 196 A recent advancement in modelbased deep transfer learning is Progressive Neural Networks PNNs This strategy involves the freezing of a pretrained model and integrating new layers specifically for training on target data 197 The concept behind progressive learning is grounded in the idea that acquiring a new skill necessitates leveraging existing knowledge This mirrors the way humans learn new abilities For instance a child learns to run by employing all the skills acquired during crawling and walking PNN constructs a new model for each task it encounters 24 F M Shiri et al Each freshly generated model is interconnected with all others aiming to learn a new task by applying the knowledge accumulated from preceding models Adversarialbased methods focus on gathering transferable features from both the source and target data by leveraging logical relationships or rules acquired in the source domain Alternatively they may utilize techniques inspired by generative adversarial networks GANs 198 These deep transfer learning techniques have proven to be effective in overcoming the challenge of limited training data enabling knowledge transfer across domains and facilitating improved performance in various applications such as image classification 199 200 speech recognition 201 202 video analysis 203 204 signal processing 205 206 and other In transfer learning several popular pretrained deep learning models are frequently used including Xception 52 MobileNet 53 DenseNet 55 EfficientNet 57 NasNet'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='MobileNet 53 DenseNet 55 EfficientNet 57 NasNet 207 and among others These models are initially trained on largescale datasets like ImageNet and their learned weights are then transferred to a target domain The architectures of these networks reflect a broader trend in deep learning design transitioning from manually crafted by human experts to automatically optimized patterns This evolution focuses on striking a balance between model accuracy and computational complexity 208 8 Hybrid Deep Learning Models Hybrid deep learning architectures which integrate elements from various deep learning models demonstrate significant potential in enhancing performance By combining different fundamental generative or discriminative models the following three categories of hybrid deep learning models can be particularly effective for addressing realworld problems  Combination of various supervised models to extract more relevant and robust features such as CNNLSTM or CNNGRU By leveraging the strengths of different architectures these hybrid models effectively capture both spatial and temporal dependencies within the data Integrating various types of generative models such as combining Autoencoders AE with Generative Adversarial Networks GANs to harness their strengths and enhance performance across a range of tasks Integrating the capabilities of generative models with supervised models to leverage the strengths of both approaches can significantly enhance performance on various tasks This hybrid strategy improves feature learning data augmentation and model robustness Examples of such combinations include DBNMLP GANCNN AECNN and so on 9 Application of Deep Learning In recent years deep learning has demonstrated remarkable effectiveness across a wide range of applications tackling various challenges in fields including healthcare computer vision speech recognition natural language processing NLP elearning smart environments and more Fig 26 highlights several potential realworld application areas of deep learning Five useful categories have been established for these applications classification detection localization segmentation and regression 10 A concept called classification divides a collection of facts into classes Detection typically involves recognizing objects and their boundaries within images videos or other data types Localization refers to the process of identifying and determining the position of specific objects or features within an image or other types of data Segmentation involves dividing an image or dataset into distinct regions or segments with each segment representing a particular object or feature of interest Regression is used to model and analyze the relationships between a dependent variable and one or more independent variables It predicts continuous outcomes based on input features 25 A Comprehensive Overview and Comparative Analysis on Deep Learning Models Figure 26 Numerous possible domains for deep learning applications in the'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='domains for deep learning applications in the real world However each realworld application area has its own specific goals and requires particular tasks and deep learning techniques Table 1 provides a summary of various deep learning tasks and methods applied across multiple realworld application domains Table 1 A summary of the practical applications of deep learning models in realworld domains Reference Application Setting Tasks Smart Homes  209 Smart Cities 210 211 212 213 214 82 Models CNNLSTM Reinforcement learning GRU based CNN based Stacked GRULSTM Human Activity Recognition Smart Energy Management Traffic Management Waste Management Smart Parking System Student Engagement Detection DenseNet selfattention Student Affective States Recognition Automatic Attendance System Automated Exam Control Medical Image Analysis Early Disease Detection Education ConvNeXt  GRU CNNLSTM CNN based VGG Vision transformer InceptionV3 Healthcare 215 216 217 218 26 Remote Patient Monitoring Analyze Genomic Data Question Answering Systems Sentiment Analysis Text Summarization CNN based Transfer learning based BERT based Transformer based Attentional LSTM LSTMCNN Deep transfer learning ViT CNN GRUCNN Autoencoders GAN CNN CNNRNN Attention GRU LSTM based Extended ViT CNN based Transformer based CNN BiLSTM LSTM based LSTMCNN Deep Autoencoder CNNLSTM RNN based Natural Language Processing NLP Speech Recognition Speech Emotion Recognition Automatic Speech Translation Plant Disease Detection Precision Agriculture Smart Irrigation System Soil Quality Prediction Earthquake Prediction Flood Forecasting Tsunami Prediction Land Cover Classification Investigation Wildfire Area Deforestation Detection Intrusion Detection Malware Detection Phishing Detection Credit Card Fraud Detection Biometric Authentication ContextAware Recommendation Sequential Recommendation Multimodal Recommendation Purchase Behavior Prediction Loan Default Prediction Stock Trend Prediction Object Detection Pedestrian Detection Localization And Mapping Lane Detection  Path Planning CNN based Defect Detection Predictive Maintenance Process Optimization Supply Chain Optimization Robotic Grasping Agriculture Natural Disaster Management Remote Sensing Cybersecurity Recommender Systems LSTM based CNN based RNN based CNN based BiLSTM Swin transformer CNN Deep CNN CNNGRU Business Autonomous Vehicles Manufacturing Transformer based LSTM GRU CNN Reinforcement learning LSTM Reinforcement learning Robotics Tracking And Motion Planning Reinforcement learning HumanRobot Interaction RNN based F M Shiri et al 219 220 221 222 223 224 201 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 27 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 10 Deep Learning Challenges While deep learning models have achieved remarkable success across various domains they also come with significant challenges Below are some of the most'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='significant challenges Below are some of the most critical challenges followed by potential solutions to address them 101 Insufficient Data Deep learning models require large amounts of data to perform well The performance of these models typically improves as the volume of data increases However in many cases sufficient data may not be available making it difficult to train deep learning models effectively 10 Three possible approaches may be used to appropriately handle the insufficient data problem The first method is Transfer Learning TL which is used to DL models by reusing pretrained model pieces in new models We thoroughly reviewed the transfer learning strategy in section 7 Data augmentation is the second method of gathering additional data The goal of data augmentation is to improve the trained models capacity for generalization Generalization is necessary for networks to overcome small datasets or datasets with unequal class distributions and it is especially crucial for realworld data 257 There are several strategies for augmenting data and each one is contingent upon the characteristics of the datasets 258 A few of these techniques are geometric transformations 259 Mixup augmentation 260 Random oversampling 261 Feature space augmentation 262 generative data augmentation 263 and many more The third approach considers using simulated data to increase the training sets volume If you have a good understanding of the physical process you can sometimes build simulators from it Consequently the outcome will include simulating as much data as necessary 10 264 102 Imbalanced Data In realworld situations particularly in those that deep learning models address the issue of class imbalance is common If the majority of instances in the data set belong to one class and the remaining instances belong to the other class then there is a class imbalance in a binary classification scenario In multiclass multilabel multiinstance learning as well as in regression difficulties and other situations class imbalances are present and are actually reinforced 265 It has been determined that there are three main approaches to addressing imbalanced data datalevel techniques algorithmlevel techniques and hybrid techniques The focus of datalevel techniques is to add or remove samples from training sets in order to balance the data distributions These techniques balance the data distributions by adding new samples to the minority class oversampling or removing samples from the majority class undersampling 266 267 A variety of oversampling techniques including Synthetic Minority Oversampling Technique SMOTE 268 BorderlineSMOTE 269 Adaptive Synthetic ADASYN 270 SVM Support Vector MachineSMOTE 271 Majority Weighted Minority Oversampling Technique MWMOTE 272 Sampling With the Majority SWIM 273 ReverseSMOTE RSMOTE 274 Constrained Oversampling CO 275 SMOTE Based on Furthest Neighbor Algorithm SOMTEFUNA 276 and many more can be used to solve imbalanced data problems Also'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='be used to solve imbalanced data problems Also there are several techniques for undersampling including EasyEnsemble 277 BalanceCascade 277 Inverse Random Undersampling 278 MLPbased Undersampling Technique MLPUS 279 and others Algorithmlevel approaches modify existing learning algorithms to mitigate the bias towards the majority class These techniques require specialized knowledge of both the application domain and the learning algorithm to diagnose why a classifier fails under imbalanced class distributions 266 Two of the most commonly used methods in this context are CostSensitive Learning 280 281 and OneClass Learning 282 28 F M Shiri et al The third approach consists of hybrid methods which combine algorithmlevel techniques with datalevel methods in the appropriate way Hybridization is required to address issues with algorithm and datalevel approaches and improve classification accuracy 283 103 Overfitting Overfitting occurs when a deep learning model learns the systematic and noise components of the training data to the point that it adversely affects the models performance on new data In fact overfitting occurs as a result of noise the small size of the training set and the complexity of the classifiers Overfitted models tend to memorize all the data including the inevitable noise in the training set rather than understanding the underlying patterns in the data 24 Overfitting is addressed with methods including dropout 90 weight decay 284 batch normalization 285 286 regularization 287 data augmentation and others although determining the ideal balance is still difficult 104 Vanishing and Exploding Gradient In deep neural networks the computation of gradients is propagated layer by layer leading to a phenomenon known as the vanishing or exploding gradient problem As gradients are backpropagated through the network they can exponentially diminish or grow respectively causing significant issues in training When gradients vanish the weights of the network are adjusted so minimally that the models learning process becomes exceedingly slow potentially stalling altogether Conversely exploding gradients can cause weights to be updated excessively leading to instability and divergence during training This problem is particularly pronounced with nonlinear activation functions such as sigmoid and tanh which compress the output into a narrow range further exacerbating the issue by limiting the gradients magnitude Consequently the model struggles to learn effectively especially in deep networks where gradients must pass through many layers 8 To mitigate the vanishing and exploding gradient problem several strategies have been developed One effective approach is to use the Rectified Linear Unit ReLU activation function which does not saturate and therefore helps to maintain the gradient flow throughout the network 288 Proper weight initialization techniques such as Xavier initialization 289 can also reduce the likelihood of gradient issues by ensuring'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='the likelihood of gradient issues by ensuring that initial weights are set in a way that prevents gradients from becoming too small or too large 290 Another solution is batch normalization which normalizes the inputs of each layer to maintain a stable distribution of activations throughout training By doing so batch normalization helps to alleviate the vanishing gradient problem and can accelerate convergence by reducing internal covariate shifts Overall addressing the vanishing and exploding gradient problem is crucial for training deep neural networks effectively enabling them to learn complex patterns without succumbing to instability or inefficiency 286 105 Catastrophic Forgetting Catastrophic forgetting is a critical challenge in the pursuit of artificial general intelligence within neural networks It occurs when a model after being trained on a new task loses its ability to perform previously learned tasks This phenomenon is particularly problematic in scenarios where a model is expected to learn sequentially across multiple tasks without forgetting earlier ones such as in lifelong learning or continual learning applications The root cause of catastrophic forgetting lies like neural networks which update their weights based on new training data When trained on a new task the model adjusts its parameters to optimize performance on that task often at the expense of previously acquired knowledge As a result the model may exhibit excellent performance on the most recent task but perform poorly on earlier ones effectively forgetting 29 A Comprehensive Overview and Comparative Analysis on Deep Learning Models them 291 Several strategies have been proposed to address catastrophic forgetting One such approach is Elastic Weight Consolidation EWC 292 which penalizes changes to the weights that are important for previous tasks thereby preserving learned knowledge while allowing the model to adapt to new tasks Incremental Moment Matching IMM  293 is another technique that merges models trained on different tasks into a single model balancing the performance across all tasks The iCaRL incremental Classifier and Representation Learning 294 method combines classification with representation learning enabling the model to learn new classes without forgetting previously learned ones Additionally the Hard Attention to the Task HAT 291 approach employs taskspecific masks that prevent interference between tasks reducing the likelihood of forgetting 106 Underspecifcation Underspecification is an emerging challenge in the deployment of machine learning ML models particularly deep learning DL models in realworld applications It refers to the phenomenon where an ML pipeline can produce a multitude of models that all perform well on the validation set but exhibit unpredictable behavior in deployment This issue arises because the pipelines design does not fully specify which model characteristics are critical for generalization in realworld scenarios The'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='for generalization in realworld scenarios The underspecification problem is often linked to the high degrees of freedom inherent in ML pipelines Factors such as random seed initialization hyperparameter selection and the stochastic nature of training can lead to the creation of models with similar validation performance but divergent behaviors in production These differences can manifest as inconsistent predictions when the model is exposed to new data or deployed in environments different from the training conditions 295 Addressing underspecification requires rigorous testing and validation beyond standard metrics Stress tests as proposed by DAmour et al 295 are designed to evaluate a models robustness under various realworld conditions identifying potential failure points that may not be apparent during standard validation These tests simulate different deployment scenarios such as varying input distributions or environmental changes to assess how the models predictions might vary Moreover some researches have been conducted to analyze and mitigate underspecification across different ML tasks 296 297 11 Analysis of Deep Learning Models This section details the methodology used in this study which focuses on applying and evaluating various deep learning models for classification tasks across three distinct datasets For our experimental analysis we utilized three publicly available datasets IMDB 298 ARAS 299 and Fruit360 300 The objective is to conduct a comparative analysis of the performance of these deep learning models The IMDB dataset which stands for Internet Movie Database provides a collection of movie reviews categorized as positive or negative sentiments ARAS is a dataset comprising annotated sensor events for human activity recognition tasks Fruit360 is a dataset consisting of images of various fruit types for classification purposes We began by evaluating eight different models CNN RNN LSTM Bidirectional LSTM GRU Bidirectional GRU TCN and Transformer on the IMDB and ARAS datasets Our analysis aimed to compare the performance of these deep learning models across diverse datasets The CNN model Convolutional Neural Network is particularly effective in capturing spatial dependencies making it suitable for tasks involving structured data RNN Recurrent Neural Network is well suited for sequential data analysis while LSTM Long ShortTerm Memory and GRU Gated Recurrent Unit models are designed to capture longterm dependencies in sequential data The 30 F M Shiri et al Bidirectional LSTM and Bidirectional GRU models provide an additional advantage by processing information in both forward and backward directions Additionally we evaluated eight different CNNbased models VGG Inception ResNet InceptionResNet Xception MobileNet DenseNet and NASNet for the classification of fruit images using the Fruit360 dataset Given that image data is not sequential or timedependent recurrent models were not suitable for this task CNNbased models are particularly'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='for this task CNNbased models are particularly effective for image analysis because of their ability to capture spatial dependencies Moreover the faster training time of CNN models is due to their parallel processing capabilities which allow for efficient computation on GPU Graphics Processing Unit thereby accelerating the training process To evaluate the performance of these models we employed assessment metrics such as accuracy precision recall and F1measure Accuracy measures the overall correctness of the models predictions while precision evaluates the proportion of correctly predicted positive instances Recall assesses the models ability to correctly identify positive instances and F1 measure provides a balanced measure of precision and recall 301             21           22   23 1    2        24 Where   True Positive   True Negative   False Positive and   False Negative By conducting a comprehensive analysis using these metrics we can gain insights into the strengths and weaknesses of each deep learning model This comparative evaluation enables us to identify the most effective model for specific datasets and applications ultimately advancing the field of deep learning and its practical applications All experiments were conducted on a GeForce RTX 3050 GPU Graphics Processing Unit with 4 Gigabyte of RAM Random Access Memory 111 Methodology and Experiments on IMDB Dataset The IMDB dataset is a widely used dataset for sentiment analysis tasks It consists of movie reviews along with their corresponding binary sentiment polarity labels The dataset contains a total of 50000 reviews evenly split into 25000 training samples and 25000 testing samples There is an equal distribution of positive and negative labels with 25000 instances of each sentiment To reduce the correlation between reviews for a given movie only 30 reviews are included in the dataset 298 Positive reviews often contain words like great well and love while negative reviews frequently use words like bad and cant However certain words such as one character and well appear frequently in both positive and negative reviews although their usage may differ in terms of frequency between the two sentiment classes 72 In our analysis we employed eight different deep learning models including CNN RNN LSTM Bidirectional LSTM GRU Bidirectional GRU TCN and Transformer for sentiment classification using the IMDB dataset Fig 27 presents a structural overview of the deep learning model intended for analyzing the performance of eight different models on the IMDB dataset 31 A Comprehensive Overview and Comparative Analysis on Deep Learning Models Figure 27 The structural for analysis of different deep learning models on IMDB dataset In this architecture text data is first passed through an embedding layer which transforms the highdimensional sparse input into dense lowerdimensional vectors of real numbers This allows the model to capture semantic relationships within the data In the second layer one'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='within the data In the second layer one of eight models CNN RNN LSTM BiLSTM GRU BiGRU TCN or Transformer is employed for feature extraction and data training This layer is crucial for capturing patterns and dependencies in the data Following this a dropout layer is included to address the issue of overfitting by randomly deactivating a portion of the neurons during training which helps improve the models generalization Subsequently the multidimensional vector turns into a onedimensional vector using a flatten layer enabling it to work with fully connected layers Finally the output is passed through a fully connected Dense layer which uses a Softmax function for classification converting the models predictions into probabilities for each class Building a neural network with high accuracy necessitates careful attention to hyperparameter selection as these adjustments significantly influence the networks performance For example setting the number of training iterations too high can lead to overfitting where the model performs well on the training data but poorly on unseen data Another critical hyperparameter is the learning rate which affects the rate of convergence during training If the learning rate is too high the network may converge too quickly potentially overshooting the global minimum of the loss function Conversely if the learning rate is too low the convergence process may become excessively slow prolonging training Therefore finding the optimal balance of hyperparameters is essential for maximizing the networks performance and ensuring effective learning In the experiment phase consistent parameters were applied across all models to ensure a standardized comparison The parameters were set as follows epochs  30 batch size  64 dropout  02 with the loss function set to Binary Crossentropy and the optimizer function set to Stochastic Gradient Descent SGD with a learning rate of 02 For the CNN model 100 filters were used with a kernel size of 3 along with the Rectified Linear Unit ReLU activation function The RNN LSTM BiLSTM GRU and BiGRU models each employed 64 units The TCN model was configured with 16 filters a kernel size of 5 and dilation rates of 1 2 4 8 The Transformer 32 F M Shiri et al model was set up with 2 attention heads a hidden layer size of 64 in the feedforward network and the ReLU activation function These parameter settings and architectural choices were designed to allow for a standardized comparison of the deep learning models on the IMDB dataset This standardization facilitates an accurate analysis of each models performance enabling a comparison of their accuracy and loss values Table 2 shows the result of different deep learning models on IMDB review dataset based on various metrics including Accuracy Precision Recall F1Score and Time of training Table 2 Result of different deep learning models on the IMDB dataset model Accuracy  Precision  Recall  F1Score  Time hms CNN 8590 8589 8588 8589 00257 RNN 5903 5903 5902'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='CNN 8590 8589 8588 8589 00257 RNN 5903 5903 5902 5903 01223 LSTM 8753 8753 8754 8754 00909 BiLSTM 8745 8746 8747 8746 01043 GRU 8755 8756 8757 8756 00510 BIGRU 8797 8792 8799 8795 00954 TCN 8442 8440 8442 8441 00738 Transformer 8803 8804 8801 8803 00344 To compare the performance of these models we utilized accuracy validationaccuracy loss and validationloss diagrams These diagrams provide insights into how well the models are learning from the data and help in evaluating their effectiveness for sentiment classification tasks Fig 28 shows the accuracy and validationaccuracy diagrams where the accuracy provides a visual representation of how the different deep learning models perform in terms of accuracy during the training process and validationaccuracy shows the trend of accuracy values on the testing set across multiple epochs for each model a Accuracy Diagram b ValidationAccuracy Diagram Figure 28 Accuracy and validationaccuracy of deep learning models on IMDB dataset 33 A Comprehensive Overview and Comparative Analysis on Deep Learning Models a Loss Diagram b ValidationLoss Diagram Figure 29 Loss and validation loss diagrams of deep learning models on IMDB dataset Fig 29 illustrates the loss and validationloss diagram where the loss diagram is a visual representation of loss values during the training process for six different models and the validation loss diagram depicts the variation in loss values on the testing set during the evaluation process for the different models The loss function measures the discrepancy between the predicted sentiment labels and the actual labels Furthermore the confusion matrices for the various deep learning models are displayed in Fig 30 These matrices provide a detailed breakdown of each models performance highlighting how well the models classify different classes By closely examining these confusion matrices we can gain insights into the precision of the models and identify patterns of misclassification for each class This analysis helps in understanding the strengths and weaknesses of the models predictions CNN RNN LSTM BiLSTM GRU BiGRU TCN Transformer Figure 30 Confusion matrix for different deep learning models on IMDB dataset 34 F M Shiri et al Figure 31 ROCAUC diagrams for different deep learning models Additionally Fig 31 displays the ROCAUC Receiver Operating CharacteristicArea Under Curve diagrams for eight different deep learning models These diagrams offer valuable insights into the classification performance of the models aiding in the assessment of their effectiveness By analyzing the ROCAUC curves we can make informed decisions regarding model selection and threshold adjustments ensuring a more accurate and effective classification approach Based on the results provided it can be concluded that the Transformer and BiGRU models achieved the best performance on the IMDB review dataset for sentiment analysis Both models demonstrated high accuracy in classifying the sentiment of movie reviews'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='in classifying the sentiment of movie reviews However it is worth noting that the training time of the Transformer model was significantly less than that of the Bi GRU model This suggests that the Transformer model was faster to train compared to the BiGRU model while still achieving excellent performance Additionally the GRU model also exhibited good accuracy in sentiment classification and required less training time than the BiGRU model Overall the results suggest that the Transformer and GRU models are effective deep learning models for sentiment analysis on the IMDB review dataset with varying tradeoffs between performance and training time 112 Methodology and Experiments on ARAS Dataset Based on the provided information the ARAS dataset 299 is a valuable resource for recognizing human activities in smart environments It consists of data streams collected from two houses over a period of 60 days with 20 binary sensors installed to monitor resident activity The dataset includes information on 27 different activities performed by two residents and the sensor events are recorded on a persecond basis Eight distinct deep learning models were used in our investigation to recognize human activities CNN RNN LSTM Bidirectional LSTM GRU Bidirectional GRU TCN and Transformer A structural overview of the deep learning model designed to analyze the performance of eight different models on the ARAS dataset is shown in Fig 32 The first phase involves preprocessing the sensor data to ensure it is in a suitable and standardized format for deep learning models The initial task in this phase is data cleaning where any recorded instances where all sensor events are zero and the resident is inside the house are 35 A Comprehensive Overview and Comparative Analysis on Deep Learning Models removed from the dataset Next a timebased static sliding window technique is applied for segmenting sensor events This method groups sequences of sensor events into intervals of equal duration Optimizing the time interval is crucial for effective segmentation after evaluating intervals ranging from 30 to 360 seconds a 90second interval was determined to be optimal for the ARAS dataset The segmentation task aids in decreasing training time and increasing accuracy for the deep learning models Figure 32 The structural for analysis of different deep learning models on the ARAS dataset After preprocessing the data is passed through an input layer In the second layer one of eight models CNN RNN LSTM BiLSTM GRU BiGRU TCN or Transformer is employed for feature extraction and training This layer plays a vital role in capturing patterns and dependencies within the data To mitigate overfitting a dropout layer follows which randomly deactivates a portion of the neurons during training thereby improving the models generalization Subsequently a flatten layer is used to convert the multidimensional vector into a onedimensional vector making it compatible with fully connected layers Finally the'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='with fully connected layers Finally the output passes through a fully connected Dense layer which uses a Softmax function for classification transforming the models predictions into probability distributions across the classes In the experimental phase we split the data from the first resident of house B allocating 70 for training and 30 for testing using a random split Additionally 20 of the training data was set aside for validation The models were trained with a fixed set of parameters 30 epochs a batch size of 64 a dropout rate of 02 the Categorical Crossentropy loss function and the Adam optimizer For the CNN model we used 100 filters with a kernel size of 3 and the rectified linear unit ReLU activation function The RNN LSTM BiLSTM GRU and BiGRU models were configured with 64 units each The TCN model was set with 16 filters a kernel size of 5 and dilation rates of 1 2 4 8 The Transformer model utilized 2 attention heads a hidden layer size of 64 in the feedforward network and the ReLU activation function Table 3 illustrates the results of experiments on ARAS dataset with various metrices including Accuracy Precision Recall F1Score and Time of training 36 F M Shiri et al Table 3 Result of different deep learning models on the ARAS dataset model Accuracy  Precision  Recall  F1Score  Time hms CNN 9314 9559 9243 9398 00118 RNN 9317 9619 9167 9388 00409 LSTM 9329 9556 9282 9381 00323 BiLSTM 9333 9666 9212 9415 00401 GRU 9365 9608 9178 9431 00315 BIGRU 9390 9587 9261 9449 00356 TCN 9404 9537 9348 9442 00406 Transformer 9456 9561 9406 9483 00314 Also Fig 33 presents the accuracy diagram and validationaccuracy diagram for the deep learning models while Fig 34 shows the loss diagram and validationloss diagram for deep learning models a Accuracy Diagram b ValidationAccuracy Diagram Figure 33 Accuracy and validation accuracy diagrams of deep learning models on ARAS dataset a Loss Diagram b ValidationLoss Diagram Figure 34 Loss and validation loss diagrams of deep learning models on ARAS dataset 37 A Comprehensive Overview and Comparative Analysis on Deep Learning Models Since we performed preprocessing tasks like data cleaning and segmentation the data is nearly normalized and balanced leading to consistent and closely grouped results across all models However the results indicate that the Transformer and TCN models outperformed the others on the ARAS dataset This outcome aligns with the datasets nature which comprises spatial and temporal sequences of sensor events Among the models the Transformer exhibited the highest performance in terms of accuracy recall and F1score while the BiLSTM model excelled in the precision metric Moreover the Transformer model demonstrated a notable advantage in training time second only to the CNN model underscoring its efficiency in processing and learning from timeseries data Additionally when examining the accuracy and loss curves it is evident that the Transformer TCN and CNN models stabilized earlier than the others'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='and CNN models stabilized earlier than the others Overall the Transformer model proved to be the most effective for working with the ARAS dataset striking a balance between accuracy training time and consistency throughout the training phases making it the optimal choice for recognizing human activities based on sensor data 113 Methodology and Experiments on the Fruit360 Dataset Since images are not sequential or timedependent recurrent models were less effective for these tasks CNNbased models on the other hand are highly valuable for image analysis due to their ability to capture spatial relationships Consequently the analysis of deep learning models on the Fruit360 dataset for image classification focused on eight CNN variants VGG Inception ResNet InceptionResNet Xception MobileNet DenseNet and NASNet These models use deep transfer learning technique for training image data and improving classification accuracy Fig 35 provides a structural overview of the deep learning models used to evaluate the performance of these eight variants on the Fruit360 dataset Figure 35 The structural for analysis of different CNNbased models on Fruit360 dataset 38 F M Shiri et al First the fruit images are passed through an input layer In the second layer one of eight models VGG Inception ResNet InceptionResNet Xception MobileNet DenseNet or NASNet is employed for feature extraction and training Next a Global Average Pooling 2D GAP layer is applied which significantly reduces the spatial dimensions of the data by collapsing each feature map into a single value To combat overfitting a dropout layer is then introduced randomly deactivating a portion of the neurons during training which enhances the models ability to generalize Finally the output is passed through a fully connected Dense layer where a Softmax function is used to classify the fruit images The dataset comprises 55244 images of 81 different fruit classes each with a resolution of 100  100 pixels For the experiments a subset of 60 fruit classes was selected containing 28484 images for training and 9558 images for testing Nonfruit items such as chestnuts and ginger root were removed from the dataset All models were trained with a consistent set of parameters 20 epochs a batch size of 512 a dropout rate of 02 the Categorical Crossentropy loss function and the Adam optimizer Additionally all models utilized the ImageNet dataset for pretraining Table 4 presents the experimental results for various models on the Fruit360 dataset including VGG16 InceptionResNetV2 Xception MobileNet DenseNet121 and NASNetLarge The table includes metrics such as Accuracy Precision Recall F1Score and Time of training InceptionV3 ResNet50 Table 4 Result of different deep learning models on the Fruit360 dataset model Accuracy  Precision  Recall  F1Score  Time hms VGG 9439 9979 8065 8920 21732 Inception 9586 9665 9514 9589 02334 ResNet 9459 9530 9364 9446 11256 InceptionResNet 9605 9701 9536 9618 05418 Xception 9738 9828 9661 9744'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='9701 9536 9618 05418 Xception 9738 9828 9661 9744 10111 MobileNet 9854 9888 9828 9858 01722 DenseNet 9894 9912 9875 9894 11030 NASNet 9699 9769 9656 9712 35005 Furthermore the accuracy validationaccuracy loss and validationloss diagrams were used to compare the performance of various models When assessing the models performance for tasks involving the categorization of fruit photos these graphs offer valuable insights into how effectively the models are learning from the data Fig 36 shows the accuracy and validation accuracy diagram of the deep learning models while Fig 37 illustrates the loss diagram and validationloss diagram of the deep learning models Based on the results it can be concluded that the DenseNet and MobileNet models achieved the best performance for fruit image classification on the Fruit360 dataset Both models demonstrated high accuracy in classifying fruit images Notably MobileNet had a significantly shorter training time compared to DenseNet indicating that it was faster to train while still delivering performance close to that of DenseNet Additionally the Xception model also showed good accuracy and required less training time than DenseNet Overall the MobileNet model stands out as a favorable choice due to its balance between accuracy and training efficiency 39 A Comprehensive Overview and Comparative Analysis on Deep Learning Models a Accuracy Diagram b ValidationAccuracy Diagram Figure 36 Accuracy and validation accuracy diagrams of different CNNbased deep learning models on Friut360 dataset a Loss Diagram b ValidationLoss Diagram Figure 37 Loss and validation loss diagrams of different CNNbased deep learning models on Friut360 dataset 12 Research Directions and Future Aspects In the preceding sections we explored a range of deep learning topics highlighting both the advantages and limitations of various deep learning models Additionally we examined the application of several models across different domains Despite the benefits demonstrated our research has identified certain gaps indicating that further advancements are necessary This section outlines potential future research directions based on our analysis  Generative Unsupervised Models Generative models a key category of deep learning models discussed in Section 4 hold significant promise for future research These models enable the creation of new data representations through exploratory analysis and can identify highorder correlations or features in data Unlike supervised learning unsupervised models can derive insights from data without the need for labeled examples making them valuable for various applications Several generative models including Autoencoders Generative Adversarial Networks GANs Deep Belief Networks DBNs and SelfOrganizing Maps SOMs have been developed and employed across diverse contexts A promising research avenue involves analyzing these models in various settings and developing new methods or variations that enhance data modeling or'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='or variations that enhance data modeling or representation for specific realworld applications The rising interest in GANs is 40 F M Shiri et al particularly noteworthy as they excel in leveraging unlabeled image data for deep representation learning and training highly nonlinear mappings between latent and data spaces The GAN framework offers flexibility to formulate new theories and methods tailored to emerging deep learning applications positioning it as a pivotal area for future exploration  HybridEnsemble Modeling Hybrid deep learning architectures have shown great potential in enhancing model performance by combining components from multiple models For instance the integration of Convolutional Neural Networks CNNs and Recurrent Neural Networks RNNs can capture both temporal and spatial dependencies in data leveraging the strengths of each model Hybrid models also benefit from combining generative and supervised learning offering superior performance and improved uncertainty handling in highrisk scenarios Developing effective hybrid models whether supervised or unsupervised presents a significant research opportunity to address a wide range of realworld problems including semisupervised learning tasks and model uncertainty This approach moves beyond conventional isolated models emphasizing the need for sophisticated methods that can handle the complexity of various data types and applications  Hyperparameter Optimization for Efficient Deep Learning As deep learning models have evolved the number of parameters computational latency and resource requirements have increased substantially 150 Selecting the appropriate hyperparameters is critical to building a neural network with high accuracy Key hyperparameters include learning rate loss function batch size number of training iterations and dropout rate among others The challenge lies in finding an optimal balance of these parameters as they significantly influence network performance However iterating through all possible combinations of hyperparameters is computationally expensive To address this metaheuristic optimization techniques such as Genetic Algorithm GA 302 Particle Swarm Optimization PSO 303 and others can be employed to explore the search space more efficiently than exhaustive methods Metaheuristic algorithms are a type of heuristic optimization algorithm that include mechanisms to avoid getting trapped in local optimization 304 Future research should focus on optimizing hyperparameters tailored to specific data types and contexts For example the learning rate plays a crucial role in training where a rate too high may cause the model to converge prematurely while a rate too low can lead to slow convergence and prolonged training times Adaptive learning rate techniques such as including Adaptive Moment Estimation Adam 305 Stochastic Gradient Descent SGD 306 adaptive gradient algorithm ADAGRAD 307 and Nesterovaccelerated Adaptive Moment Estimation Nadam 308 and more recent'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Moment Estimation Nadam 308 and more recent innovations like Evolved Sign Momentum Lion 309 offer promising avenues for improving network performance and minimizing loss functions Future research could further explore these optimizers focusing on their comparative effectiveness in enhancing model performance through iterative weight and bias adjustments  Federated Learning Federated learning is an emerging deep learning paradigm that enables collaborative model training across multiple organizations or teams without the need to share raw data This approach is particularly relevant in contexts where data privacy is paramount However federated learning introduces new challenges especially with the advent of data fusion technologies that combine data from multiple sources with varying formats As data diversity and volume continue to grow optimizing data and model utilization in federated learning becomes increasingly important Addressing challenges such as safeguarding user privacy developing universal models and ensuring the stability of data fusion outcomes will be crucial for the future application of federated learning across multiple domains 310  Quantum Deep Learning Quantum computing and deep learning have both seen significant advancements over the past few decades Quantum computing which leverages the principles of quantum mechanics to store and process information has the potential to outperform classical supercomputers on certain tasks making it a powerful tool for complex problemsolving The intersection of quantum computing and deep learning has led to the emergence of quantum deep learning and quantuminspired deep learning algorithms Future research directions in this area 41 A Comprehensive Overview and Comparative Analysis on Deep Learning Models include investigating and developing quantum deep learning models such as Quantum Convolutional Neural Network Quantum CNN 311 Quantum Recurrent Neural Network Quantum RNN 312 Quantum Generative Adversarial Network Quantum GAN 313 and others Additionally exploring the application of these models across various domains and creating novel quantum deep learning architectures represents a cuttingedge frontier in the field 314 315 In conclusion the research directions outlined above underscore the dynamic and evolving nature of deep learning By addressing these challenges and exploring new avenues the field can continue to advance driving innovation and enabling the development of more powerful and efficient models for a wide range of applications 13 Conclusion This article provides an extensive overview of deep learning technology and its applications in machine learning and artificial intelligence The article covers various aspects of deep learning including neural networks MLP models and different types of deep learning models such as CNN RNN TCN Transformer generative models DRL and transfer learning The classification of deep learning models allows for a better understanding of their'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='models allows for a better understanding of their specific applications and characteristics The RNN models including LSTM BiLSTM GRU and BiGRU are particularly suited for time series data due to their ability to capture temporal dependencies On the other hand CNNbased models excel in image data analysis by effectively capturing spatial features The experiments conducted on three public datasets namely IMDB ARAS and Fruit360 further reinforce the suitability of specific deep learning models for different data types The results demonstrate that the CNNbased models such as DenseNet and MobileNet perform exceptionally well in image classification tasks RNN models such as LSTM and GRU show strong performance in time series analysis However the Transformer model outperforms classical RNNbased models particularly in text analysis due to its use of the attention mechanism Overall this article highlights the diverse applications and effectiveness of deep learning models in various domains It emphasizes the importance of selecting the appropriate deep learning model based on the nature of the data and the task at hand The insights gained from the experiments contribute to a better understanding of the strengths and weaknesses of different deep learning models facilitating informed decisionmaking in practical applications References 1 P P Shinde and S Shah A review of machine learning and deep learning applications in 4th Int Conf Comput Commun Ctrl Autom ICCUBEA Pune India 1618 Aug 2018 IEEE pp 16 doi 101109ICCUBEA20188697857 2 C Janiesch P Zschech and K Heinrich Machine learning and deep learning Electron Mark vol 31 no 3 pp 685695 2021 doi 101007s12525021004752 3 W Han et al A survey of machine learning and deep learning in remote sensing of geological environment Challenges advances and opportunities ISPRS J Photogramm Remote Sens vol 202 pp 87113 2023 doi 101016jcogr202304001 4 S Zhang et al Deep Learning in Human Activity Recognition with Wearable Sensors A Review on Advances Sens vol 22 no 4 Feb 14 2022 doi 103390s22041476 5 S Li Y Tao E Tang T Xie and R Chen A survey of field programmable gate array FPGAbased graph convolutional neural network accelerators challenges and opportunities PeerJ Computer Science vol 8 pp e1166 2022 42 F M Shiri et al 6 A Mathew P Amudha and S Sivakumari Deep learning techniques an overview in Adv Mach Learn Technol App AMLTA 2020 2021 pp 599608 7 J Liu and Y Jin A comprehensive survey of robust deep learning in computer vision J Autom Intell  2023 doi 101016jjai202310002 8 A Shrestha and A Mahmood Review of deep learning algorithms and architectures IEEE access vol 7 pp 5304053065 2019 doi 101109ACCESS20192912200 9 M A Wani F A Bhat S Afzal and A I Khan Advances in deep learning Springer 2020 10 L Alzubaidi et al Review of deep learning Concepts CNN architectures challenges applications future directions J Big Data vol 8 pp 174 2021 doi 101186s40537021004448 11 I H Sarker Deep learning a comprehensive overview on'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Sarker Deep learning a comprehensive overview on techniques taxonomy applications and research directions SN Comput Sci vol 2 no 6 pp 420 2021 doi 101007s42979021008151 12 M N Hasan T Ahmed M Ashik M J Hasan T Azmin and J Uddin An Analysis of Covid19 Pandemic Outbreak on Economy using Neural Network and Random Forest J Inf Syst Telecommun JIST vol 2 no 42 pp 163 2023 doi 1052547jist342461142163 13 N B Gaikwad V Tiwari A Keskar and N Shivaprakash Efficient FPGA implementation of multilayer perceptron for realtime human activity classification IEEE Access vol 7 pp 26696 26706 2019 doi 101109ACCESS20192900084 14 KC Ke and MS Huang Quality prediction for injection molding by using a multilayer perceptron neural network Polym vol 12 no 8 pp 1812 2020 doi 103390polym12081812 15 A Tasdelen and B Sen A hybrid CNNLSTM model for premiRNA classification Sci Rep vol 11 no 1 pp 19 2021 doi 101038s41598021936560 16 L Qin N Yu and D Zhao Applying the convolutional neural network deep learning technology to behavioural recognition in intelligent video Tehniki vjesnik vol 25 no 2 pp 528535 2018 doi 1017559TV20171229024444 17 Z Li F Liu W Yang S Peng and J Zhou A Survey of Convolutional Neural Networks Analysis Applications and Prospects IEEE Trans Neural Netw Learn Syst vol 33 no 12 pp 69997019 Dec 2022 doi 101109TNNLS20213084827 18 B P Babu and S J Narayanan OnevsAll Convolutional Neural Networks for Synthetic Aperture Radar Target Recognition Cybern Inf Technol vol 22 pp 179197 2022 doi 102478cait2022 0035 19 S Mekruksavanich and A Jitpattanakul Deep convolutional neural network with rnns for complex activity recognition using wristworn wearable sensor data Electro vol 10 no 14 pp 1685 2021 doi 103390electronics10141685 20 W Lu J Li J Wang and L Qin A CNNBiLSTMAM method for stock price prediction Neural Comput Appl vol 33 pp 47414753 2021 doi 101007s0052102005532z 21 W Rawat and Z Wang Deep convolutional neural networks for image classification A comprehensive review Neural Comput vol 29 no 9 pp 23522449 2017 doi 101162NECOa00990 22 L Chen S Li Q Bai J Yang S Jiang and Y Miao Review of image classification algorithms based on convolutional neural networks Remote Sens vol 13 no 22 pp 4712 2021 doi 103390rs13224712 43 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 23 J Gu et al Recent advances in convolutional neural networks Pattern Recognit vol 77 pp 354 377 2018 doi 101016jpatcog201710013 24 S Salman and X Liu Overfitting mechanism and avoidance in deep neural networks arXiv preprint arXiv190106566 2019 25 A Ajit K Acharya and A Samanta A review of convolutional neural networks in 2020 Int Conf Emerg Tren Inf Technol Engr icETITE 2020 IEEE pp 15 26 W Liu Z Wang X Liu N Zeng Y Liu and F E Alsaadi A survey of deep neural network architectures and their applications Neurocomputing vol 234 pp 1126 2017 doi 101016jneucom201612038 27 K He X Zhang S Ren and J Sun Spatial pyramid pooling in deep convolutional networks for visual recognition'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='convolutional networks for visual recognition IEEE Trans Pattern Anal Mach Intell vol 37 no 9 pp 19041916 2015 doi 101109TPAMI20152389824 28 D Yu H Wang P Chen and Z Wei Mixed pooling for convolutional neural networks in Rough Sets Knwl Technol 9th Int Conf RSKT Shanghai China October 2426 2014 Springer pp 364 375 doi 101007978331911740934 29 Y Gong L Wang R Guo and S Lazebnik Multiscale orderless pooling of deep convolutional activation features in Comput Vis ECCV 13th Europ Conf Zurich Switzerland September 612 2014 Springer pp 392407 30 M D Zeiler and R Fergus Stochastic pooling for regularization of deep convolutional neural networks arXiv preprint arXiv13013557 2013 31 V Dumoulin and F Visin A guide to convolution arithmetic for deep learning arXiv preprint arXiv160307285 2016 32 M Krichen Convolutional neural networks A survey Comput  vol 12 no 8 pp 151 2023 doi 103390computers12080151 33 S Klarslan K Adem and M elik An overview of the activation functions used in deep learning algorithms J New Results Sci vol 10 no 3 pp 7588 2021 doi 1054187jnrs1011739 34 C Nwankpa W Ijomah A Gachagan and S Marshall Activation functions Comparison of trends in practice and research for deep learning arXiv preprint arXiv181103378 2018 35 K Hara D Saito and H Shouno Analysis of function of rectified linear unit used in deep learning in Int Jt Conf Neural Netw IJCNN Killarney Ireland 2015 pp 18 36 A L Maas A Y Hannun and A Y Ng Rectifier nonlinearities improve neural network acoustic models in Proc Int Conf Mach Learn ICML 2013 vol 30 no 1 Atlanta GA USA pp 3 37 K He X Zhang S Ren and J Sun Delving deep into rectifiers Surpassing humanlevel performance on imagenet classification in Proc IEEE Int Conf Comput Vis 2015 pp 10261034 38 B Xu N Wang T Chen and M Li Empirical evaluation of rectified activations in convolutional network arXiv preprint arXiv150500853 2015 39 X Jin C Xu J Feng Y Wei J Xiong and S Yan Deep learning with sshaped rectified linear activation units in Proc AAAI Conf Artif Intell 2016 vol 30 no 1 doi 101609aaaiv30i110287 40 DA Clevert T Unterthiner and S Hochreiter Fast and accurate deep network learning by exponential linear units elus arXiv preprint arXiv151107289 2015 44 F M Shiri et al 41 D Hendrycks and K Gimpel Gaussian error linear units gelus arXiv preprint arXiv160608415 2016 42 A Krizhevsky I Sutskever and G E Hinton Imagenet classification with deep convolutional neural networks Adv Neural Inf Process Syst vol 25 2012 43 K Simonyan and A Zisserman Very deep convolutional networks for largescale image recognition arXiv preprint arXiv14091556 2014 44 C Szegedy et al Going deeper with convolutions in Proc IEEE Conf Comput Vis Pattern Recognit 2015 pp 19 45 C Szegedy V Vanhoucke S Ioffe J Shlens and Z Wojna Rethinking the inception architecture for computer vision in Proc IEEE Conf Comput Vis Pattern Recognit 2016 pp 28182826 46 K He X Zhang S Ren and J Sun Deep residual learning for image recognition in Proc IEEECVF Conf Comput Vis'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='image recognition in Proc IEEECVF Conf Comput Vis Pattern Recognit 2016 pp 770778 47 K He X Zhang S Ren and J Sun Identity mappings in deep residual networks in Comput Vis ECCV 14th Europ Conf Amsterdam The Netherlands October 1114 2016 Springer pp 630645 48 S Zagoruyko and N Komodakis Wide residual networks arXiv preprint arXiv160507146 2016 49 G Larsson M Maire and G Shakhnarovich Fractalnet Ultradeep neural networks without residuals arXiv preprint arXiv160507648 2016 50 F N Iandola S Han M W Moskewicz K Ashraf W J Dally and K Keutzer SqueezeNet AlexNetlevel accuracy with 50x fewer parameters and 05 MB model size arXiv preprint arXiv160207360 2016 51 C Szegedy S Ioffe V Vanhoucke and A Alemi Inceptionv4 inceptionresnet and the impact of residual connections on learning in Proc AAAI Conf Artif Intell 2017 vol 31 no 1 doi 101609aaaiv31i111231 52 F Chollet Xception Deep learning with depthwise separable convolutions in Proc IEEE Conf Comput Vis Pattern Recognit 2017 pp 12511258 53 A G Howard et al Mobilenets Efficient convolutional neural networks for mobile vision applications arXiv preprint arXiv170404861 2017 54 M Sandler A Howard M Zhu A Zhmoginov and LC Chen Mobilenetv2 Inverted residuals and linear bottlenecks in Proc IEEE Conf Comput Vis Pattern Recognit 2018 pp 45104520 55 G Huang Z Liu L Van Der Maaten and K Q Weinberger Densely connected convolutional networks in Proc IEEE Conf Comput Vis Pattern Recognit 2017 pp 47004708 56 J Hu L Shen and G Sun Squeezeandexcitation networks in Proc IEEE Conf Comput Vis Pattern Recognit 2018 pp 71327141 57 M Tan and Q Le Efficientnet Rethinking model scaling for convolutional neural networks in Int Conf Mach Learn 2019 PMLR pp 61056114 58 M Tan and Q Le Efficientnetv2 Smaller models and faster training in Int Conf Mach Learn 2021 PMLR pp 1009610106 59 S Abbaspour F Fotouhi A Sedaghatbaf H Fotouhi M Vahabi and M Linden A Comparative Analysis of Hybrid Deep Learning Models for Human Activity Recognition Sens vol 20 no 19 2020 doi 103390s20195707 45 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 60 W Fang Y Chen and Q Xue Survey on research of RNNbased spatiotemporal sequence prediction algorithms J Big Data vol 3 no 3 pp 97 2021 doi 1032604jbd2021016993 61 J Xiao and Z Zhou Research progress of RNN language model in 2020 IEEE Int Conf Artif Intell Comput App ICAICA Dalian China 2729 June 2020 IEEE pp 12851288 doi 101109ICAICA5012720209182390 62 J YueHei Ng M Hausknecht S Vijayanarasimhan O Vinyals R Monga and G Toderici Beyond short snippets Deep networks for video classification in Proc IEEECVF Conf Comput Vis Pattern Recognit 2015 pp 46944702 63 A Shewalkar D Nyavanandi and S A Ludwig Performance evaluation of deep neural networks applied to speech recognition RNN LSTM and GRU J Artif Intell Soft Comput Res vol 9 no 4 pp 235245 2019 doi 102478jaiscr20190006 64 H Apaydin H Feizi M T Sattari M S Colak S Shamshirband and KW Chau Comparative analysis of recurrent neural network'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Comparative analysis of recurrent neural network architectures for reservoir inflow forecasting Water vol 12 no 5 pp 1500 2020 doi 103390w12051500 65 S Hochreiter and J Schmidhuber Long shortterm memory Neural Comput vol 9 no 8 pp 1735 1780 1997 MITPress 66 A Graves M Liwicki S Fernndez R Bertolami H Bunke and J Schmidhuber A novel connectionist system for unconstrained handwriting recognition IEEE Trans Pattern Anal Mach Intell vol 31 no 5 pp 855868 2008 doi 101109TPAMI2008137 67 J Chung C Gulcehre K Cho and Y Bengio Empirical evaluation of gated recurrent neural networks on sequence modeling arXiv preprint arXiv14123555 2014 68 J Chen D Jiang and Y Zhang A hierarchical bidirectional GRU model with attention for EEGbased emotion classification IEEE Access vol 7 pp 118530118540 2019 101109ACCESS20192936817 69 M Fortunato C Blundell and O Vinyals Bayesian recurrent neural networks arXiv preprint arXiv170402798 2017 70 F Kratzert D Klotz C Brenner K Schulz and M Herrnegger Rainfallrunoff modelling using long shortterm memory LSTM networks Hydrol Earth Syst Sci vol 22 no 11 pp 60056022 2018 doi 105194hess2260052018 71 A Graves Generating sequences with recurrent neural networks arXiv preprint arXiv13080850 2013 72 S Minaee E Azimi and A Abdolrashidi Deepsentiment Sentiment analysis using ensemble of cnn and bilstm models arXiv preprint arXiv190404206 2019 73 D Gaur and S Kumar Dubey Development of Activity Recognition Model using LSTMRNN Deep Learning Algorithm J inf organ sci vol 46 no 2 pp 277291 2022 doi 1031341jios4621 74 X Zhu P Sobihani and H Guo Long shortterm memory over recursive structures in Int Conf Mach Learn 2015 PMLR pp 16041612 75 F Gu MH Chung M Chignell S Valaee B Zhou and X Liu A survey on deep learning for human activity recognition ACM Comput Surv vol 54 no 8 pp 134 2021 doi 1011453472290 doi 46 F M Shiri et al 76 T H Aldhyani and H Alkahtani A bidirectional long shortterm memory model algorithm for predicting COVID19 in gulf countries Life vol 11 no 11 pp 1118 2021 doi 103390life11111118 77 D Liciotti M Bernardini L Romeo and E Frontoni A sequential deep learning application for recognising human activities in smart homes Neurocomputing vol 396 pp 501513 2020 doi 101016jneucom201810104 78 A Dutta S Kumar and M Basu A gated recurrent unit approach to bitcoin price prediction J Risk Financial Manag vol 13 no 2 pp 23 2020 doi 103390jrfm13020023 79 A Gumaei M M Hassan A Alelaiwi and H Alsalman A Hybrid Deep Learning Model for Human Activity Recognition Using Multimodal Body Sensing Data IEEE Access vol 7 pp 9915299160 2019 doi 101109access20192927134 80 D Bahdanau K Cho and Y Bengio Neural machine translation by jointly learning to align and translate arXiv preprint arXiv14090473 2014 81 C Chai et al A Multifeature Fusion ShortTerm Traffic Flow Prediction Model Based on Deep Learnings J Adv Transp vol 2022 no 1 pp 1702766 2022 doi 10115520221702766 82 C Lea M D Flynn R Vidal A Reiter and G D Hager Temporal convolutional networks'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='and G D Hager Temporal convolutional networks for action segmentation and detection in Proc IEEE Conf Comput Vis Pattern Recognit 2017 pp 156165 83 S Bai J Z Kolter and V Koltun An empirical evaluation of generic convolutional and recurrent networks for sequence modeling arXiv preprint arXiv180301271 2018 84 Y He and J Zhao Temporal convolutional networks for anomaly detection in time series J Phys Conf Ser vol 1213 no 4 pp 042050 2019 doi 1010881742659612134042050 85 J Zhu L Su and Y Li Wind power forecasting based on new hybrid model with TCN residual modification Energy AI vol 10 pp 100199 2022 doi 101016jegyai2022100199 86 D Li F Jiang M Chen and T Qian Multistepahead wind speed forecasting based on a hybrid decomposition method and temporal convolutional networks Energy vol 238 pp 121981 2022 doi 103390en16093792 87 X Zhang F Dong G Chen and Z Dai Advance prediction of coastal groundwater levels with temporal convolutional and long shortterm memory networks Hydrol Earth Syst Sci vol 27 no 1 pp 8396 2023 doi 105194hess27832023 88 F Yu and V Koltun Multiscale context aggregation by dilated convolutions arXiv preprint arXiv151107122 2015 89 T Salimans and D P Kingma Weight normalization A simple reparameterization to accelerate training of deep neural networks Adv Neural Inf Process Syst vol 29 no 29 2016 90 N Srivastava G Hinton A Krizhevsky I Sutskever and R Salakhutdinov Dropout a simple way to prevent neural networks from overfitting The journal of machine learning research vol 15 no 1 pp 19291958 2014 91 Z Liu et al Kan Kolmogorovarnold networks arXiv preprint arXiv240419756 2024 92 J Braun and M Griebel On a constructive proof of Kolmogorovs superposition theorem Constr Approx vol 30 pp 653675 2009 doi 101007s0036500990542 93 A D Bodner A S Tepsich J N Spolski and S Pourteau Convolutional KolmogorovArnold Networks arXiv preprint arXiv240613155 2024 47 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 94 R Genet and H Inzirillo Tkan Temporal kolmogorovarnold networks arXiv preprint arXiv240507344 2024 95 K Pan X Zhang and L Chen Research on the Training and Application Methods of a Lightweight Agricultural DomainSpecific Large Language Model Supporting Mandarin Chinese and Uyghur Appl Sci vol 14 no 13 pp 5764 2024 doi 103390app14135764 96 R Genet and H Inzirillo A Temporal KolmogorovArnold Transformer for Time Series Forecasting arXiv preprint arXiv240602486 2024 97 K Xu L Chen and S Wang KolmogorovArnold Networks for Time Series Bridging Predictive Power and Interpretability arXiv preprint arXiv240602496 2024 98 A A Aghaei fKAN Fractional KolmogorovArnold Networks with trainable Jacobi basis functions arXiv preprint arXiv240607456 2024 99 Z Bozorgasl and H Chen Wavkan Wavelet kolmogorovarnold networks arXiv preprint arXiv240512832 2024 100 F Zhang and X Zhang GraphKAN Enhancing Feature Extraction with Graph Kolmogorov Arnold Networks arXiv preprint arXiv240613597 2024 101 A Jabbar X Li and B Omar A survey on'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='2024 101 A Jabbar X Li and B Omar A survey on generative adversarial networks Variants applications and training ACM Comput Surv vol 54 no 8 pp 149 2021 doi 1011453463475 102 D Bank N Koenigstein and R Giryes Autoencoders in Machine learning for data science handbookdata mining and knowledge discovery handbook Springer 2023 pp 353374 103 I Goodfellow et al Generative adversarial nets Adv Neural Inf Process Syst vol 27 pp 26722680 2014 104 N Zhang S Ding J Zhang and Y Xue An overview on restricted Boltzmann machines Neurocomputing vol 275 pp 11861199 2018 doi 101016jneucom201709065 105 G E Hinton Deep belief networks Scholarpedia vol 4 no 5 pp 5947 2009 doi 104249scholarpedia5947 106 I Goodfellow Y Bengio and A Courville Deep learning MIT press 2016 107 J Zhai S Zhang J Chen and Q He Autoencoder and its various variants in 2018 IEEE Int Conf Syst Man Cybern SMC Miyazaki Japan 710 Oct 2018 IEEE pp 415419 doi 101109SMC201800080 108 A Makhzani J Shlens N Jaitly I Goodfellow and B Frey Adversarial autoencoders arXiv preprint arXiv151105644 2015 109 Y Wang H Yao and S Zhao Autoencoder based dimensionality reduction NEUROCOMPUTING vol 184 pp 232242 2016 doi 101016jneucom201508104 110 Y N Kunang S Nurmaini D Stiawan and A Zarkasi Automatic features extraction using autoencoder in intrusion detection system in 2018 Int Conf Electr engr Compu Sci ICECOS Pangkal Indonesia 24 Oct 2018 IEEE pp 219224 doi 101109ICECOS20188605181 111 C Zhou and R C Paffenroth Anomaly detection with robust deep autoencoders in Proc 23rd ACM SIGKDD Int Conf Knwl Discov Data Mining 2017 pp 665674 doi 10114530979833098052 112 A Creswell and A A Bharath Denoising adversarial autoencoders IEEE Trans Neural Netw Learn Syst vol 30 no 4 pp 968984 2018 doi 101109TNNLS20182852738 48 F M Shiri et al 113 D P Kingma and M Welling Autoencoding variational bayes arXiv preprint arXiv13126114 2013 114 A Ng Sparse autoencoder CS294A Lecture notes vol 72 no 2011 pp 119 2011 115 S Rifai et al Higher order contractive autoencoder in Mach Learn Knwl Discov DB Europ Conf ECML PKDD Athens Greece September 59 2011 Springer pp 645660 116 P Vincent H Larochelle Y Bengio and PA Manzagol Extracting and composing robust features with denoising autoencoders in Proc 25th Int Conf Mach Learn 2008 pp 10961103 doi 10114513901561390294 117 D P Kingma and M Welling An introduction to variational autoencoders Foundations and Trends in Machine Learning vol 12 no 4 pp 307392 2019 118 MY Liu and O Tuzel Coupled generative adversarial networks Adv Neural Inf Process Syst vol 29 2016 119 C Wang C Xu X Yao and D Tao Evolutionary generative adversarial networks IEEE Trans Evol Comput vol 23 no 6 pp 921934 2019 doi 101109TEVC20192895748 120 A Aggarwal M Mittal and G Battineni Generative adversarial network An overview of theory and applications Int J Inf Manag Data Insights vol 1 no 1 pp 100004 2021 doi101016jjjimei2020100004 121 BC Chen and A Kae Toward realistic image compositing with adversarial learning in Proc IEEECVF'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='with adversarial learning in Proc IEEECVF Conf Comput Vis Pattern Recognit 2019 pp 84158424 122 D P Jaiswal S Kumar and Y Badr Towards an artificial intelligence aided design approach application to anime faces with generative adversarial networks Procedia Comput Sci vol 168 pp 5764 2020 doi 101016jprocs202002257 123 Y Liu Q Li and Z Sun Attributeaware face aging with waveletbased generative adversarial networks in Proc IEEECVF Conf Comput Vis Pattern Recognit 2019 pp 1187711886 124 J Islam and Y Zhang GANbased synthetic brain PET image generation Brain Inform vol 7 pp 112 2020 doi 101186s40708020001042 125 H Lan A D N Initiative A W Toga and F Sepehrband SCGAN 3D selfattention conditional GAN with spectral normalization for multimodal neuroimaging synthesis BioRxiv pp 202006 09143297 2020 doi 10110120200609143297 126 K A Zhang A CuestaInfante L Xu and K Veeramachaneni SteganoGAN High capacity image steganography with GANs arXiv preprint arXiv190103892 2019 127 S Nam Y Kim and S J Kim Textadaptive generative adversarial networks manipulating images with natural language Adv Neural Inf Process Syst vol 31 2018 128 L Sixt B Wild and T Landgraf Rendergan Generating realistic labeled data Front Robot AI  vol 5 pp 66 2018 doi 103389frobt201800066 129 K Lin D Li X He Z Zhang and MT Sun Adversarial ranking for language generation Adv Neural Inf Process Syst vol 30 2017 130 D Xu C Wei P Peng Q Xuan and H Guo GEGAN A novel deep learning framework for road traffic state estimation Transp Res Part C Emerg vol 117 pp 102635 2020 doi 101016jtrc2020102635 49 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 131 A Clark J Donahue and K Simonyan Adversarial video generation on complex datasets arXiv preprint arXiv190706571 2019 132 E L Denton S Chintala and R Fergus Deep generative image models using a laplacian pyramid of adversarial networks Adv Neural Inf Process Syst vol 28 2015 133 C Li and M Wand Precomputed realtime texture synthesis with markovian generative adversarial networks in Comput Vis ECCV 14th Europ Conf Amsterdam Netherlands October 1114 2016 Springer pp 702716 doi 101007978331946487943 134 L Metz B Poole D Pfau and J SohlDickstein Unrolled generative adversarial networks arXiv preprint arXiv161102163 2016 135 M Arjovsky S Chintala and L Bottou Wasserstein generative adversarial networks in Int Conf Mach Learn 2017 PMLR pp 214223 136 D Berthelot T Schumm and L Metz Began Boundary equilibrium generative adversarial networks arXiv preprint arXiv170310717 2017 137 JY Zhu T Park P Isola and A A Efros Unpaired imagetoimage translation using cycle consistent adversarial networks in Proc IEEE Int Conf Comput Vis 2017 pp 22232232 138 T Kim M Cha H Kim J K Lee and J Kim Learning to discover crossdomain relations with generative adversarial networks in Int Conf Mach Learn 2017 PMLR pp 18571865 139 A JolicoeurMartineau The relativistic discriminator a key element missing from standard GAN arXiv preprint arXiv180700734 2018 140 T'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='GAN arXiv preprint arXiv180700734 2018 140 T Karras S Laine and T Aila A stylebased generator architecture for generative adversarial networks in Proc IEEECVF Conf Comput Vis Pattern Recognit 2019 pp 44014410 141 G Zhao M E Meyerand and R M Birn Bayesian conditional GAN for MRI brain image synthesis arXiv preprint arXiv200511875 2020 142 K Chen D Zhang L Yao B Guo Z Yu and Y Liu Deep learning for sensorbased human activity recognition Overview challenges and opportunities ACM Comput Surv vol 54 no 4 pp 140 2021 doi 1011453447744 143 N Alqahtani et al Deep belief networks DBN with IoTbased alzheimers disease detection and classification Appl Sci vol 13 no 13 pp 7833 2023 doi 103390app13137833 144 A P Kale R M Wahul A D Patange R Soman and W Ostachowicz Development of Deep belief network for tool faults recognition Sens vol 23 no 4 pp 1872 2023 doi 103390s23041872 145 E Sansano R Montoliu and O Belmonte Fernandez A study of deep neural networks for human activity recognition Comput Intell vol 36 no 3 pp 11131139 2020 doi 101111coin12318 146 A Vaswani et al Attention is all you need Advances in neural information processing systems vol 30 2017 147 J L Ba J R Kiros and G E Hinton Layer normalization arXiv preprint arXiv160706450 2016 148 K Gavrilyuk R Sanford M Javan and C G Snoek Actortransformers for group activity recognition in Proc IEEECVF Conf Comput Vis Pattern Recognit 2020 pp 839848 149 Y Tay M Dehghani D Bahri and D Metzler Efficient transformers A survey ACM Comput Surv vol 55 no 6 pp 128 2022 doi 1024963ijcai2023764 50 F M Shiri et al 150 G Menghani Efficient deep learning A survey on making deep learning models smaller faster and better ACM Comput Surv vol 55 no 12 pp 137 2023 doi 1011453578938 151 Y Liu and L Wu Intrusion Detection Model Based on Improved Transformer Applied Sciences vol 13 no 10 pp 6251 2023 152 D Chen S Yongchareon E M K Lai J Yu Q Z Sheng and Y Li Transformer With Bidirectional GRU for Nonintrusive SensorBased Activity Recognition in a Multiresident Environment IEEE Internet Things J vol 9 no 23 pp 2371623727 2022 doi 101109jiot20223190307 153 J Devlin MW Chang K Lee and K Toutanova Bert Pretraining of deep bidirectional transformers for language understanding arXiv preprint arXiv181004805 2018 154 A Radford K Narasimhan T Salimans and I Sutskever Improving language understanding by generative pretraining 2018 155 A Radford J Wu R Child D Luan D Amodei and I Sutskever Language models are unsupervised multitask learners OpenAI blog vol 1 no 8 pp 9 2019 156 Z Dai Z Yang Y Yang J Carbonell Q V Le and R Salakhutdinov Transformerxl Attentive language models beyond a fixedlength context arXiv preprint arXiv190102860 2019 157 Z Yang Z Dai Y Yang J Carbonell R R Salakhutdinov and Q V Le Xlnet Generalized autoregressive pretraining for language understanding Adv Neural Inf Process Syst vol 32 2019 158 N Shazeer Fast transformer decoding One writehead is all you need arXiv preprint arXiv191102150 2019 159 YH H Tsai S Bai P P'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='arXiv191102150 2019 159 YH H Tsai S Bai P P Liang J Z Kolter LP Morency and R Salakhutdinov Multimodal transformer for unaligned multimodal language sequences in Proc Conf Assoc Comput Linguist Mtg 2019 vol 2019 NIH Public Access p 6558 160 A Dosovitskiy et al An image is worth 16x16 words Transformers for image recognition at scale arXiv preprint arXiv201011929 2020 161 W Wang et al Pyramid vision transformer A versatile backbone for dense prediction without convolutions in Proc IEEECVF Conf Comput Vis Pattern Recognit 2021 pp 568578 162 Z Liu et al Swin transformer Hierarchical vision transformer using shifted windows in Proc IEEECVF Int Conf Comput Vis  2021 pp 1001210022 163 L Yuan et al Tokenstotoken vit Training vision transformers from scratch on imagenet in Proc IEEECVF Int Conf Comput Vis  2021 pp 558567 164 K Han A Xiao E Wu J Guo C Xu and Y Wang Transformer in transformer Adv Neural Inf Process Syst vol 34 pp 1590815919 2021 165 K Han J Guo Y Tang and Y Wang Pyramidtnt Improved transformerintransformer baselines with pyramid architecture arXiv preprint arXiv220100978 2022 166 W Fedus B Zoph and N Shazeer Switch transformers Scaling to trillion parameter models with simple and efficient sparsity J Mach Learn Res  vol 23 no 120 pp 139 2022 167 Z Liu H Mao CY Wu C Feichtenhofer T Darrell and S Xie A convnet for the 2020s in Proceedings of the IEEECVF conference on computer vision and pattern recognition 2022 pp 11976 11986 51 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 168 J Zhang et al Eatformer Improving vision transformer inspired by evolutionary algorithm Int J Comput Vis pp 128 2024 doi 101007s11263024020346 169 N Vithayathil Varghese and Q H Mahmoud A survey of multitask deep reinforcement learning Electron vol 9 no 9 pp 1363 2020 doi 103390electronics9091363 170 N Le V S Rathour K Yamazaki K Luu and M Savvides Deep reinforcement learning in computer vision a comprehensive survey Artif Intell Rev pp 187 2022 doi 101007s10462021 100619 171 M L Puterman Markov decision processes discrete stochastic dynamic programming John Wiley  Sons 2014 172 Z Zhang D Zhang and R C Qiu Deep reinforcement learning for power system applications An overview CSEE J Power Energy Syst vol 6 no 1 pp 213225 2019 doi 1017775CSEEJPES201900920 173 S E Li Deep reinforcement learning in Reinforcement learning for sequential decision and optimal control Springer 2023 pp 365402 174 V Mnih et al Humanlevel control through deep reinforcement learning NATURE vol 518 no 7540 pp 529533 2015 doi 101038nature14236 175 H Van Hasselt A Guez and D Silver Deep reinforcement learning with double qlearning in Proc AAAI Conf Artif Intell 2016 vol 30 no 1 doi 101609aaaiv30i110295 176 Z Wang T Schaul M Hessel H Hasselt M Lanctot and N Freitas Dueling network architectures for deep reinforcement learning in Int Conf Mach Learn 2016 PMLR pp 1995 2003 177 R Coulom Efficient selectivity and backup operators in MonteCarlo tree search in Comput Gam 5th'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='in MonteCarlo tree search in Comput Gam 5th Int Conf Turin Italy 2007 Springer pp 7283 178 N Justesen P Bontrager J Togelius and S Risi Deep learning for video game playing IEEE Trans Games vol 12 no 1 pp 120 2019 doi 101109TG20192896986 179 K Souchleris G K Sidiropoulos and G A Papakostas Reinforcement learning in game industryReview prospects and challenges Appl Sci vol 13 no 4 pp 2443 2023 doi 103390app13042443 180 S Gu E Holly T Lillicrap and S Levine Deep reinforcement learning for robotic manipulation with asynchronous offpolicy updates in 2017 IEEE Int Conf robot autom ICRA 2017 IEEE pp 33893396 181 D Han B Mulyana V Stankovic and S Cheng A survey on deep reinforcement learning algorithms for robotic manipulation Sens vol 23 no 7 pp 3762 2023 doi 103390s23073762 182 K M Lee H Myeong and G Song SeedNet Automatic Seed Generation with Deep Reinforcement Learning for Robust Interactive Segmentation in IEEECVF Conf Comput Vis Pattern Recognit CVPR Salt Lake City UT USA 1823 June 2018 IEEE Computer Society pp 17601768 doi 101109cvpr201800189 183 H Allioui et al A multiagent deep reinforcement learning approach for enhancement of COVID19 CT image segmentation J Pers Med vol 12 no 2 pp 309 2022 doi 103390jpm12020309 52 F M Shiri et al 184 F Sahba Deep reinforcement learning for object segmentation in video sequences in 2016 Int Conf Comput Sci Comput Intell CSCI Las Vegas NV USA 1517 Dec 2016 IEEE pp 857860 doi 101109CSCI20160166 185 H Liu et al Learning to identify critical states for reinforcement learning from videos in Proc IEEECVF Conf Comput Vis Pattern Recognit 2023 pp 19551965 186 A Shojaeighadikolaei A Ghasemi A G Bardas R Ahmadi and M Hashemi WeatherAware DataDriven Microgrid Energy Management Using Deep Reinforcement Learning in 2021 North American Power Symp NAPS College Station TX USA 1416 Nov 2021 IEEE pp 16 doi 101109NAPS5273220219654550 187 B Zhang W Hu A M Ghias X Xu and Z Chen Multiagent deep reinforcement learning based distributed control architecture for interconnected multienergy microgrid energy management and optimization Energy Conv Manag vol 277 pp 116647 2023 doi 101016jenconman2022116647 188 M Long H Zhu J Wang and M I Jordan Deep transfer learning with joint adaptation networks in Int Conf Mach Learn 2017 PMLR pp 22082217 189 C Tan F Sun T Kong W Zhang C Yang and C Liu A survey on deep transfer learning in Artif Neural NET Mach Learn ICANN 2018 27th Int Conf Artif Neural NET Rhodes Greece October 47 2018 Springer pp 270279 doi 101007978303001424727 190 F Zhuang et al A comprehensive survey on transfer learning P IEEE vol 109 no 1 pp 43 76 2020 191 M K Rusia and D K Singh A ColorTextureBased Deep Neural Network Technique to Detect Face Spoofing Attacks Cybern Inf Technol vol 22 no 3 pp 127145 2022 doi 102478cait 20220032 192 Y Yao and G Doretto Boosting for transfer learning with multiple sources in 2010 IEEE Comput Conf Comput socy Vis Pattern Recognit San Francisco CA USA 1318 June 2010 IEEE pp 18551862 doi'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='CA USA 1318 June 2010 IEEE pp 18551862 doi 101109CVPR20105539857 193 D Pardoe and P Stone Boosting for regression transfer in Proc 27th Int Conf Mach Learn 2010 pp 863870 194 E Tzeng J Hoffman N Zhang K Saenko and T Darrell Deep domain confusion Maximizing for domain invariance arXiv preprint arXiv14123474 2014 195 M Long Y Cao J Wang and M Jordan Learning transferable features with deep adaptation networks in Int Conf Mach Learn 2015 PMLR pp 97105 196 M Iman H R Arabnia and K Rasheed A review of deep transfer learning and recent advancements Technol vol 11 no 2 pp 40 2023 doi 103390technologies11020040 197 A A Rusu et al Progressive neural networks arXiv preprint arXiv160604671 2016 198 Y Guo J Zhang B Sun and Y Wang Adversarial Deep Transfer Learning in Fault Diagnosis Progress Challenges and Future Prospects Sens vol 23 no 16 pp 7263 2023 doi 103390s23167263 199 Y Gulzar Fruit image classification model based on MobileNetV2 with deep transfer learning technique Sustain vol 15 no 3 pp 1906 2023 doi 103390su15031906 53 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 200 N Kumar M Gupta D Gupta and S Tiwari Novel deep transfer learning model for COVID 19 patient detection using Xray chest images J Ambient Intell Humaniz Comput vol 14 no 1 pp 469478 2023 doi 101007s12652021033066 201 H Kheddar Y Himeur S AlMaadeed A Amira and F Bensaali Deep transfer learning for automatic speech recognition Towards better generalization KnowlBased Syst vol 277 pp 110851 2023 doi 101016jknosys2023110851 202 L Yuan T Wang G Ferraro H Suominen and MA Rizoiu Transfer learning for hate speech detection in social media Journal of Computational Social Science vol 6 no 2 pp 10811101 2023 203 A Ray M H Kolekar R Balasubramanian and A Hafiane Transfer learning enhanced vision based human activity recognition A decadelong analysis Int J Inf Manag Data Insights  vol 3 no 1 pp 100142 2023 doi 101016jjjimei2022100142 204 T Kujani and V D Kumar Head movements for behavior recognition from real time video based on deep learning ConvNet transfer learning J Ambient Intell Humaniz Comput vol 14 no 6 pp 70477061 2023 doi 101007s12652021035582 205 A Maity A Pathak and G Saha Transfer learning based heart valve disease classification from Phonocardiogram signal Biomed Signal Process Control  vol 85 pp 104805 2023 doi 101016jbspc2023104805 206 K Rezaee S Savarkar X Yu and J Zhang A hybrid deep transfer learningbased approach for Parkinsons disease classification in surface electromyography signals Biomed Signal Process Control vol 71 pp 103161 2022 doi 101016jbspc2021103161 207 B Zoph V Vasudevan J Shlens and Q V Le Learning transferable architectures for scalable image recognition in Proc IEEECVF Conf Comput Vis Pattern Recognit 2018 pp 86978710 208 Y Zhang et al Deep learning in food category recognition Inf Fusion vol 98 pp 101859 2023 doi 101016jinffus2023101859 209 E Ramanujam and T Perumal MLMOHSM Multilabel Multioutput Hybrid Sequential Model for'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Multioutput Hybrid Sequential Model for multiresident smart home activity recognition J Ambient Intell Humaniz Comput vol 14 no 3 pp 23132325 2023 doi 101007s12652022044874 210 M Ren X Liu Z Yang J Zhang Y Guo and Y Jia A novel forecasting based scheduling method for household energy management system based on deep reinforcement learning Sustain Cities Soc vol 76 pp 103207 2022 doi 101016jscs2021103207 211 S M Abdullah et al Optimizing traffic flow in smart cities Soft GRUbased recurrent neural networks for enhanced congestion prediction using deep learning Sustain vol 15 no 7 pp 5949 2023 doi 103390su15075949 212 M I B Ahmed et al Deep learning approach to recyclable products classification Towards sustainable waste management Sustain vol 15 no 14 pp 11138 2023 doi 103390su151411138 213 C Zeng C Ma K Wang and Z Cui Parking occupancy prediction method based on multi factors and stacked GRULSTM IEEE Access vol 10 pp 4736147370 2022 doi 101109ACCESS20223171330 214 N K Mehta S S Prasad S Saurav R Saini and S Singh Threedimensional DenseNet self attention neural network for automatic detection of students engagement Appl Intell vol 52 no 12 pp 1380313823 2022 doi 101007s10489022032004 54 F M Shiri et al 215 A K Shukla A Shukla and R Singh Automatic attendance system based on CNNLSTM and face recognition Int J Inf Technol vol 16 no 3 pp 12931301 2024 doi 101007s41870023 014951 216 B Rajalakshmi V K Dandu S L Tallapalli and H Karanwal ACE Automated Exam Control and EProctoring System Using Deep Face Recognition in 2023 Int Conf Circuit Power Comput Technol ICCPCT Kollam India 1011 Aug 2023 IEEE pp 301306 doi 101109ICCPCT58313202310245126 217 I Pacal MaxCerVixT A novel lightweight vision transformerbased Approach for precise cervical cancer detection KnowlBased Syst  vol 289 pp 111482 2024 doi 101016jknosys2024111482 218 M M Rana et al A robust and clinically applicable deep learning model for early detection of Alzheimers IET Image Process vol 17 no 14 pp 39593975 2023 doi 101049ipr212910 219 S Vimal Y H Robinson S Kadry H V Long and Y Nam IoT based smart health monitoring with CNN using edge computing J Internet Technol vol 22 no 1 pp 173185 2021 doi 103966160792642021012201017 220 T S Johnson et al Diagnostic Evidence GAuge of Single cells DEGAS a flexible deep transfer learning framework for prioritizing cells in relation to disease Genome Med vol 14 no 1 pp 11 2022 doi 101186s13073022010122 221 W Zheng S Lu Z Cai R Wang L Wang and L Yin PALBERT an improved question answering model Comput Model engr Sci pp 110 2023 doi 1032604cmes2023046692 222 F Wang et al TEDT transformerbased encodingdecoding translation network for multimodal sentiment analysis Cogn Comput vol 15 no 1 pp 289303 2023 doi 101007s1255902210073 9 223 M Nafees Muneera and P Sriramya An enhanced optimized abstractive text summarization traditional approach employing multilayered attentional stacked LSTM with the attention RNN in Comput Vis Mach Intell Paradigm 2023 Springer pp 303318'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='Vis Mach Intell Paradigm 2023 Springer pp 303318 doi 101007978981197169 328 224 M A Uddin M S Uddin Chowdury M U Khandaker N Tamam and A Sulieman The Efficacy of Deep LearningBased Mixed Model for Speech Emotion Recognition Comput Mater Contin vol 74 no 1 2023 doi 1032604cmc2023031177 225 M De Silva and D Brown Multispectral Plant Disease Detection with Vision Transformer Convolutional Neural Network Hybrid Approaches Sens vol 23 no 20 pp 8531 2023 doi 103390s23208531 226 T Akilan and K Baalamurugan Automated weather forecasting and field monitoring using GRUCNN model along with IoT to support precision agriculture Expert Syst Appl vol 249 pp 123468 2024 doi 101016jeswa2024123468 227 R Benameur A Dahane B Kechar and A E H Benyamina An Innovative Smart and Sustainable LowCost Irrigation System for Anomaly Detection Using Deep Learning Sens vol 24 no 4 pp 1162 2024 doi 103390s24041162 55 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 228 M HosseinpourZarnaq M Omid F Sarmadian and H GhasemiMobtaker A CNN model for predicting soil properties using VISNIR spectral data Environ Earth Sci vol 82 no 16 pp 382 2023 doi 101007s12665023110730 229 M Shakeel K Itoyama K Nishida and K Nakadai Detecting earthquakes a novel deep learningbased approach for effective disaster response Appl Intell vol 51 no 11 pp 83058315 2021 doi 101007s10489021022857 230 Y Zhang Z Zhou J Van Griensven Th S X Yang and B Gharabaghi Flood Forecasting Using Hybrid LSTM and GRU Models with Lag Time Preprocessing Water vol 15 no 22 pp 3982 2023 doi 103390w15223982 231 H Xu and H Wu Accurate tsunami wave prediction using long shortterm memory based neural networks Ocean Model vol 186 pp 102259 2023 doi 101016jocemod2023102259 232 J Yao B Zhang C Li D Hong and J Chanussot Extended vision transformer ExViT for land use and land cover classification A multimodal deep learning framework IEEE Trans Geosci Remote Sens vol 61 pp 115 2023 doi 101109TGRS20233284671 233 A Y Cho Se Park Dj Kim J Kim C Li and J Song Burned area mapping using Unitemporal Planetscope imagery with a deep learning based approach IEEE J Sel Top Appl Earth Obs Remote Sens vol 16 pp 242253 2022 doi 101109JSTARS20223225070 234 M Alshehri A Ouadou and G J Scott Deep Transformerbased Network Deforestation Detection in the Brazilian Amazon Using Sentinel2 Imagery IEEE Geosci Remote Sens Lett 2024 doi 101109LGRS20243355104 235 V Hnamte and J Hussain DCNNBiLSTM An efficient hybrid deep learningbased intrusion detection system Telemat Inform Rep vol 10 pp 100053 2023 doi 101016jteler2023100053 236 E S Alomari et al Malware detection using deep learning and correlationbased feature selection Symmetry vol 15 no 1 pp 123 2023 doi 103390sym15010123 237 Z Alshingiti R Alaqel J AlMuhtadi Q E U Haq K Saleem and M H Faheem A deep learningbased phishing detection system using CNN LSTM and LSTMCNN Electron vol 12 no 1 pp 232 2023 doi 103390electronics12010232 238 H Fanai and H Abbasimehr A novel combined'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='238 H Fanai and H Abbasimehr A novel combined approach based on deep Autoencoder and deep classifiers for credit card fraud detection Expert Syst Appl vol 217 pp 119562 2023 doi 101016jeswa2023119562 239 R A Joshi and N Sambre Personalized CNN Architecture for Advanced MultiModal Biometric Authentication in 2024 Int Conf Invent Comput Technol ICICT Lalitpur Nepal 2426 April 2024 IEEE pp 890894 doi 101109ICICT60155202410544987 240 J SohafiBonab M H Aghdam and K Majidzadeh DCARS Deep contextaware recommendation system based on session latent context Appl Soft Comput vol 143 pp 110416 2023 doi 101016jasoc2023110416 241 J Duan PF Zhang R Qiu and Z Huang Long shortterm enhanced memory for sequential recommendation World Wide Web vol 26 no 2 pp 561583 2023 doi 101007s11280022 010569 56 F M Shiri et al 242 P Mondal D Chakder S Raj S Saha and N Onoe Graph convolutional neural network for multimodal movie recommendation in Proc 38th ACMSIGAPP Symp Appl Comput 2023 pp 16331640 doi 10114535557763577853 243 Z Liu Prediction Model of Ecommerce Users Purchase Behavior Based on Deep Learning Front Bus Econ Manag vol 15 no 2 pp 147149 2024 doi 1054097p22ags78 244 S Deng R Li Y Jin and H He CNNbased feature cross and classifier for loan default prediction in 2020 Int Conf Image video Process Artif Intell 2020 vol 11584 SPIE pp 368 373 245 C Han and X Fu Challenge and opportunity deep learningbased stock price prediction by using Bidirectional LSTM model Front Bus Econ Manag vol 8 no 2 pp 5154 2023 doi 1054097fbemv8i26616 246 Y Cao C Li Y Peng and H Ru MCSYOLO A multiscale object detection method for autonomous driving road environment recognition IEEE Access vol 11 pp 2234222354 2023 doi 101109ACCESS20233252021 247 D K Jain X Zhao G GonzlezAlmagro C Gan and K Kotecha Multimodal pedestrian detection using metaheuristics with deep convolutional neural network in crowded scenes Inf Fusion vol 95 pp 401414 2023 doi 101016jinffus202302014 248 S Sindhu and M Saravanan An optimised extreme learning machine OELM for simultaneous localisation and mapping in autonomous vehicles Int J Syst Syst Eng vol 13 no 2 pp 140159 2023 doi 101504IJSSE2023131231 249 G Singal H Singhal R Kushwaha V Veeramsetty T Badal and S Lamba RoadWay lane detection for autonomous driving vehicles via deep learning Multimed Tools Appl vol 82 no 4 pp 49654978 2023 doi 101007s11042022121710 250 H Shang C Sun J Liu X Chen and R Yan Defectaware transformer network for intelligent visual surface defect detection Adv Eng Inform vol 55 pp 101882 2023 doi 101016jaei2023101882 251 T Zonta C A Da Costa F A Zeiser G de Oliveira Ramos R Kunst and R da Rosa Righi A predictive maintenance model for optimizing production schedule using deep neural networks J Manuf Syst vol 62 pp 450462 2022 doi 101016jjmsy202112013 252 Z He KP Tran S Thomassey X Zeng J Xu and C Yi A deep reinforcement learning based multicriteria decision support system for optimizing textile chemical process Comput Ind vol 125 pp 103373 2021 doi'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='process Comput Ind vol 125 pp 103373 2021 doi 101016jcompind2020103373 253 M Pacella and G Papadia Evaluation of deep learning with long shortterm memory networks for time series forecasting in supply chain management PROC CIRP vol 99 pp 604609 2021 doi 101016jprocir202103081 254 P Shukla H Kumar and G C Nandi Robotic grasp manipulation using evolutionary computing and deep reinforcement learning Intell Serv Robot vol 14 no 1 pp 6177 2021 doi 101007s11370020003427 255 K Kamali I A Bonev and C Desrosiers Realtime motion planning for robotic teleoperation using dynamicgoal deep reinforcement learning in 2020 17th Conf Comput Robot Vis CRV 13 15 May 2020 IEEE pp 182189 doi 101109CRV50864202000032 57 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 256 J Zhang H Liu Q Chang L Wang and R X Gao Recurrent neural network for motion trajectory prediction in humanrobot collaborative assembly CIRP annals vol 69 no 1 pp 912 2020 doi 101016jcirp202004077 257 B K Iwana and S Uchida An empirical survey of data augmentation for time series classification with neural networks PLOS ONE vol 16 no 7 pp e0254841 2021 doi 101371journalpone0254841 258 C Khosla and B S Saini Enhancing performance of deep learning models with different data augmentation techniques A survey in 2020 Int Conf Intell engr Mgmt ICIEM London UK 17 19 June 2020 IEEE pp 7985 doi 101109ICIEM4876220209160048 259 M Paschali W Simson A G Roy R Gbl C Wachinger and N Navab Manifold exploring data augmentation with geometric transformations for increased performance and robustness in Inf Process Medical Image 26th Int Conf IPMI 2019 Hong Kong China June 27 2019 Springer pp 517529 260 H Guo Y Mao and R Zhang Augmenting data with mixup for sentence classification An empirical study arXiv preprint arXiv190508941 2019 261 O O AbayomiAlli R Damaeviius A Qazi M AdedoyinOlowe and S Misra Data augmentation and deep learning methods in sound classification A systematic review Electro vol 11 no 22 pp 3795 2022 doi 103390electronics11223795 262 TH Cheung and DY Yeung Modals Modalityagnostic automated data augmentation in the latent space in Int Conf Learn Represen 2020 263 C Shorten T M Khoshgoftaar and B Furht Text data augmentation for deep learning J Big Data vol 8 no 1 pp 101 2021 doi 101186s40537021004920 264 F Wang H Wang H Wang G Li and G Situ Learning from simulation An endtoend deep learning approach for computational ghost imaging Opt Express vol 27 no 18 pp 2556025572 2019 doi 101364OE27025560 265 K Ghosh C Bellinger R Corizzo P Branco B Krawczyk and N Japkowicz The class imbalance problem in deep learning Mach Learn vol 113 no 7 pp 48454901 2024 doi 101007s10994022062688 266 D Singh E Merdivan J Kropf and A Holzinger Class imbalance in multiresident activity recognition an evaluative study on explainability of deep learning approaches Univers Access Inf Soc pp 119 2024 doi 101007s10209024011230 267 A S Tarawneh A B Hassanat G A Altarawneh and A Almuhaimeed Stop oversampling'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='A Altarawneh and A Almuhaimeed Stop oversampling for class imbalance learning A review IEEE ACCESS vol 10 pp 4764347660 2022 doi 101109ACCESS20223169512 268 N V Chawla K W Bowyer L O Hall and W P Kegelmeyer SMOTE synthetic minority oversampling technique J Artif Intell Res vol 16 pp 321357 2002 doi 101613jair953 269 H Han WY Wang and BH Mao BorderlineSMOTE a new oversampling method in imbalanced data sets learning in Int Conf Intell Comput 2005 Springer pp 878887 270 H He Y Bai E A Garcia and S Li ADASYN Adaptive synthetic sampling approach for imbalanced learning in 2008 Int Jt Conf Neural Netw 2008 IEEE pp 13221328 58 F M Shiri et al 271 Y Tang YQ Zhang N V Chawla and S Krasser SVMs modeling for highly imbalanced classification IEEE Trans Syst Man Cybern Part B Cybernetics vol 39 no 1 pp 281288 2008 doi 101109TSMCB20082002909 272 S Barua M M Islam X Yao and K Murase MWMOTEmajority weighted minority oversampling technique for imbalanced data set learning IEEE Trans Knowl Data Eng vol 26 no 2 pp 405425 2012 doi 102478cait20220035 273 C Bellinger S Sharma N Japkowicz and O R Zaane Framework for extreme imbalance classification SWIMsampling with the majority class Knowl Inf Syst vol 62 pp 841866 2020 doi 101007s1011501901380z 274 R Das S K Biswas D Devi and B Sarma An oversampling technique by integrating reverse nearest neighbor in SMOTE ReverseSMOTE in 2020 Int Conf Smart Electron Commun ICOSEC 2020 IEEE pp 12391244 275 C Liu et al Constrained oversampling An oversampling approach to reduce noise generation in imbalanced datasets with class overlapping IEEE ACCESS vol 10 pp 9145291465 2020 doi 101109ACCESS20203018911 276 A S Tarawneh A B Hassanat K Almohammadi D Chetverikov and C Bellinger Smotefuna Synthetic minority oversampling technique based on furthest neighbour algorithm IEEE ACCESS vol 8 pp 5906959082 2020 doi 101109ACCESS20202983003 277 XY Liu J Wu and ZH Zhou Exploratory undersampling for classimbalance learning IEEE Trans Syst Man Cybern Part B Cybernetics vol 39 no 2 pp 539550 2008 doi 101109TSMCB20082007853 278 M A Tahir J Kittler and F Yan Inverse random under sampling for class imbalance problem and its application to multilabel classification Pattern Recognit vol 45 no 10 pp 37383750 2012 doi 101016jpatcog201203014 279 V Babar and R Ade A novel approach for handling imbalanced data in medical diagnosis using undersampling technique Commun Appl Electron vol 5 no 7 pp 3642 2016 280 Z H Zhou and X Y Liu On multiclass costsensitive learning Comput Intell vol 26 no 3 pp 232257 2010 doi 101111j14678640201000358x 281 C X Ling and V S Sheng Costsensitive learning and the class imbalance problem ency Mach Learn vol 2011 pp 231235 2008 282 N Seliya A Abdollah Zadeh and T M Khoshgoftaar A literature review on oneclass classification and its potential applications in big data J Big Data vol 8 pp 131 2021 doi 101186s4053702100514x 283 V S Spelmen and R Porkodi A review on handling imbalanced data in Int Conf Curr Trend Toward Converg Technol'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='in Int Conf Curr Trend Toward Converg Technol ICCTCT Coimbatore India 13 March 2018 IEEE pp 111 doi 101109ICCTCT20188551020 284 G Zhang C Wang B Xu and R Grosse Three mechanisms of weight decay regularization arXiv preprint arXiv181012281 2018 285 C Laurent G Pereyra P Brakel Y Zhang and Y Bengio Batch normalized recurrent neural networks in 2016 IEEE Int Conf Acoust Speech Signal Process ICASSP Shanghai China 2025 March 2016 IEEE pp 26572661 doi 101109ICASSP20167472159 59 A Comprehensive Overview and Comparative Analysis on Deep Learning Models 286 S Ioffe and C Szegedy Batch normalization Accelerating deep network training by reducing internal covariate shift in Int Conf Mach Learn 2015 pmlr pp 448456 287 G Pereyra G Tucker J Chorowski  Kaiser and G Hinton Regularizing neural networks by penalizing confident output distributions arXiv preprint arXiv170106548 2017 288 G E Dahl T N Sainath and G E Hinton Improving deep neural networks for LVCSR using rectified linear units and dropout in IEEE Int Conf Acoust Speech Signal Process 2013 IEEE pp 86098613 289 X Glorot and Y Bengio Understanding the difficulty of training deep feedforward neural networks in Proc 13 Int Conf Artif Intell Stats 2010 JMLR Workshop and Conference Proceedings pp 249256 290 G Srivastava S Vashisth I Dhall and S Saraswat Behavior analysis of a deep feedforward neural network by varying the weight initialization methods in Smart Innov Commun Comput Sci Proc ICSICCS 2020 2021 Springer pp 167175 doi 101007978981155345515 291 J Serra D Suris M Miron and A Karatzoglou Overcoming catastrophic forgetting with hard attention to the task in Int Conf Mach Learn 2018 PMLR pp 45484557 292 J Kirkpatrick et al Overcoming catastrophic forgetting in neural networks Proc Natl Acad Sci vol 114 no 13 pp 35213526 2017 doi 101073pnas1611835114 293 SW Lee JH Kim J Jun JW Ha and BT Zhang Overcoming catastrophic forgetting by incremental moment matching Adv Neural Inf Process Syst vol 30 2017 294 SA Rebuffi A Kolesnikov G Sperl and C H Lampert icarl Incremental classifier and representation learning in Proc IEEEConf Comput Vis Pattern Recognit 2017 pp 20012010 295 A DAmour et al Underspecification presents challenges for credibility in modern machine learning J Mach Learn Res vol 23 no 226 pp 161 2022 296 D Teney M Peyrard and E Abbasnejad Predicting is not understanding Recognizing and addressing underspecification in machine learning in Europ Conf Comput Vis 2022 Springer pp 458476 297 N Chotisarn W Pimanmassuriya and S Gulyanon Deep learning visualization for underspecification analysis in product design matching model development IEEE ACCESS vol 9 pp 108049108061 2021 doi 101109ACCESS20213102174 298 A Maas R E Daly P T Pham D Huang A Y Ng and C Potts Learning word vectors for sentiment analysis in Proc 49th Annual Meeting Assoc Comput Linguist Hum langu Tech Portland Oregon June 19  2 2011 pp 142150 299 H Alemdar H Ertan O D Incel and C Ersoy ARAS human activity datasets in multiple homes with'),\n",
       " Document(metadata={'source': 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf'}, page_content='human activity datasets in multiple homes with multiple residents in 2013 7th Int Conf Perv Comput Technol Healthcare Workshop 2013 IEEE pp 232235 300 H Murean and M Oltean Fruit recognition from images using deep learning arXiv preprint arXiv171200580 2017 301 F M P Shiri T Perumal N Mustapha R Mohamed M A Ahmadon and S Yamaguchi A Survey on MultiResident Activity Recognition in Smart Environments Evolution of Information Communication and Computing System pp 1227 2023 60 F M Shiri et al 302 X Xiao M Yan S Basodi C Ji and Y Pan Efficient hyperparameter optimization in deep learning using a variable length genetic algorithm arXiv preprint arXiv200612703 2020 303 H J Escalante M Montes and L E Sucar Particle swarm model selection J Mach Learn Res vol 10 no 2 pp 405440 2009 304 M Parhizgar and F M Shiri Solving quadratic assignment problem using water cycle optimization algorithm International Journal of Intelligent Information Systems vol 3 no 61 pp 7579 2014 305 D P Kingma and J Ba Adam A method for stochastic optimization arXiv preprint arXiv14126980 2014 306 L Bottou Stochastic gradient descent tricks in Neural Networks Tricks of the Trade Second Edition Springer 2012 pp 421436 307 J Duchi E Hazan and Y Singer Adaptive subgradient methods for online learning and stochastic optimization J Mach Learn Res vol 12 no 7 pp 21212159 2011 308 T Dozat Incorporating nesterov momentum into adam in Proc 4th Int Conf Learn Represent ICLR Workshop Track San Juan Puerto Rico 2016 pp 14 309 X Chen et al Symbolic discovery of optimization algorithms Adv Neural Inf Process Syst vol 36 2024 310 L Alzubaidi et al A survey on deep learning tools dealing with data scarcity definitions challenges solutions tips and applications J Big Data vol 10 no 1 pp 46 2023 doi 101186s40537023007272 311 I Cong S Choi and M D Lukin Quantum convolutional neural networks Nat Phys vol 15 no 12 pp 12731278 2019 doi 101038s4156701906488 312 Y Takaki K Mitarai M Negoro K Fujii and M Kitagawa Learning temporal data with a variational quantum recurrent neural network Phys Rev A vol 103 no 5 pp 052414 2021 doi 101103PhysRevA103052414 313 S Lloyd and C Weedbrook Quantum generative adversarial learning Phys Rev Lett vol 121 no 4 pp 040502 2018 doi 101103PhysRevLett121040502 314 S Garg and G Ramakrishnan Advances in quantum deep learning An overview arXiv preprint arXiv200504316 2020 315 F Valdez and P Melin A review on quantum computing and deep learning algorithms and their applications Soft Comput vol 27 no 18 pp 1321713236 2023 doi 101007s0050002207037 4 61'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='4 1 0 2 c e D 4 1  L C  s c  3 v 5 1 2 3  9 0 4 1  v i X r a Sequence to Sequence Learning with Neural Networks Ilya Sutskever Google ilyasugooglecom Oriol Vinyals Google vinyalsgooglecom Quoc V Le Google qvlgooglecom Abstract Deep Neural Networks DNNs are powerful models that have achieved excel lent performance on difcult learning tasks Although DNNs work well whenever large labeled training sets are available they cannot be used to map sequences to sequences In this paper we present a general endtoend approach to sequence learning that makes minimal assumptions on the sequence structure Our method uses a multilayered Long ShortTerm Memory LSTM to map the input sequence to a vector of a xed dimensionality and then another deep LSTM to decode the target sequence from the vector Our main result is that on an English to French translation task from the WMT14 dataset the translations produced by the LSTM achieve a BLEU score of 348 on the entire test set where the LSTMs BLEU score was penalized on outofvocabulary words Additionally the LSTM did not have difculty on long sentences For comparison a phrasebased SMT system achieves a BLEU score of 333 on the same dataset When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system its BLEU score increases to 365 which is close to the previous best result on this task The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas sive voice Finally we found that reversing the order of the words in all source sentences but not target sentences improved the LSTMs performance markedly because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier 1 Introduction Deep Neural Networks DNNs are extremely powerful machine learning models that achieve ex cellent performance on difcult problems such as speech recognition 13 7 and visual object recog nition 19 6 21 20 DNNs are powerful because they can perform arbitrary parallel computation for a modest number of steps A surprising example of the power of DNNs is their ability to sort N N bit numbers using only 2 hidden layers of quadratic size 27 So while neural networks are related to conventional statistical models they learn an intricate computation Furthermore large DNNs can be trained with supervised backpropagation whenever the labeled training set has enough information to specify the networks parameters Thus if there exists a parameter setting of a large DNN that achieves good results for example because humans can solve the task very rapidly supervised backpropagation will nd these parameters and solve the problem Despite their exibility and power DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of xed dimensionality It is a signicant limitation since many important problems are best expressed with'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='many important problems are best expressed with sequences whose lengths are not known apriori For example speech recognition and machine translation are sequential problems Likewise ques tion answering can also be seen as mapping a sequence of words representing the question to a 1 sequence of words representing the answer It is therefore clear that a domainindependent method that learns to map sequences to sequences would be useful Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and outputs is known and xed In this paper we show that a straightforward application of the Long ShortTerm Memory LSTM architecture 16 can solve general sequence to sequence problems The idea is to use one LSTM to read the input sequence one timestep at a time to obtain large xed dimensional vector representation and then to use another LSTM to extract the output sequence from that vector g 1 The second LSTM is essentially a recurrent neural network language model 28 23 30 except that it is conditioned on the input sequence The LSTMs ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application due to the considerable time lag between the inputs and their corresponding outputs g 1 There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks Our approach is closely related to Kalchbrenner and Blunsom 18 who were the rst to map the entire input sentence to vector and is related to Cho et al 5 although the latter was used only for rescoring hypotheses produced by a phrasebased system Graves 10 introduced a novel differentiable attention mechanism that allows neural networks to focus on dif ferent parts of their input and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al 2 The Connectionist Sequence Classication is another popular technique for mapping sequences to sequences with neural networks but it assumes a monotonic alignment between the inputs and the outputs 11 Figure 1 Our model reads an input sentence ABC and produces WXYZ as the output sentence The model stops making predictions after outputting the endofsentence token Note that the LSTM reads the input sentence in reverse because doing so introduces many short term dependencies in the data that make the optimization problem much easier The main result of this work is the following On the WMT14 English to French translation task we obtained a BLEU score of 3481 by directly extracting translations from an ensemble of 5 deep LSTMs with 384M parameters and 8000 dimensional state each using a simple lefttoright beam search decoder This is by far the best result achieved by direct translation with large neural net works For comparison the BLEU score of an SMT baseline on this dataset is 3330 29 The 3481 BLEU score was achieved by an LSTM with a vocabulary of 80k words so the score was penalized'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='of 80k words so the score was penalized whenever the reference translation contained a word not covered by these 80k This result shows that a relatively unoptimized smallvocabulary neural network architecture which has much room for improvement outperforms a phrasebased SMT system Finally we used the LSTM to rescore the publicly available 1000best lists of the SMT baseline on the same task 29 By doing so we obtained a BLEU score of 365 which improves the baseline by 32 BLEU points and is close to the previous best published result on this task which is 370 9 Surprisingly the LSTM did not suffer on very long sentences despite the recent experience of other researchers with related architectures 26 We were able to do well on long sentences because we reversed the order of words in the source sentence but not the target sentences in the training and test set By doing so we introduced many short term dependencies that made the optimization problem much simpler see sec 2 and 33 As a result SGD could learn LSTMs that had no trouble with long sentences The simple trick of reversing the words in the source sentence is one of the key technical contributions of this work A useful property of the LSTM is that it learns to map an input sentence of variable length into a xeddimensional vector representation Given that translations tend to be paraphrases of the source sentences the translation objective encourages the LSTM to nd sentence representations that capture their meaning as sentences with similar meanings are close to each other while different 2 sentences meanings will be far A qualitative evaluation supports this claim showing that our model is aware of word order and is fairly invariant to the active and passive voice 2 The model The Recurrent Neural Network RNN 31 28 is a natural generalization of feedforward neural networks to sequences Given a sequence of inputs x1     xT  a standard RNN computes a sequence of outputs y1     yT  by iterating the following equation ht  sigm cid0W hxxt  W hhht1cid1 yt  W yhht The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time However it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and nonmonotonic relation ships The simplest strategy for general sequence learning is to map the input sequence to a xedsized vector using one RNN and then to map the vector to the target sequence with another RNN this approach has also been taken by Cho et al 5 While it could work in principle since the RNN is provided with all the relevant information it would be difcult to train the RNNs due to the resulting long term dependencies gure 1 14 4 16 15 However the Long ShortTerm Memory LSTM 16 is known to learn problems with long range temporal dependencies so an LSTM may succeed in this setting The goal of the LSTM is to estimate the conditional probability py1     yT x1     xT  where x1'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='probability py1     yT x1     xT  where x1     xT  is an input sequence and y1     yT  is its corresponding output sequence whose length T  may differ from T  The LSTM computes this conditional probability by rst obtaining the xed dimensional representation v of the input sequence x1     xT  given by the last hidden state of the LSTM and then computing the probability of y1     yT  with a standard LSTMLM formulation whose initial hidden state is set to the representation v of x1     xT  T  py1     yT  x1     xT   Y t1 pytv y1     yt1 In this equation each pytv y1     yt1 distribution is represented with a softmax over all the words in the vocabulary We use the LSTM formulation from Graves 10 Note that we require that each sentence ends with a special endofsentence symbol EOS which enables the model to dene a distribution over sequences of all possible lengths The overall scheme is outlined in gure 1 where the shown LSTM computes the representation of A B C EOS and then uses this representation to compute the probability of W X Y Z EOS Our actual models differ from the above description in three important ways First we used two different LSTMs one for the input sequence and another for the output sequence because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously 18 Second we found that deep LSTMs signicantly outperformed shallow LSTMs so we chose an LSTM with four layers Third we found it extremely valuable to reverse the order of the words of the input sentence So for example instead of mapping the sentence a b c to the sentence    the LSTM is asked to map c b a to    where    is the translation of a b c This way a is in close proximity to  b is fairly close to  and so on a fact that makes it easy for SGD to establish communication between the input and the output We found this simple data transformation to greatly improve the performance of the LSTM 3 Experiments We applied our method to the WMT14 English to French MT task in two ways We used it to directly translate the input sentence without using a reference SMT system and we it to rescore the nbest lists of an SMT baseline We report the accuracy of these translation methods present sample translations and visualize the resulting sentence representation 3 1 31 Dataset details We used the WMT14 English to French dataset We trained our models on a subset of 12M sen tences consisting of 348M French words and 304M English words which is a clean selected subset from 29 We chose this translation task and this specic training set subset because of the public availability of a tokenized training and test set together with 1000best lists from the baseline SMT 29 As typical neural language models rely on a vector representation for each word we used a xed vocabulary for both languages We used 160000 of the most frequent words for the source language and 80000 of the most frequent words for'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='language and 80000 of the most frequent words for the target language Every outofvocabulary word was replaced with a special UNK token 32 Decoding and Rescoring The core of our experiments involved training a large deep LSTM on many sentence pairs We trained it by maximizing the log probability of a correct translation T given the source sentence S so the training objective is 1S X log pT S TSS where S is the training set Once training is complete we produce translations by nding the most likely translation according to the LSTM T  arg max T pT S We search for the most likely translation using a simple lefttoright beam search decoder which maintains a small number B of partial hypotheses where a partial hypothesis is a prex of some translation At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary This greatly increases the number of the hypotheses so we discard all but the B most likely hypotheses according to the models log probability As soon as the EOS symbol is appended to a hypothesis it is removed from the beam and is added to the set of complete hypotheses While this decoder is approximate it is simple to implement Interestingly our system performs well even with a beam size of 1 and a beam of size 2 provides most of the benets of beam search Table 1 We also used the LSTM to rescore the 1000best lists produced by the baseline system 29 To rescore an nbest list we computed the log probability of every hypothesis with our LSTM and took an even average with their score and the LSTMs score 33 Reversing the Source Sentences While the LSTM is capable of solving problems with long term dependencies we discovered that the LSTM learns much better when the source sentences are reversed the target sentences are not reversed By doing so the LSTMs test perplexity dropped from 58 to 47 and the test BLEU scores of its decoded translations increased from 259 to 306 While we do not have a complete explanation to this phenomenon we believe that it is caused by the introduction of many short term dependencies to the dataset Normally when we concatenate a source sentence with a target sentence each word in the source sentence is far from its corresponding word in the target sentence As a result the problem has a large minimal time lag 17 By reversing the words in the source sentence the average distance between corresponding words in the source and target language is unchanged However the rst few words in the source language are now very close to the rst few words in the target language so the problems minimal time lag is greatly reduced Thus backpropagation has an easier time establishing communication between the source sentence and the target sentence which in turn results in substantially improved overall performance Initially we believed that reversing the input sentences would only lead to more condent predic tions in the early parts of the target sentence and to less condent predictions in the later'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='and to less condent predictions in the later parts How ever LSTMs trained on reversed source sentences did much better on long sentences than LSTMs 4 2 trained on the raw source sentences see sec 37 which suggests that reversing the input sentences results in LSTMs with better memory utilization 34 Training details We found that the LSTM models are fairly easy to train We used deep LSTMs with 4 layers with 1000 cells at each layer and 1000 dimensional word embeddings with an input vocabulary of 160000 and an output vocabulary of 80000 Thus the deep LSTM uses 8000 real numbers to represent a sentence We found deep LSTMs to signicantly outperform shallow LSTMs where each additional layer reduced perplexity by nearly 10 possibly due to their much larger hidden state We used a naive softmax over 80000 words at each output The resulting LSTM has 384M parameters of which 64M are pure recurrent connections 32M for the encoder LSTM and 32M for the decoder LSTM The complete training details are given below We initialized all of the LSTMs parameters with the uniform distribution between 008 and 008 We used stochastic gradient descent without momentum with a xed learning rate of 07 After 5 epochs we begun halving the learning rate every half epoch We trained our models for a total of 75 epochs We used batches of 128 sequences for the gradient and divided it the size of the batch namely 128 Although LSTMs tend to not suffer from the vanishing gradient problem they can have exploding gradients Thus we enforced a hard constraint on the norm of the gradient 10 25 by scaling it when its norm exceeded a threshold For each training batch we compute s  kgk2 where g is the gradient divided by 128 If s  5 we set g  5g s  Different sentences have different lengths Most sentences are short eg length 2030 but some sentences are long eg length  100 so a minibatch of 128 randomly chosen training sentences will have many short sentences and few long sentences and as a result much of the computation in the minibatch is wasted To address this problem we made sure that all sentences in a minibatch are roughly of the same length yielding a 2x speedup 35 Parallelization A C implementation of deep LSTM with the conguration from the previous section on a sin gle GPU processes a speed of approximately 1700 words per second This was too slow for our purposes so we parallelized our model using an 8GPU machine Each layer of the LSTM was executed on a different GPU and communicated its activations to the next GPU  layer as soon as they were computed Our models have 4 layers of LSTMs each of which resides on a separate GPU The remaining 4 GPUs were used to parallelize the softmax so each GPU was responsible for multiplying by a 1000  20000 matrix The resulting implementation achieved a speed of 6300 both English and French words per second with a minibatch size of 128 Training took about a ten days with this implementation 36 Experimental Results We used the cased BLEU score 24 to'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='Results We used the cased BLEU score 24 to evaluate the quality of our translations We computed our BLEU scores using multibleupl1 on the tokenized predictions and ground truth This way of evaluating the BELU score is consistent with 5 and 2 and reproduces the 333 score of 29 However if we evaluate the best WMT14 system 9 whose predictions can be downloaded from statmtorgmatrix in this manner we get 370 which is greater than the 358 reported by statmtorgmatrix The results are presented in tables 1 and 2 Our best results are obtained with an ensemble of LSTMs that differ in their random initializations and in the random order of minibatches While the decoded translations of the LSTM ensemble do not outperform the best WMT14 system it is the rst time that a pure neural translation system outperforms a phrasebased SMT baseline on a large scale MT 1There several variants of the BLEU score and each variant is dened with a perl script 5 Method Bahdanau et al 2 Baseline System 29 Single forward LSTM beam size 12 Single reversed LSTM beam size 12 Ensemble of 5 reversed LSTMs beam size 1 Ensemble of 2 reversed LSTMs beam size 12 Ensemble of 5 reversed LSTMs beam size 2 Ensemble of 5 reversed LSTMs beam size 12 test BLEU score ntst14 2845 3330 2617 3059 3300 3327 3450 3481 Table 1 The performance of the LSTM on WMT14 English to French test set ntst14 Note that an ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of size 12 Method Baseline System 29 Cho et al 5 Best WMT14 result 9 Rescoring the baseline 1000best with a single forward LSTM Rescoring the baseline 1000best with a single reversed LSTM Rescoring the baseline 1000best with an ensemble of 5 reversed LSTMs Oracle Rescoring of the Baseline 1000best lists test BLEU score ntst14 3330 3454 370 3561 3585 365 45 Table 2 Methods that use neural networks together with an SMT system on the WMT14 English to French test set ntst14 task by a sizeable margin despite its inability to handle outofvocabulary words The LSTM is within 05 BLEU points of the best WMT14 result if it is used to rescore the 1000best list of the baseline system 37 Performance on long sentences We were surprised to discover that the LSTM did well on long sentences which is shown quantita tively in gure 3 Table 3 presents several examples of long sentences and their translations 38 Model Analysis 4 15 I was given a card by her in the garden 3 Mary admires John 10 In the garden  she gave me a card She gave me a card in the garden 2 Mary is in love with John 5 1 0 Mary respects John 0 1 John admires Mary 2 John is in love with Mary 5 She was given a card by me in the garden In the garden  I gave her a card 3 10 4 5 John respects Mary 15 I gave her a card in the garden 6 20 8 6 4 2 0 2 4 6 8 10 15 10 5 0 5 10 15 20 Figure 2 The gure shows a 2dimensional PCA projection of the LSTM hidden states that are obtained after processing the phrases in the gures The phrases are clustered by meaning which in these'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='phrases are clustered by meaning which in these examples is primarily a function of word order which would be difcult to capture with a bagofwords model Notice that both clusters have similar internal structure One of the attractive features of our model is its ability to turn a sequence of words into a vector of xed dimensionality Figure 2 visualizes some of the learned representations The gure clearly shows that the representations are sensitive to the order of words while being fairly insensitive to the 6 Type Our model Ulrich UNK  membre du conseil d administration du constructeur automobile Audi  Sentence Truth Our model Truth afrme qu il s agit d une pratique courante depuis des annees pour que les telephones portables puissent etre collectes avant les reunions du conseil d administration an qu ils ne soient pas utilises comme appareils d ecoute a distance  Ulrich Hackenberg  membre du conseil d administration du constructeur automobile Audi  declare que la collecte des telephones portables avant les reunions du conseil  an qu ils ne puissent pas etre utilises comme appareils d ecoute a distance  est une pratique courante depuis des annees   Les telephones cellulaires  qui sont vraiment une question  non seulement parce qu ils pourraient potentiellement causer des interferences avec les appareils de navigation  mais nous savons  selon la FCC  qu ils pourraient interferer avec les tours de telephone cellulaire lorsqu ils sont dans l air   dit UNK   Les telephones portables sont veritablement un probleme  non seulement parce qu ils pourraient eventuellement creer des interferences avec les instruments de navigation  mais parce que nous savons  d apres la FCC  qu ils pourraient perturber les antennesrelais de telephonie mobile s ils sont utilises a bord   a declare Rosenker  Our model Avec la cremation  il y a un  sentiment de violence contre le corps d un etre cher   qui sera  reduit a une pile de cendres  en tres peu de temps au lieu d un processus de decomposition  qui accompagnera les etapes du deuil   Il y a  avec la cremation   une violence faite au corps aime   qui va etre  reduit a un tas de cendres  en tres peu de temps  et non apres un processus de decomposition  qui  accompagnerait les phases du deuil   Truth Table 3 A few examples of long translations produced by the LSTM alongside the ground truth translations The reader can verify that the translations are sensible using Google translate 40 LSTM 348 baseline 333 40 LSTM 348 baseline 333 35 35 e r o c s U E L B 30 e r o c s U E L B 30 25 25 20 4 7 8 12 17 22 28 35 79 20 0 500 1000 1500 2000 2500 3000 3500 test sentences sorted by their length test sentences sorted by average word frequency rank Figure 3 The left plot shows the performance of our system as a function of sentence length where the xaxis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths There is no degradation on sentences with less than 35 words there is only a'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='sentences with less than 35 words there is only a minor degradation on the longest sentences The right plot shows the LSTMs performance on sentences with progressively more rare words where the xaxis corresponds to the test sentences sorted by their average word frequency rank replacement of an active voice with a passive voice The twodimensional projections are obtained using PCA 4 Related work There is a large body of work on applications of neural networks to machine translation So far the simplest and most effective way of applying an RNNLanguage Model RNNLM 23 or a 7 Feedforward Neural Network Language Model NNLM 3 to an MT task is by rescoring the n best lists of a strong MT baseline 22 which reliably improves translation quality More recently researchers have begun to look into ways of including information about the source language into the NNLM Examples of this work include Auli et al 1 who combine an NNLM with a topic model of the input sentence which improves rescoring performance Devlin et al 8 followed a similar approach but they incorporated their NNLM into the decoder of an MT system and used the decoders alignment information to provide the NNLM with the most useful words in the input sentence Their approach was highly successful and it achieved large improvements over their baseline Our work is closely related to Kalchbrenner and Blunsom 18 who were the rst to map the input sentence into a vector and then back to a sentence although they map sentences to vectors using convolutional neural networks which lose the ordering of the words Similarly to this work Cho et al 5 used an LSTMlike RNN architecture to map sentences into vectors and back although their primary focus was on integrating their neural network into an SMT system Bahdanau et al 2 also attempted direct translations with a neural network that used an attention mechanism to overcome the poor performance on long sentences experienced by Cho et al 5 and achieved encouraging results Likewise PougetAbadie et al 26 attempted to address the memory problem of Cho et al 5 by translating pieces of the source sentence in way that produces smooth translations which is similar to a phrasebased approach We suspect that they could achieve similar improvements by simply training their networks on reversed source sentences Endtoend training is also the focus of Hermann et al 12 whose model represents the inputs and outputs by feedforward networks and map them to similar points in space However their approach cannot generate translations directly to get a translation they need to do a look up for closest vector in the precomputed database of sentences or to rescore a sentence 5 Conclusion In this work we showed that a large deep LSTM that has a limited vocabulary and that makes almost no assumption about problem structure can outperform a standard SMTbased system whose vocabulary is unlimited on a largescale MT task The success of our simple LSTMbased approach on MT suggests that it'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='simple LSTMbased approach on MT suggests that it should do well on many other sequence learning problems provided they have enough training data We were surprised by the extent of the improvement obtained by reversing the words in the source sentences We conclude that it is important to nd a problem encoding that has the greatest number of short term dependencies as they make the learning problem much simpler In particular while we were unable to train a standard RNN on the nonreversed translation problem shown in g 1 we believe that a standard RNN should be easily trainable when the source sentences are reversed although we did not verify it experimentally We were also surprised by the ability of the LSTM to correctly translate very long sentences We were initially convinced that the LSTM would fail on long sentences due to its limited memory and other researchers reported poor performance on long sentences with a model similar to ours 5 2 26 And yet LSTMs trained on the reversed dataset had little difculty translating long sentences Most importantly we demonstrated that a simple straightforward and a relatively unoptimized ap proach can outperform an SMT system so further work will likely lead to even greater translation accuracies These results suggest that our approach will likely do well on other challenging sequence to sequence problems 6 Acknowledgments We thank Samy Bengio Jeff Dean Matthieu Devin Geoffrey Hinton Nal Kalchbrenner Thang Luong Wolf gang Macherey Rajat Monga Vincent Vanhoucke Peng Xu Wojciech Zaremba and the Google Brain team for useful comments and discussions 8 References 1 M Auli M Galley C Quirk and G Zweig Joint language and translation modeling with recurrent neural networks In EMNLP 2013 2 D Bahdanau K Cho and Y Bengio Neural machine translation by jointly learning to align and translate arXiv preprint arXiv14090473 2014 3 Y Bengio R Ducharme P Vincent and C Jauvin A neural probabilistic language model In Journal of Machine Learning Research pages 11371155 2003 4 Y Bengio P Simard and P Frasconi Learning longterm dependencies with gradient descent is difcult IEEE Transactions on Neural Networks 52157166 1994 5 K Cho B Merrienboer C Gulcehre F Bougares H Schwenk and Y Bengio Learning phrase represen tations using RNN encoderdecoder for statistical machine translation In Arxiv preprint arXiv14061078 2014 6 D Ciresan U Meier and J Schmidhuber Multicolumn deep neural networks for image classication In CVPR 2012 7 G E Dahl D Yu L Deng and A Acero Contextdependent pretrained deep neural networks for large vocabulary speech recognition IEEE Transactions on Audio Speech and Language Processing  Special Issue on Deep Learning for Speech and Language Processing 2012 8 J Devlin R Zbib Z Huang T Lamar R Schwartz and J Makhoul Fast and robust neural network joint models for statistical machine translation In ACL 2014 9 Nadir Durrani Barry Haddow Philipp Koehn and Kenneth Heaeld Edinburghs phrasebased machine translation systems'),\n",
       " Document(metadata={'source': 'sequence_to_sequence_learning.pdf'}, page_content='phrasebased machine translation systems for wmt14 In WMT 2014 10 A Graves Generating sequences with recurrent neural networks In Arxiv preprint arXiv13080850 2013 11 A Graves S Fernandez F Gomez and J Schmidhuber Connectionist temporal classication labelling unsegmented sequence data with recurrent neural networks In ICML 2006 12 K M Hermann and P Blunsom Multilingual distributed representations without word alignment In ICLR 2014 13 G Hinton L Deng D Yu G Dahl A Mohamed N Jaitly A Senior V Vanhoucke P Nguyen T Sainath and B Kingsbury Deep neural networks for acoustic modeling in speech recognition IEEE Signal Processing Magazine 2012 14 S Hochreiter Untersuchungen zu dynamischen neuronalen netzen Masters thesis Institut fur Infor matik Technische Universitat Munchen 1991 15 S Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient ow in recurrent nets the difculty of learning longterm dependencies 2001 16 S Hochreiter and J Schmidhuber Long shortterm memory Neural Computation 1997 17 S Hochreiter and J Schmidhuber LSTM can solve hard long time lag problems 1997 18 N Kalchbrenner and P Blunsom Recurrent continuous translation models In EMNLP 2013 19 A Krizhevsky I Sutskever and G E Hinton ImageNet classication with deep convolutional neural networks In NIPS 2012 20 QV Le MA Ranzato R Monga M Devin K Chen GS Corrado J Dean and AY Ng Building highlevel features using large scale unsupervised learning In ICML 2012 21 Y LeCun L Bottou Y Bengio and P Haffner Gradientbased learning applied to document recognition Proceedings of the IEEE 1998 22 T Mikolov Statistical Language Models based on Neural Networks PhD thesis Brno University of Technology 2012 23 T Mikolov M Karaat L Burget J Cernocky and S Khudanpur Recurrent neural network based language model In INTERSPEECH pages 10451048 2010 24 K Papineni S Roukos T Ward and W J Zhu BLEU a method for automatic evaluation of machine translation In ACL 2002 25 R Pascanu T Mikolov and Y Bengio On the difculty of training recurrent neural networks arXiv preprint arXiv12115063 2012 26 J PougetAbadie D Bahdanau B van Merrienboer K Cho and Y Bengio Overcoming the curse of sentence length for neural machine translation using automatic segmentation arXiv preprint arXiv14091257 2014 27 A Razborov On small depth threshold circuits In Proc 3rd Scandinavian Workshop on Algorithm Theory 1992 28 D Rumelhart G E Hinton and R J Williams Learning representations by backpropagating errors Nature 3236088533536 1986 29 H Schwenk University le mans httpwwwliumunivlemansfrschwenkcslm jointpaper 2014 Online accessed 03September2014 30 M Sundermeyer R Schluter and H Ney LSTM neural networks for language modeling In INTER SPEECH 2010 31 P Werbos Backpropagation through time what it does and how to do it Proceedings of IEEE 1990 9'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='3 2 0 2 g u A 2  L C  s c  7 v 2 6 7 3 0  6 0 7 1  v i X r a Provided proper attribution is provided Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works Attention Is All You Need Ashish Vaswani Google Brain avaswanigooglecom Noam Shazeer Google Brain noamgooglecom Niki Parmar Google Research nikipgooglecom Jakob Uszkoreit Google Research uszgooglecom Llion Jones Google Research lliongooglecom Aidan N Gomez  University of Toronto aidancstorontoedu ukasz Kaiser Google Brain lukaszkaisergooglecom Illia Polosukhin  illiapolosukhingmailcom Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder The best performing models also connect the encoder and decoder through an attention mechanism We propose a new simple network architecture the Transformer based solely on attention mechanisms dispensing with recurrence and convolutions entirely Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train Our model achieves 284 BLEU on the WMT 2014 English toGerman translation task improving over the existing best results including ensembles by over 2 BLEU On the WMT 2014 EnglishtoFrench translation task our model establishes a new singlemodel stateoftheart BLEU score of 418 after training for 35 days on eight GPUs a small fraction of the training costs of the best models from the literature We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data Equal contribution Listing order is random Jakob proposed replacing RNNs with selfattention and started the effort to evaluate this idea Ashish with Illia designed and implemented the first Transformer models and has been crucially involved in every aspect of this work Noam proposed scaled dotproduct attention multihead attention and the parameterfree position representation and became the other person involved in nearly every detail Niki designed implemented tuned and evaluated countless model variants in our original codebase and tensor2tensor Llion also experimented with novel model variants was responsible for our initial codebase and efficient inference and visualizations Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor replacing our earlier codebase greatly improving results and massively accelerating our research Work performed while at Google Brain Work performed while at Google Research 31st Conference on Neural Information Processing Systems NIPS 2017 Long Beach CA USA 1 Introduction Recurrent neural networks long shortterm memory 13 and gated recurrent 7 neural networks in particular have been firmly established as state of the art approaches in sequence modeling and'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation 35 2 5 Numerous efforts have since continued to push the boundaries of recurrent language models and encoderdecoder architectures 38 24 15 Recurrent models typically factor computation along the symbol positions of the input and output sequences Aligning the positions to steps in computation time they generate a sequence of hidden states ht as a function of the previous hidden state ht1 and the input for position t This inherently sequential nature precludes parallelization within training examples which becomes critical at longer sequence lengths as memory constraints limit batching across examples Recent work has achieved significant improvements in computational efficiency through factorization tricks 21 and conditional computation 32 while also improving model performance in case of the latter The fundamental constraint of sequential computation however remains Attention mechanisms have become an integral part of compelling sequence modeling and transduc tion models in various tasks allowing modeling of dependencies without regard to their distance in the input or output sequences 2 19 In all but a few cases 27 however such attention mechanisms are used in conjunction with a recurrent network In this work we propose the Transformer a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 16 ByteNet 18 and ConvS2S 9 all of which use convolutional neural networks as basic building block computing hidden representations in parallel for all input and output positions In these models the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions linearly for ConvS2S and logarithmically for ByteNet This makes it more difficult to learn dependencies between distant positions 12 In the Transformer this is reduced to a constant number of operations albeit at the cost of reduced effective resolution due to averaging attentionweighted positions an effect we counteract with MultiHead Attention as described in section 32 Selfattention sometimes called intraattention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence Selfattention has been used successfully in a variety of tasks including reading comprehension abstractive summarization textual entailment and learning taskindependent sentence representations 4 27 28 22 Endtoend memory networks are based on a recurrent attention mechanism instead of'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='on a recurrent attention mechanism instead of sequence aligned recurrence and have been shown to perform well on simplelanguage question answering and language modeling tasks 34 To the best of our knowledge however the Transformer is the first transduction model relying entirely on selfattention to compute representations of its input and output without using sequence aligned RNNs or convolution In the following sections we will describe the Transformer motivate selfattention and discuss its advantages over models such as 17 18 and 9 3 Model Architecture Most competitive neural sequence transduction models have an encoderdecoder structure 5 2 35 Here the encoder maps an input sequence of symbol representations x1  xn to a sequence of continuous representations z  z1  zn Given z the decoder then generates an output sequence y1  ym of symbols one element at a time At each step the model is autoregressive 10 consuming the previously generated symbols as additional input when generating the next 2 Figure 1 The Transformer  model architecture The Transformer follows this overall architecture using stacked selfattention and pointwise fully connected layers for both the encoder and decoder shown in the left and right halves of Figure 1 respectively 31 Encoder and Decoder Stacks Encoder The encoder is composed of a stack of N  6 identical layers Each layer has two sublayers The first is a multihead selfattention mechanism and the second is a simple position wise fully connected feedforward network We employ a residual connection 11 around each of the two sublayers followed by layer normalization 1 That is the output of each sublayer is LayerNormx  Sublayerx where Sublayerx is the function implemented by the sublayer itself To facilitate these residual connections all sublayers in the model as well as the embedding layers produce outputs of dimension dmodel  512 Decoder The decoder is also composed of a stack of N  6 identical layers In addition to the two sublayers in each encoder layer the decoder inserts a third sublayer which performs multihead attention over the output of the encoder stack Similar to the encoder we employ residual connections around each of the sublayers followed by layer normalization We also modify the selfattention sublayer in the decoder stack to prevent positions from attending to subsequent positions This masking combined with fact that the output embeddings are offset by one position ensures that the predictions for position i can depend only on the known outputs at positions less than i 32 Attention An attention function can be described as mapping a query and a set of keyvalue pairs to an output where the query keys values and output are all vectors The output is computed as a weighted sum 3 Scaled DotProduct Attention MultiHead Attention Figure 2 left Scaled DotProduct Attention right MultiHead Attention consists of several attention layers running in parallel of the values where the weight assigned to each value is'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='values where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key 321 Scaled DotProduct Attention We call our particular attention Scaled DotProduct Attention Figure 2 The input consists of queries and keys of dimension dk and values of dimension dv We compute the dot products of the dk and apply a softmax function to obtain the weights on the query with all keys divide each by values  In practice we compute the attention function on a set of queries simultaneously packed together into a matrix Q The keys and values are also packed together into matrices K and V  We compute the matrix of outputs as AttentionQ K V   softmax QK T  dk V The two most commonly used attention functions are additive attention 2 and dotproduct multi plicative attention Dotproduct attention is identical to our algorithm except for the scaling factor  Additive attention computes the compatibility function using a feedforward network with of a single hidden layer While the two are similar in theoretical complexity dotproduct attention is much faster and more spaceefficient in practice since it can be implemented using highly optimized matrix multiplication code 1 dk While for small values of dk the two mechanisms perform similarly additive attention outperforms dot product attention without scaling for larger values of dk 3 We suspect that for large values of dk the dot products grow large in magnitude pushing the softmax function into regions where it has extremely small gradients 4 To counteract this effect we scale the dot products by 1 dk  322 MultiHead Attention Instead of performing a single attention function with dmodeldimensional keys values and queries we found it beneficial to linearly project the queries keys and values h times with different learned linear projections to dk dk and dv dimensions respectively On each of these projected versions of queries keys and values we then perform the attention function in parallel yielding dvdimensional 4To illustrate why the dot products get large assume that the components of q and k are independent random i1 qiki has mean 0 and variance dk variables with mean 0 and variance 1 Then their dot product q  k  cid80dk 4 1 output values These are concatenated and once again projected resulting in the final values as depicted in Figure 2 Multihead attention allows the model to jointly attend to information from different representation subspaces at different positions With a single attention head averaging inhibits this MultiHeadQ K V   Concathead1  headhW O where headi  AttentionQW Q i  KW K i  V W V i  Where the projections are parameter matrices W Q and W O  Rhdvdmodel i  Rdmodeldk  W K i  Rdmodeldk  W V i  Rdmodeldv In this work we employ h  8 parallel attention layers or heads For each of these we use dk  dv  dmodelh  64 Due to the reduced dimension of each head the total computational cost is similar to that of singlehead attention with full dimensionality 323'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='singlehead attention with full dimensionality 323 Applications of Attention in our Model The Transformer uses multihead attention in three different ways In encoderdecoder attention layers the queries come from the previous decoder layer and the memory keys and values come from the output of the encoder This allows every position in the decoder to attend over all positions in the input sequence This mimics the typical encoderdecoder attention mechanisms in sequencetosequence models such as 38 2 9 The encoder contains selfattention layers In a selfattention layer all of the keys values and queries come from the same place in this case the output of the previous layer in the encoder Each position in the encoder can attend to all positions in the previous layer of the encoder Similarly selfattention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position We need to prevent leftward information flow in the decoder to preserve the autoregressive property We implement this inside of scaled dotproduct attention by masking out setting to  all values in the input of the softmax which correspond to illegal connections See Figure 2 33 Positionwise FeedForward Networks In addition to attention sublayers each of the layers in our encoder and decoder contains a fully connected feedforward network which is applied to each position separately and identically This consists of two linear transformations with a ReLU activation in between FFNx  max0 xW1  b1W2  b2 2 While the linear transformations are the same across different positions they use different parameters from layer to layer Another way of describing this is as two convolutions with kernel size 1 The dimensionality of input and output is dmodel  512 and the innerlayer has dimensionality df f  2048 34 Embeddings and Softmax Similarly to other sequence transduction models we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel We also use the usual learned linear transfor mation and softmax function to convert the decoder output to predicted nexttoken probabilities In our model we share the same weight matrix between the two embedding layers and the presoftmax dmodel linear transformation similar to 30 In the embedding layers we multiply those weights by  5 Table 1 Maximum path lengths perlayer complexity and minimum number of sequential operations for different layer types n is the sequence length d is the representation dimension k is the kernel size of convolutions and r the size of the neighborhood in restricted selfattention Layer Type SelfAttention Recurrent Convolutional SelfAttention restricted Complexity per Layer On2  d On  d2 Ok  n  d2 Or  n  d Sequential Maximum Path Length Operations O1 On O1 O1 O1 On Ologkn Onr 35 Positional Encoding Since our model contains no recurrence and no convolution in order for the model to make use of the order of the sequence we must inject'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='use of the order of the sequence we must inject some information about the relative or absolute position of the tokens in the sequence To this end we add positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks The positional encodings have the same dimension dmodel as the embeddings so that the two can be summed There are many choices of positional encodings learned and fixed 9 In this work we use sine and cosine functions of different frequencies P Epos2i  sinpos100002idmodel P Epos2i1  cospos100002idmodel where pos is the position and i is the dimension That is each dimension of the positional encoding corresponds to a sinusoid The wavelengths form a geometric progression from 2 to 10000  2 We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions since for any fixed offset k P Eposk can be represented as a linear function of P Epos We also experimented with using learned positional embeddings 9 instead and found that the two versions produced nearly identical results see Table 3 row E We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training 4 Why SelfAttention In this section we compare various aspects of selfattention layers to the recurrent and convolu tional layers commonly used for mapping one variablelength sequence of symbol representations x1  xn to another sequence of equal length z1  zn with xi zi  Rd such as a hidden layer in a typical sequence transduction encoder or decoder Motivating our use of selfattention we consider three desiderata One is the total computational complexity per layer Another is the amount of computation that can be parallelized as measured by the minimum number of sequential operations required The third is the path length between longrange dependencies in the network Learning longrange dependencies is a key challenge in many sequence transduction tasks One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network The shorter these paths between any combination of positions in the input and output sequences the easier it is to learn longrange dependencies 12 Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types As noted in Table 1 a selfattention layer connects all positions with a constant number of sequentially executed operations whereas a recurrent layer requires On sequential operations In terms of computational complexity selfattention layers are faster than recurrent layers when the sequence 6 length n is smaller than the representation dimensionality d which is most often the case with sentence representations used by stateoftheart models in machine translations such as wordpiece 38 and bytepair 31 representations To improve computational'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='31 representations To improve computational performance for tasks involving very long sequences selfattention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position This would increase the maximum path length to Onr We plan to investigate this approach further in future work A single convolutional layer with kernel width k  n does not connect all pairs of input and output positions Doing so requires a stack of Onk convolutional layers in the case of contiguous kernels or Ologkn in the case of dilated convolutions 18 increasing the length of the longest paths between any two positions in the network Convolutional layers are generally more expensive than recurrent layers by a factor of k Separable convolutions 6 however decrease the complexity considerably to Ok  n  d  n  d2 Even with k  n however the complexity of a separable convolution is equal to the combination of a selfattention layer and a pointwise feedforward layer the approach we take in our model As side benefit selfattention could yield more interpretable models We inspect attention distributions from our models and present and discuss examples in the appendix Not only do individual attention heads clearly learn to perform different tasks many appear to exhibit behavior related to the syntactic and semantic structure of the sentences 5 Training This section describes the training regime for our models 51 Training Data and Batching We trained on the standard WMT 2014 EnglishGerman dataset consisting of about 45 million sentence pairs Sentences were encoded using bytepair encoding 3 which has a shared source target vocabulary of about 37000 tokens For EnglishFrench we used the significantly larger WMT 2014 EnglishFrench dataset consisting of 36M sentences and split tokens into a 32000 wordpiece vocabulary 38 Sentence pairs were batched together by approximate sequence length Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens 52 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs For our base models using the hyperparameters described throughout the paper each training step took about 04 seconds We trained the base models for a total of 100000 steps or 12 hours For our big modelsdescribed on the bottom line of table 3 step time was 10 seconds The big models were trained for 300000 steps 35 days 53 Optimizer We used the Adam optimizer 20 with 1  09 2  098 and   109 We varied the learning rate over the course of training according to the formula lrate  d05 model  minstepnum05 stepnum  warmupsteps15 This corresponds to increasing the learning rate linearly for the first warmupsteps training steps and decreasing it thereafter proportionally to the inverse square root of the step number We used warmupsteps  4000 54 Regularization We employ three types of regularization during training 7 3 Table 2 The Transformer'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='during training 7 3 Table 2 The Transformer achieves better BLEU scores than previous stateoftheart models on the EnglishtoGerman and EnglishtoFrench newstest2014 tests at a fraction of the training cost Model ByteNet 18 DeepAtt  PosUnk 39 GNMT  RL 38 ConvS2S 9 MoE 32 DeepAtt  PosUnk Ensemble 39 GNMT  RL Ensemble 38 ConvS2S Ensemble 9 Transformer base model Transformer big BLEU ENDE ENFR 2375 246 2516 2603 2630 2636 273 284 392 3992 4046 4056 404 4116 4129 381 418 Training Cost FLOPs ENDE ENFR 23  1019 96  1018 20  1019 18  1020 77  1019 10  1020 14  1020 15  1020 12  1020 80  1020 11  1021 12  1021 33  1018 23  1019 Residual Dropout We apply dropout 33 to the output of each sublayer before it is added to the sublayer input and normalized In addition we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks For the base model we use a rate of Pdrop  01 Label Smoothing During training we employed label smoothing of value ls  01 36 This hurts perplexity as the model learns to be more unsure but improves accuracy and BLEU score 6 Results 61 Machine Translation On the WMT 2014 EnglishtoGerman translation task the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 20 BLEU establishing a new stateoftheart BLEU score of 284 The configuration of this model is listed in the bottom line of Table 3 Training took 35 days on 8 P100 GPUs Even our base model surpasses all previously published models and ensembles at a fraction of the training cost of any of the competitive models On the WMT 2014 EnglishtoFrench translation task our big model achieves a BLEU score of 410 outperforming all of the previously published single models at less than 14 the training cost of the previous stateoftheart model The Transformer big model trained for EnglishtoFrench used dropout rate Pdrop  01 instead of 03 For the base models we used a single model obtained by averaging the last 5 checkpoints which were written at 10minute intervals For the big models we averaged the last 20 checkpoints We used beam search with a beam size of 4 and length penalty   06 38 These hyperparameters were chosen after experimentation on the development set We set the maximum output length during inference to input length  50 but terminate early when possible 38 Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature We estimate the number of floating point operations used to train a model by multiplying the training time the number of GPUs used and an estimate of the sustained singleprecision floatingpoint capacity of each GPU 5 62 Model Variations To evaluate the importance of different components of the Transformer we varied our base model in different ways measuring the change in performance on EnglishtoGerman translation on the 5We used values of 28 37 60 and 95 TFLOPS for K80 K40'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='used values of 28 37 60 and 95 TFLOPS for K80 K40 M40 and P100 respectively 8 Table 3 Variations on the Transformer architecture Unlisted values are identical to those of the base model All metrics are on the EnglishtoGerman translation development set newstest2013 Listed perplexities are perwordpiece according to our bytepair encoding and should not be compared to perword perplexities base A B C D N dmodel 6 512 2 4 8 256 1024 dff 2048 1024 4096 h 8 1 4 16 32 dk 64 512 128 32 16 16 32 32 128 dv 64 512 128 32 16 32 128 Pdrop 01 00 02 ls 01 00 02 PPL train steps dev 100K 492 529 500 491 501 516 501 611 519 488 575 466 512 475 577 495 467 547 492 300K 433 BLEU params 106 dev 258 65 249 255 258 254 251 254 237 253 255 245 260 254 262 246 255 253 257 257 264 58 60 36 50 80 28 168 53 90 E big 6 positional embedding instead of sinusoids 1024 4096 16 03 213 development set newstest2013 We used beam search as described in the previous section but no checkpoint averaging We present these results in Table 3 In Table 3 rows A we vary the number of attention heads and the attention key and value dimensions keeping the amount of computation constant as described in Section 322 While singlehead attention is 09 BLEU worse than the best setting quality also drops off with too many heads In Table 3 rows B we observe that reducing the attention key size dk hurts model quality This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial We further observe in rows C and D that as expected bigger models are better and dropout is very helpful in avoiding overfitting In row E we replace our sinusoidal positional encoding with learned positional embeddings 9 and observe nearly identical results to the base model 63 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing This task presents specific challenges the output is subject to strong structural constraints and is significantly longer than the input Furthermore RNN sequencetosequence models have not been able to attain stateoftheart results in smalldata regimes 37 We trained a 4layer transformer with dmodel  1024 on the Wall Street Journal WSJ portion of the Penn Treebank 25 about 40K training sentences We also trained it in a semisupervised setting using the larger highconfidence and BerkleyParser corpora from with approximately 17M sentences 37 We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semisupervised setting We performed only a small number of experiments to select the dropout both attention and residual section 54 learning rates and beam size on the Section 22 development set all other parameters remained unchanged from the EnglishtoGerman base translation model During inference we 9 Table 4 The Transformer generalizes well to English constituency parsing Results are on Section 23 of'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='constituency parsing Results are on Section 23 of WSJ Parser Training Vinyals  Kaiser el al 2014 37 WSJ only discriminative WSJ only discriminative WSJ only discriminative WSJ only discriminative WSJ only discriminative semisupervised semisupervised semisupervised semisupervised semisupervised multitask generative Petrov et al 2006 29 Zhu et al 2013 40 Dyer et al 2016 8 Transformer 4 layers Zhu et al 2013 40 Huang  Harper 2009 14 McClosky et al 2006 26 Vinyals  Kaiser el al 2014 37 Transformer 4 layers Luong et al 2015 23 Dyer et al 2016 8 WSJ 23 F1 883 904 904 917 913 913 913 921 921 927 930 933 increased the maximum output length to input length  300 We used a beam size of 21 and   03 for both WSJ only and the semisupervised setting Our results in Table 4 show that despite the lack of taskspecific tuning our model performs sur prisingly well yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar 8 In contrast to RNN sequencetosequence models 37 the Transformer outperforms the Berkeley Parser 29 even when training only on the WSJ training set of 40K sentences 7 Conclusion In this work we presented the Transformer the first sequence transduction model based entirely on attention replacing the recurrent layers most commonly used in encoderdecoder architectures with multiheaded selfattention For translation tasks the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers On both WMT 2014 EnglishtoGerman and WMT 2014 EnglishtoFrench translation tasks we achieve a new state of the art In the former task our best model outperforms even all previously reported ensembles We are excited about the future of attentionbased models and plan to apply them to other tasks We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local restricted attention mechanisms to efficiently handle large inputs and outputs such as images audio and video Making generation less sequential is another research goals of ours The code we used to train and evaluate our models is available at httpsgithubcom tensorflowtensor2tensor Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments corrections and inspiration References 1 Jimmy Lei Ba Jamie Ryan Kiros and Geoffrey E Hinton Layer normalization arXiv preprint arXiv160706450 2016 2 Dzmitry Bahdanau Kyunghyun Cho and Yoshua Bengio Neural machine translation by jointly learning to align and translate CoRR abs14090473 2014 3 Denny Britz Anna Goldie MinhThang Luong and Quoc V Le Massive exploration of neural machine translation architectures CoRR abs170303906 2017 4 Jianpeng Cheng Li Dong and Mirella Lapata Long shortterm memorynetworks for machine reading arXiv preprint arXiv160106733 2016 10 5 Kyunghyun Cho Bart van Merrienboer Caglar Gulcehre Fethi Bougares Holger Schwenk and Yoshua Bengio Learning phrase'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='Holger Schwenk and Yoshua Bengio Learning phrase representations using rnn encoderdecoder for statistical machine translation CoRR abs14061078 2014 6 Francois Chollet Xception Deep learning with depthwise separable convolutions arXiv preprint arXiv161002357 2016 7 Junyoung Chung aglar Glehre Kyunghyun Cho and Yoshua Bengio Empirical evaluation of gated recurrent neural networks on sequence modeling CoRR abs14123555 2014 8 Chris Dyer Adhiguna Kuncoro Miguel Ballesteros and Noah A Smith Recurrent neural network grammars In Proc of NAACL 2016 9 Jonas Gehring Michael Auli David Grangier Denis Yarats and Yann N Dauphin Convolu tional sequence to sequence learning arXiv preprint arXiv170503122v2 2017 10 Alex Graves Generating sequences with recurrent neural networks arXiv preprint arXiv13080850 2013 11 Kaiming He Xiangyu Zhang Shaoqing Ren and Jian Sun Deep residual learning for im age recognition In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition pages 770778 2016 12 Sepp Hochreiter Yoshua Bengio Paolo Frasconi and Jrgen Schmidhuber Gradient flow in recurrent nets the difficulty of learning longterm dependencies 2001 13 Sepp Hochreiter and Jrgen Schmidhuber Long shortterm memory Neural computation 9817351780 1997 14 Zhongqiang Huang and Mary Harper Selftraining PCFG grammars with latent annotations across languages In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 832841 ACL August 2009 15 Rafal Jozefowicz Oriol Vinyals Mike Schuster Noam Shazeer and Yonghui Wu Exploring the limits of language modeling arXiv preprint arXiv160202410 2016 16 ukasz Kaiser and Samy Bengio Can active memory replace attention In Advances in Neural Information Processing Systems NIPS 2016 17 ukasz Kaiser and Ilya Sutskever Neural GPUs learn algorithms In International Conference on Learning Representations ICLR 2016 18 Nal Kalchbrenner Lasse Espeholt Karen Simonyan Aaron van den Oord Alex Graves and Ko ray Kavukcuoglu Neural machine translation in linear time arXiv preprint arXiv161010099v2 2017 19 Yoon Kim Carl Denton Luong Hoang and Alexander M Rush Structured attention networks In International Conference on Learning Representations 2017 20 Diederik Kingma and Jimmy Ba Adam A method for stochastic optimization In ICLR 2015 21 Oleksii Kuchaiev and Boris Ginsburg Factorization tricks for LSTM networks arXiv preprint arXiv170310722 2017 22 Zhouhan Lin Minwei Feng Cicero Nogueira dos Santos Mo Yu Bing Xiang Bowen Zhou and Yoshua Bengio A structured selfattentive sentence embedding arXiv preprint arXiv170303130 2017 23 MinhThang Luong Quoc V Le Ilya Sutskever Oriol Vinyals and Lukasz Kaiser Multitask sequence to sequence learning arXiv preprint arXiv151106114 2015 24 MinhThang Luong Hieu Pham and Christopher D Manning Effective approaches to attention based neural machine translation arXiv preprint arXiv150804025 2015 11 25 Mitchell P Marcus Mary Ann Marcinkiewicz and Beatrice Santorini Building a'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='Marcinkiewicz and Beatrice Santorini Building a large annotated corpus of english The penn treebank Computational linguistics 192313330 1993 26 David McClosky Eugene Charniak and Mark Johnson Effective selftraining for parsing In Proceedings of the Human Language Technology Conference of the NAACL Main Conference pages 152159 ACL June 2006 27 Ankur Parikh Oscar Tckstrm Dipanjan Das and Jakob Uszkoreit A decomposable attention model In Empirical Methods in Natural Language Processing 2016 28 Romain Paulus Caiming Xiong and Richard Socher A deep reinforced model for abstractive summarization arXiv preprint arXiv170504304 2017 29 Slav Petrov Leon Barrett Romain Thibaux and Dan Klein Learning accurate compact and interpretable tree annotation In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 433440 ACL July 2006 30 Ofir Press and Lior Wolf Using the output embedding to improve language models arXiv preprint arXiv160805859 2016 31 Rico Sennrich Barry Haddow and Alexandra Birch Neural machine translation of rare words with subword units arXiv preprint arXiv150807909 2015 32 Noam Shazeer Azalia Mirhoseini Krzysztof Maziarz Andy Davis Quoc Le Geoffrey Hinton and Jeff Dean Outrageously large neural networks The sparselygated mixtureofexperts layer arXiv preprint arXiv170106538 2017 33 Nitish Srivastava Geoffrey E Hinton Alex Krizhevsky Ilya Sutskever and Ruslan Salakhutdi nov Dropout a simple way to prevent neural networks from overfitting Journal of Machine Learning Research 15119291958 2014 34 Sainbayar Sukhbaatar Arthur Szlam Jason Weston and Rob Fergus Endtoend memory networks In C Cortes N D Lawrence D D Lee M Sugiyama and R Garnett editors Advances in Neural Information Processing Systems 28 pages 24402448 Curran Associates Inc 2015 35 Ilya Sutskever Oriol Vinyals and Quoc VV Le Sequence to sequence learning with neural networks In Advances in Neural Information Processing Systems pages 31043112 2014 36 Christian Szegedy Vincent Vanhoucke Sergey Ioffe Jonathon Shlens and Zbigniew Wojna Rethinking the inception architecture for computer vision CoRR abs151200567 2015 37 Vinyals  Kaiser Koo Petrov Sutskever and Hinton Grammar as a foreign language In Advances in Neural Information Processing Systems 2015 38 Yonghui Wu Mike Schuster Zhifeng Chen Quoc V Le Mohammad Norouzi Wolfgang Macherey Maxim Krikun Yuan Cao Qin Gao Klaus Macherey et al Googles neural machine translation system Bridging the gap between human and machine translation arXiv preprint arXiv160908144 2016 39 Jie Zhou Ying Cao Xuguang Wang Peng Li and Wei Xu Deep recurrent models with fastforward connections for neural machine translation CoRR abs160604199 2016 40 Muhua Zhu Yue Zhang Wenliang Chen Min Zhang and Jingbo Zhu Fast and accurate shiftreduce constituent parsing In Proceedings of the 51st Annual Meeting of the ACL Volume 1 Long Papers pages 434443 ACL August 2013 12 Attention Visualizations this this difficult'),\n",
       " Document(metadata={'source': 'attention_all_you_need.pdf'}, page_content='12 Attention Visualizations this this difficult difficult process process majority have more more new new American American have of It passed passed registration registration governments governments voting It in in since since or or majority of voting laws pad EOS EOS InputInput Layer5   pad a the the spirit spirit that that making pad pad pad pad pad pad pad pad pad pad 2009 2009 is is making a laws Figure 3 An example of the attention mechanism following longdistance dependencies in the encoder selfattention in layer 5 of 6 Many of the attention heads attend to a distant dependency of the verb making completing the phrase makingmore difficult Attentions here shown only for the word making Different colors represent different heads Best viewed in color 13 application be be never just The Law will but InputInput Layer5  just just its should perfect just pad Figure 4 Two attention heads also in layer 5 of 6 apparently involved in anaphora resolution Top Full attentions for head 5 Bottom Isolated attentions from just the word its for attention heads 5 and 6 Note that the attentions are very sharp for this word 14 application be be never just The Law will but InputInput Layer5  just just its should perfect just pad Figure 5 Many of the attention heads exhibit behaviour that seems related to the structure of the sentence We give two such examples above from two different heads from the encoder selfattention at layer 5 of 6 The heads clearly learned to perform different tasks 15')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information Review Recurrent Neural Networks A Comprehensive Review of Architectures Variants and Applications Ibomoiye Domor Mienye 1  Theo G Swart 1 and George Obaido 2 1 Institute for Intelligent Systems University of Johannesburg Johannesburg 2006 South Africa tgswartujacza 2 Center for HumanCompatible Artificial Intelligence CHAI Berkeley Institute for Data Science BIDS University of California Berkeley CA 94720 USA gobaidoberkeleyedu Correspondence ibomoiyemujacza  These authors contributed equally to this work Abstract Recurrent neural networks RNNs have significantly advanced the field of machine learn ing ML by enabling the effective processing of sequential data This paper provides a comprehensive review of RNNs and their applications highlighting advancements in architectures such as long shortterm memory LSTM networks gated recurrent units GRUs bidirectional LSTM BiLSTM echo state networks ESNs peephole LSTM and stacked LSTM The study examines the application of RNNs to different domains including natural language processing NLP speech recognition time series forecasting autonomous vehicles and anomaly detection Additionally the study discusses recent innovations such as the integration of attention mechanisms and the development of hybrid models that combine RNNs with convolutional neural networks CNNs and transformer architec tures This review aims to provide ML researchers and practitioners with a comprehensive overview of the current state and future directions of RNN research Keywords deep learning GRU LSTM machine learning NLP RNN Citation Mienye ID Swart TG 1 Introduction Obaido G Recurrent Neural Networks A Comprehensive Review of Architectures Variants and Applications Information 2024 15 517 httpsdoiorg103390info15090517 Academic Editor Mara N Moreno Deep learning DL has reshaped the field of artificial intelligence AI driving ad vancements in a wide array of applications from image recognition and natural language processing NLP to autonomous driving and medical diagnostics 15 This rapid growth is fueled by the increasing availability of big data advancements in computing power and the development of sophisticated algorithms 69 As DL models continue to evolve they are increasingly being deployed in critical sectors demonstrating their ability to outperform traditional machine learning ML techniques in handling complex tasks Garca Received 21 July 2024 Revised 22 August 2024 Accepted 23 August 2024 Published 25 August 2024 Recurrent neural networks RNNs are a class of deep learning models that are funda mentally designed to handle sequential data 1011 Unlike feedforward neural networks RNNs possess the unique feature of maintaining a memory of previous inputs by using their internal state memory to process sequences of inputs 12 This makes them ideally suited for applications such as natural language processing speech recognition and time series forecasting where context and the order of data points are crucial Copyright\n"
     ]
    }
   ],
   "source": [
    "print(processed_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the order of data points are crucial Copyright  2024 by the authors Licensee MDPI Basel Switzerland This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution CC BY license https creativecommonsorglicensesby The inception of RNNs dates back to the 1970s with foundational work by Werbos 13 which introduced the concept of backpropagation through time BPTT that laid the foun dation for training recurrent neural networks However RNNs struggled with practical applications due to the vanishing gradient problem where gradients either grow or shrink exponentially during backpropagation 14 Meanwhile the introduction of Long Short Term Memory LSTM networks by Hochreiter and Schmidhuber 15 was a turning point for RNNs allowing for the learning of dependencies over much longer periods Addi tionally gated recurrent units GRUs proposed by Cho et al 16 offered a simplified alternative to LSTM while maintaining comparable performance 40 Information 2024 15 517 httpsdoiorg103390info15090517 httpswwwmdpicomjournalinformation Information 2024 15 517 Over the years these RNN architectures have been applied in different fields achieving excellent performance 1719 Despite their advancements and adoption in various fields RNNs have continued to evolve Specifically the increasing complexity of data and tasks in recent years has driven continuous innovations in RNN architectures and variants These developments have expanded the application of RNNs from simple sequence prediction to complex tasks such as multimodal learning and realtime decisionmaking systems Recent studies and reviews have highlighted the significant progress made in the field of RNNs For example Lipton et al 20 provided an overview of the theoretical foundations and applications of RNNs while Yu et al 21 focused on the LSTM cell and different variants Additionally Tarwani et al 22 reviewed the application and role of RNNs in natural language processing However many of these reviews do not fully capture the latest advancements and applications given the rapid pace of innovation in this field Additionally there remains a gap in the literature that comprehensively covers the latest advancements in RNN architectures and their applications across a broader range of fields Therefore this paper aims to fill that gap by providing a comprehensive review of RNNs assessing their theoretical advancements and practical implementations as well as cuttingedge applications thus helping shape future research on neural networks The rest of this paper is organized as follows Section 2 reviews related works Section 3 covers the fundamentals of RNNs including basic architecture and components Section 4 explores advanced RNN variants such as LSTM and GRU Section 5 highlights innovations in RNN architectures and training methodologies Section 6 presents some publicly available datasets used for RNN studies Section 7 discusses various applications of RNNs in the\n"
     ]
    }
   ],
   "source": [
    "print(processed_documents[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 The candidate hidden state h t represents the new content to be added to the current hidden state modulated via the reset gate Furthermore the simplified architecture of GRUs allows them to be computationally more efficient than LSTM while still addressing the vanishing gradient problem This efficiency makes GRUs wellsuited for tasks where computational resources are limited or when training needs to be faster GRUs have been successfully applied in various sequence modeling tasks Their ability to capture longterm dependencies with fewer parameters makes them a popular choice in many applications Additionally studies have shown that GRUs can achieve performance comparable to LSTM 7880 making them an attractive alternative for many use cases Figure 5 Architecture of the GRU network Comparison with LSTM GRUs have fewer parameters compared to LSTM due to the absence of a separate cell state and combined gating mechanisms 81 This often leads to faster training times and comparable performance to LSTM in many tasks However despite their advantages the choice between GRUs and LSTM often depends on the specific task and dataset Some tasks may benefit more from the additional complexity and gating mechanisms of LSTM while others may perform equally well with the simpler GRU architecture 44 Other Notable Variants While LSTM and GRUs are the most widely used RNN variants other architectures like peephole LSTM echo state networks and independently recurrent neural networks offer unique advantages for specific applications 441 Peephole LSTM Peephole LSTM introduced by Gers and Schmidhuber 82 enhances standard LSTM by allowing the gates to have access to the cell state through peephole connections This additional connection enables the LSTM to better regulate the gating mechanisms based on the current cell state improving timing decisions in applications such as speech recognition and financial time series prediction 83 In the following equations the input gate it forget gate ft and output gate ot are enhanced with peephole connections it  Wxixt  Whiht1  Wcict1  bi where it is the input gate and Wci is the peephole weight connecting the cell state ct1 to the input gate ft  Wx f xt  Wh f ht1  Wc f ct1  b f  10 of 34 26 27 Information 2024 15 517 where ft is the forget gate and Wc f is the peephole weight connecting the cell state ct1 to the forget gate ot  Wxoxt  Whoht1  Wcoct  bo where ot is the output gate and Wco is the peephole weight connecting the cell state ct to the output gate 442 Echo State Networks Echo state networks ESNs proposed by Jaeger 84 represent a class of RNNs in which the hidden layer also known as the reservoir is fixed and randomly connected while only the output layer is trained This architecture significantly simplifies the training process making ESNs particularly suitable for realtime signal processing time series prediction and adaptive control systems The state update and output computation in ESNs are achieved through the\n"
     ]
    }
   ],
   "source": [
    "print(processed_documents[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Description\n",
    "Initialize the HuggingFaceEmbeddings class with a pre-trained model for text embedding.\n",
    "\n",
    "This function sets up the HuggingFaceEmbeddings class using the specified model from the\n",
    "Hugging Face model hub. The model 'sentence-transformers/all-MiniLM-L6-v2' is a smaller,\n",
    "efficient version of the MiniLM model, optimized for generating dense vector embeddings of text.\n",
    "\n",
    "Args:\n",
    "    model_name (str): The name of the pre-trained model to use for embedding generation. \n",
    "                       In this case, the 'sentence-transformers/all-MiniLM-L6-v2' model is used, which \n",
    "                       is optimized for sentence-level embeddings.\n",
    "\n",
    "Returns:\n",
    "    HuggingFaceEmbeddings: An instance of the HuggingFaceEmbeddings class initialized with the chosen model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Correct initialization for HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Description\n",
    "Initialize a Chroma vector store to persist and retrieve text embeddings for efficient similarity search.\n",
    "\n",
    "This function sets up the Chroma vector store, a highly scalable and efficient solution for storing and querying\n",
    "dense vector embeddings generated by the HuggingFace model. The Chroma vector store allows for fast similarity \n",
    "searches on embedded documents, enabling tasks like document retrieval or search relevance ranking.\n",
    "\n",
    "Args:\n",
    "    collection_name (str): The name of the collection where the vector embeddings will be stored.\n",
    "    embedding_function (function): The function used to generate the embeddings for the documents. \n",
    "                                   In this case, the embeddings are generated by a HuggingFace model.\n",
    "    persist_directory (str, optional): The directory path where the vector store will be saved locally.\n",
    "                                       This is useful if you want to persist the data across sessions.\n",
    "                                       If persistence is not required, this argument can be omitted.\n",
    "\n",
    "Returns:\n",
    "    Chroma: An initialized Chroma vector store instance, capable of storing and querying document embeddings.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"research_documents_vector\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Description\n",
    "Generate unique UUIDs for each document in the processed documents list.\n",
    "\n",
    "This function creates a list of unique UUIDs (Universally Unique Identifiers) for each document\n",
    "to serve as a unique identifier in the vector store. UUIDs are used to ensure that each document\n",
    "can be uniquely referenced, stored, and retrieved in subsequent operations such as similarity searches.\n",
    "\n",
    "Args:\n",
    "    processed_documents (list): A list of processed document objects for which UUIDs will be generated.\n",
    "\n",
    "Returns:\n",
    "    list: A list of UUID strings, one for each document in the processed_documents list.\n",
    "\n",
    "Usage:\n",
    "    This list of UUIDs can be used as identifiers when storing the documents in a vector store \n",
    "    or database, ensuring that each document has a unique key for retrieval.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Add documents to the vector store\n",
    "from uuid import uuid4\n",
    "uuids = [str(uuid4()) for _ in range(len(processed_documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afd9cb02-e1a2-48fe-bb56-eb920dcb3cfd',\n",
       " '786f6505-1dca-4245-8265-97dcc3df704f',\n",
       " 'ea889b07-949e-418c-8903-df4682d104e7',\n",
       " 'cbbcfea6-8644-4cb6-b59e-d6497a503e26',\n",
       " 'd38c8538-00c5-434d-af02-b681cb99e032',\n",
       " 'fc682595-ae84-45ac-af29-25a52580e2c9',\n",
       " 'd8765172-f81c-4bea-8ff3-fa2c5453c89b',\n",
       " '529dde93-132c-49bb-a1e0-3eeeaee8a66e',\n",
       " 'b757c2ee-f3b1-48c4-a9f9-c85e5b09f5e8',\n",
       " '66625301-bb45-4422-80d9-c59a1bb290bd',\n",
       " 'f8e9aeff-3076-4aef-a030-23e63489d7a6',\n",
       " 'bfa41a8b-1f78-443a-a8f5-a0f05482127f',\n",
       " '9a3b335e-6909-4077-b14f-e61153db65c9',\n",
       " '348bf88f-71d8-47b5-9921-f322933d31ce',\n",
       " 'feed7ebc-0907-4092-bc50-aa5ae6bceb08',\n",
       " '536d5768-f394-4fa5-990a-0549c7cab401',\n",
       " 'c3d36242-b6e6-4071-b7df-08d02e95a24c',\n",
       " '082a5ed1-48f8-4aa5-91c4-893c3772d4f6',\n",
       " '750a527e-f55f-4090-ac1a-08d96c7de47c',\n",
       " 'c2fe56cb-0bc5-41eb-9ec1-63dafcdb8bfb',\n",
       " 'f5460f2d-d00a-48e6-9dac-6198b3e663e1',\n",
       " '692e02ac-adf0-4010-a5d2-b41758dfb97e',\n",
       " 'ac575b6f-3597-4f6d-aaee-7b57effde904',\n",
       " '0db288da-2d91-4132-a313-9b37e5bb1615',\n",
       " '3a777577-ec20-4c21-8b62-4d33292d8356',\n",
       " '07163413-0f1b-459a-a2bd-ae9aa9ed73b4',\n",
       " '7e7076d7-7078-4338-a70e-a2518aef24a8',\n",
       " 'e15c9fd8-0c34-42f7-8db3-68caa8a95be7',\n",
       " '3f007356-e9f8-477d-9420-ffce3cda140c',\n",
       " '68e8adc5-0b87-4672-8167-b568468f97cf',\n",
       " 'af4c7a23-9ea0-46ac-90c7-d0faba52f808',\n",
       " 'bd1d5291-e45e-440f-87d6-3f3cd5c9b580',\n",
       " '2c8770d4-6692-46a8-a48d-71dcc4195cc8',\n",
       " 'b197a0b5-13a6-48fc-8bef-8232c29c73aa',\n",
       " 'b0c7492e-6236-4813-a35b-2d9b01ada7fd',\n",
       " '3c1ef1e8-4474-4f9e-8803-0dd585c03ac6',\n",
       " 'b644b9c2-57e5-42b8-9bea-cdb1da43b6da',\n",
       " 'de03590f-2cfa-40d1-afb3-338b9e2aaf02',\n",
       " '1d5f77fb-6d7e-4ca9-ba21-04fccfe2ed75',\n",
       " 'd6245dff-3094-4a93-b1be-261a2c8c45b9',\n",
       " 'db38c1f3-e47e-48c1-9e9d-313ca4b5a54c',\n",
       " 'e44b9963-87fe-4b83-ab86-792715d92f35',\n",
       " 'c4020631-d675-4f32-95a8-f0dc6073ddd3',\n",
       " '89157016-fccd-4f96-9829-caf5ca7ec3f5',\n",
       " 'a9bb4e33-9882-4697-9f72-25aa143c2259',\n",
       " '578a9240-029a-423b-86d1-48d96d643d5b',\n",
       " '74feb403-91b3-49c7-9b35-26b84c485a4f',\n",
       " 'eb68c611-6e07-41cc-acea-4b50df3c324f',\n",
       " '68b4d255-0cd6-41d5-804c-7f562854b4b5',\n",
       " '2ed2b0e6-6d0d-4bc2-97c5-b11d4e5b2f5b',\n",
       " 'aea0ac26-f718-4bf8-b339-572f295e03ec',\n",
       " '89c5818b-7d43-430f-962c-71835c0ecdb2',\n",
       " 'f8862c6d-0acc-44f6-b2c4-48bd8c8b686f',\n",
       " '1cdf0fbd-9563-405b-adcf-71b537f5b4c1',\n",
       " '3388ad0b-599a-4717-9be9-0129052f1ffc',\n",
       " '5fdfa6c3-e943-4ab7-a214-1321cd118c82',\n",
       " 'd8d2d36c-52ef-487b-91d5-c9ab817037fe',\n",
       " '556f8178-da72-4828-bdd2-17da7d3c130e',\n",
       " '5d4bd896-9bee-4a3e-b8e7-80ae5a0ec4da',\n",
       " 'a8700f2e-ee01-465e-a493-2c909dbb9bc6',\n",
       " '1cf68d88-6b1d-4cee-b52d-ce15c3eb7c0f',\n",
       " '560f9364-df05-4c63-91c3-447b3f285dc0',\n",
       " '930fde15-31e0-4fab-97e7-cc642355f360',\n",
       " 'bea866bb-f57e-4b61-a86e-ee7598e42763',\n",
       " '468fa54b-76b2-4f00-af1f-76ebf53e44f0',\n",
       " '069d7ead-ec0a-4b45-97f3-0ccf75d0aec0',\n",
       " '2f91b2a6-33f4-420b-a2b2-7ec0c4fb29e6',\n",
       " 'f443e829-ba23-4360-9180-078ef3f3986f',\n",
       " 'a8b93717-d884-4836-aa8e-e015f6445c22',\n",
       " '683455be-a300-433e-9418-5dcf2f06f0ef',\n",
       " 'cf4b2e49-c232-42bb-9b2a-67defd3c3ab6',\n",
       " 'f81deabd-b399-436f-ad27-de7f6a1d2b0d',\n",
       " '7c35796e-6261-4fbb-89cd-21329b098317',\n",
       " '3e4d512f-1f19-4f29-8d85-81bd582c8055',\n",
       " '3609472e-7bd6-4093-bdf6-49a39357b0ea',\n",
       " 'cc56941e-bb05-4b44-9e52-61910d7de18c',\n",
       " 'cd73ad03-14ef-415c-b7a9-b04891a62c8a',\n",
       " '2726f69f-e615-4f61-99d9-e5e719554d21',\n",
       " 'ee777799-d54f-4865-bbdf-daa63ea354d3',\n",
       " 'decf6c50-5d9c-476a-b747-dcbf15ce2330',\n",
       " 'aa0c37cb-7de0-401f-8bc3-19c7c689d546',\n",
       " '244fdff5-9cb8-4e4d-8ad1-c4b61a6053ee',\n",
       " '53f070c2-92f9-41ce-8dcf-3874cde04432',\n",
       " 'fd70e770-cb17-4b5a-b31f-974b5a1b3154',\n",
       " '959ae9b9-bf87-49e8-9bf2-4c98dfa65477',\n",
       " '79c1415e-96fc-460f-b363-df503f05f831',\n",
       " 'fafc2617-2f44-4b5e-bf60-d1633fe7c5a5',\n",
       " '24bc4204-f4d4-40d6-8617-0b987fd91c83',\n",
       " '68a4b1bc-ae93-4ea8-a574-4e61dd441c04',\n",
       " '103daaa1-4fde-4717-9615-0068ae5b8821',\n",
       " '6e3769e9-446f-4d62-bd2a-f28306e5256e',\n",
       " '304cc012-6f35-4667-a261-c5e80a7db258',\n",
       " 'ae6cb080-ecbb-44fd-bb6f-8d8c0f2c953f',\n",
       " '2ec5bdd2-efb0-41b3-ab0a-49d7d309d782',\n",
       " 'cc0b89d0-79dc-451d-853e-62176684ce8d',\n",
       " '486ba356-2b77-4bd8-9360-27e99ab50416',\n",
       " '0782e52d-3936-4bea-b7e0-b5e49c139fb2',\n",
       " '7b7ce75e-f72c-45a1-bf62-6703b6be7981',\n",
       " '2a968169-51c5-46a4-af16-b00c5c23e4a5',\n",
       " '84c88fa1-3407-4f11-b596-0e8046a07637',\n",
       " 'ade23f03-de51-4812-b513-ececb29e24c5',\n",
       " '18d3b152-ad3f-4bf2-b9fd-6700d202e4d6',\n",
       " 'cdd9cf2d-ad9a-41be-a264-75ba09ccf51c',\n",
       " 'eef427e6-efea-49ab-9c2b-f196ce451790',\n",
       " '4c6f9a74-4994-49ed-a8fe-7001987a93f6',\n",
       " '9fa44a46-82db-4e0e-91a5-f93f692b287a',\n",
       " '125b4e1e-0897-46cb-a439-4b92b86821e5',\n",
       " '15b5cb51-a477-44a9-bb59-a36f1063455e',\n",
       " 'a8017419-0cbd-4b6e-b699-2d463186fc09',\n",
       " '08d3de0b-eda5-4042-8d6a-185d093f3d31',\n",
       " '0cfe8fbe-5c31-4369-831d-c980f758e4ea',\n",
       " '15f1a12c-f4e7-4486-845e-f893a5613077',\n",
       " 'ea56151f-5925-4e42-afc1-e979b95398ea',\n",
       " 'ceb3bb3f-61ad-490a-95d0-f35265a4482a',\n",
       " '8499ba73-514f-4ecf-b1af-82f625fb1b6d',\n",
       " '614ea18f-4188-4430-acdb-9bfb24922848',\n",
       " 'ee3a198a-1276-4a89-8fc9-eed1ee502e0b',\n",
       " 'e8824c49-987f-41fa-8444-842a9467cc8d',\n",
       " '67c49255-3425-41bc-97cb-bcb48face432',\n",
       " 'cd45b047-907b-44e4-9dd5-3329857ae880',\n",
       " '3ad59fa4-67fe-4782-9aa6-7abaf2172c04',\n",
       " '9f1f018d-f152-46c3-acde-c41736b9db77',\n",
       " 'd50fddf4-0ed9-441b-b970-9225bde8af60',\n",
       " 'eca74641-6bdc-42c7-b008-fa259197554a',\n",
       " '271be7c8-b652-4cc1-aab8-a819eefbd576',\n",
       " '748a7768-d334-41e9-bf78-14ab0fec7806',\n",
       " 'c14deafe-3bce-421f-848b-90672f263fbe',\n",
       " 'eb879b0f-88f3-40e5-b8a0-0c1383470a87',\n",
       " 'ac27ff82-cfd0-461e-81a3-1f8f61ed4874',\n",
       " '6821be63-75d3-4441-90d3-ecbb2aeb596e',\n",
       " '865dbc88-ae36-4d8d-9c0d-04e8b9e203d6',\n",
       " 'f93ae8cc-e717-4c74-8d50-ebf23a6cb306']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Description\n",
    "Add processed documents to the vector store with associated unique identifiers (UUIDs).\n",
    "\n",
    "This function is responsible for adding a list of processed documents to the vector store. The documents\n",
    "are paired with unique identifiers (UUIDs) to ensure each document can be individually referenced and retrieved.\n",
    "The `add_documents` method takes the processed documents and their corresponding UUIDs as input and stores them\n",
    "in the vector store for future operations such as similarity search and retrieval.\n",
    "\n",
    "Args:\n",
    "    documents (list): A list of processed document objects that need to be added to the vector store.\n",
    "    ids (list): A list of unique UUID strings, one for each document in the documents list, to serve as document IDs.\n",
    "\n",
    "Returns:\n",
    "    None: This method updates the vector store in-place and does not return any value.\n",
    "\n",
    "Usage:\n",
    "    This method is typically used after documents have been processed and embedded. The documents, now in vector\n",
    "    form, are added to the vector store along with their UUIDs, making it easier to retrieve and identify specific documents\n",
    "    later based on their embeddings.\n",
    "\n",
    "Note:\n",
    "    Ensure that the `documents` list and the `ids` list (UUIDs) have the same length. Each UUID will correspond to a document,\n",
    "    providing a unique reference for each document in the vector store.\n",
    "\"\"\"\n",
    "\n",
    "vector_store.add_documents(documents=processed_documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Description\n",
    "Collect unique sources from the processed documents.\n",
    "\n",
    "This code iterates over a list of processed documents and extracts the unique 'source' metadata from each document.\n",
    "The goal is to gather all the distinct sources (e.g., filenames, URLs, or other metadata) that contributed to the documents\n",
    "in the vector store. The resulting list, `sources`, will contain only unique source names, ensuring that no duplicates are\n",
    "included.\n",
    "\n",
    "Args:\n",
    "    processed_documents (list): A list of processed document objects, each with metadata containing a 'source' key.\n",
    "\n",
    "Returns:\n",
    "    sources (list): A list containing unique source names derived from the 'source' field in the document metadata.\n",
    "\n",
    "Usage:\n",
    "    This approach is useful for identifying all the distinct sources from which documents were processed or extracted.\n",
    "    It helps in tracking and ensuring diversity of input sources, or it can be used for source-based grouping or analysis.\n",
    "\n",
    "Note:\n",
    "    The `sources` list will contain each source only once, even if multiple documents originate from the same source.\n",
    "\"\"\"\n",
    "\n",
    "sources = []\n",
    "for i in processed_documents:\n",
    "    if i.metadata[\"source\"] not in sources:\n",
    "        sources.append(i.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recurrent_Neural_Networks_A_Comprehensive_Review_of_Architectures_Variants_and_Applications.pdf',\n",
       " 'GRU.pdf',\n",
       " 'A_Comprehensive_Overview_and_Comparative_Analysis_on_Deep_Learning_Models_ CNN_RNN_LSTM_GRU.pdf',\n",
       " 'sequence_to_sequence_learning.pdf',\n",
       " 'attention_all_you_need.pdf']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query=\"Can you explain the architecture of all sequence-to-sequence traditional models models like LSTM, GRU, and RNN, along with how Transformers \\\n",
    "       improved upon these models. also provide trasnformer architecture details\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.791732] 7 discusses various applications of RNNs in the literature Section 8 addresses challenges and future research directions Finally Section 9 concludes the study 2 Related Works RNNs have been applied in different applications achieving stateoftheart perfor mance especially in timeseries applications Early developments in RNNs including locally recurrent globally feedforward networks as reviewed by Tsoi and Back 23 and the blockdiagonal recurrent neural networks proposed by Mastorocostas and Theocharis 24 laid important groundwork for understanding complex sequence modeling Several reviews have been conducted on RNNs and their applications each contribut ing to the understanding and development of the field For instance Dutta et al 25 provided a comprehensive overview of the theoretical foundations of RNNs and their applications in sequence learning Their review highlighted the challenges associated with training RNNs particularly the vanishing gradient problem and discussed the ad vancements in LSTM and GRU architectures However the review primarily focused on the theoretical aspects and applications of RNNs and did not extensively cover the latest innovations and practical applications in emerging fields such as bioinformatics and autonomous systems Quradaa et al 26 presented a startoftheart review of RNNs covering the core architectures with a focus on applications in code clones Similarly the review by Tarwani et al 22 provided an indepth analysis of RNNs in the context of NLP While this review offered valuable insights into the advancements in NLP it lacked a broader perspective on other application domains and recent architec tural innovations Another significant review by Goodfellow et al 27 focused on the fundamentals of deep learning including RNNs and discussed their applications across various domains This review provided a solid foundation but did not delve deeply into the specific advancements in RNN architectures and their specialized applications Greff et al 28 conducted an extensive study comparing various LSTM variants to determine their effectiveness in different applications While this review provided a thorough comparison it primarily focused on LSTM architectures and did not address other RNN variants or the latest hybrid models In similar research AlSelwi et al 29 reviewed LSTM applications in the literature covering articles from the 20182023 time period Zaremba et al 30 reviewed the use of RNNs in language modeling highlighting 2 of 34 Information 2024 15 517 significant achievements and the ongoing challenges in this field Their work offered valuable insights into the application of RNNs in NLP but was limited to language modeling and did not explore other potential applications Bai et al 31 provided a critical review of RNNs and their variants comparing them with other sequence modeling techniques like CNNs and attentionbased models Che et al 32 focused on the application of RNNs in healthcare particularly for electronic\n",
      "* [SIM=0.862958] data in both forward and backward directions for better context understanding Medium stability de pends on depth Speech recognition and sentiment analysis Deep RNN Multiple RNN layers are stacked to learn hi erarchical features Variable and the risk of vanishing gradients increases with depth Complex sequence model ing like video processing ESN Fixed hidden layer weights trained only at the output Not applicable as train ing bypasses typical gradient issues Time series prediction and system control Peephole LSTM Adds peephole connec tions to LSTM gates Stable and similar to LSTM Recognition of complex temporal patterns like mu sical notation IndRNN Allows training of deeper networks by maintaining indepen dence between time steps Reduces risk of vanish ing and exploding gra dients Very long sequences such as in video processing or long text generation 5 Innovations in RNN Architectures and Training Methodologies In recent years there have been significant innovations in RNN architectures and train ing methodologies aimed at enhancing performance and addressing existing limitations Information 2024 15 517 51 Hybrid Architectures Combining RNNs with other neural network architectures has led to hybrid models that leverage the strengths of each component For example integrating CNNs with RNNs has proven effective in video analysis where CNNs handle spatial features while RNNs capture temporal dynamics 9393 This approach allows the model to process both spatial and temporal information enhancing its ability to recognize patterns and make predictions Furthermore incorporating attention mechanisms into RNNs has also improved their ability to model longrange dependencies Attention mechanisms enable the network to focus on relevant parts of the input sequence which is useful in tasks such as machine translation and text summarization The attention mechanism can be described as follows at  softmaxut ct  T  i1 atihi where at is the attention weight ut is the score function and ct is the context vector 52 Neural Architecture Search Neural architecture search NAS has automated the design of RNN architectures enabling the discovery of more efficient and powerful models 9495 NAS techniques such as those pioneered by Zoph and Le 96 explore various combinations of layers activation functions and hyperparameters to find optimal configurations that outperform manually designed architectures The NAS process can be formulated as an optimization problem A  arg max AS AccuracyA where A represents an architecture S is the search space and A is the optimal architecture 53 Advanced Optimization Techniques Advanced optimization techniques have been developed to improve the training efficiency and stability of RNNs Gradient clipping is a technique used to prevent the gradients from becoming too large which can destabilize training 5897 g  g max1 g    where g is the gradient and  is the threshold value Furthermore adaptive learning rates such as those used in the Adam\n",
      "* [SIM=0.954292] GateVariants of Gated Recurrent Unit GRU Neural Networks Rahul Dey and Fathi M Salem Circuits Systems and Neural Networks CSANN LAB Department of Electrical and Computer Engineering Michigan State University East Lansing MI 488241226 USA deyrahulmsuedu  salemfmsuedu Abstract  The paper evaluates three variants of the Gated Recurrent Unit GRU in recurrent neural networks RNN by reducing parameters in the update and reset gates We evaluate the three variant GRU models on MNIST and IMDB datasets and show that these GRURNN variant models perform as well as the the original GRU RNN model while reducing computational expense performance sentiment classification from a given review paragraph II BACKGROUND RNN LSTM AND GRU In principal RNN are more suitable relationships among sequential data types The socalled simple RNN has a recurrent hidden state as in for capturing I INTRODUCTION Gated Recurrent Neural Network RNN have shown success in several applications involving sequential or temporal data 113 For example they have been applied extensively in speech recognition natural language processing machine translation etc 2 5 Long ShortTerm Memory LSTM RNN and the recently introduced Gated Recurrent Unit GRU RNN have been successfully shown to perform well with long sequence applications 25 812 Their success is primarily due to the gating network signals that control how the present input and previous memory are used to update the current activation and produce the current state These gates have their own sets of weights that are adaptively updated in the learning phase ie the training and evaluation process While these models empower successful in learning parameterization through their gate networks Consequently there is an added computational expense visvis the simple RNN model 2 5 6 It is noted that the LSTM RNN employs 3 distinct gate networks while the GRU RNN reduce the gate networks to two In 14 it is proposed to reduce the external gates to the minimum of one with preliminary evaluation of sustained performance in RNN they introduce an increase In this paper we focus on the GRU RNN and explore three new gatevariants with reduced parameterization We comparatively evaluate the performance of the original and the variant GRU RNN on two public datasets Using the MNIST dataset one generates two sequences 2 5 6 14 One sequence is obtained from each 28x28 image sample as pixel wise long sequence of length 28x28784 basically scanning from the upper left to the bottom right of the image Also one generates a rowwise short sequence of length 28 with each element being a vector of dimension 28 14 15 The third sequence type employs the IMDB movie review dataset where one defines the length of the sequence in order to achieve high cid2  cid4cid5cid6cid7cid2  cid9cid21  cid12cid13 cid51cid13 where cid7cid15 is the external mdimensional input vector at time cid2  cid15 the ndimensional hidden state g is the pointwise activation function such as the logistic\n",
      "* [SIM=1.077509] activation function such as the logistic function the hyperbolic tangent function or the rectified Linear Unit ReLU 2 6 and cid6 cid9 cid17cid18cid19 cid12 are the appropriately sized parameters two weights and bias Specifically in this case cid6 is an cid18  cid21 matrix cid9 is an cid18  cid18 matrix and cid12 is an cid18  1 matrix or vector Bengio at al 1 showed that it is difficult to capture longterm dependencies using such simple RNN because the stochastic gradients tend to either vanish or explode with long sequences Two particular models the Long ShortTerm Memory LSTM unit RNN 3 4 and Gated Recurrent Unit GRU RNN 2 have been proposed to solve the vanishing or exploding gradient problems We will present these two models in sufficient details for our purposes below A Long ShortTerm Memory LSTM RNN The LSTM RNN architecture uses the computation of the simple RNN of Eqn 1 as an intermediate candidate for the internal memory cell state say cid22cid15 and add it in a element wise weightedsum to the previous value of the internal memory state say cid22cid15cid24cid25 to produce the current value of the memory cell state cid22cid15 This is expressed succinctly in the following discrete dynamic equations cid22cid15  cid27cid15cid22cid15cid24cid25  cid29cid15  cid22cid15 cid52cid13 cid22cid15  cid4cid5cid6cid31cid7cid15  cid9cid31cid15cid24cid25  cid12cid31cid13 cid53cid13 cid15  cid15gcid5cid22cid15cid13 cid54cid13 In Eqns 3 and 4 the activation nonlinearity cid4 is typically the hyperbolic tangent function but more recently may be implemented as a rectified Linear Unit reLU The weighted sum is implemented in Eqn 2 via elementwise Hadamard multiplication denoted by  to gating signals The gating control signals cid29cid15 cid27cid15 cid17cid18cid19 cid15 denote respectively the input forget and output gating signals at time cid2  These control gating signals are in fact replica of the basic equation 3 with their own parameters and replacing cid4 by the logistic function The logistic function limits the gating signals to within 0 and 1 The specific mathematical form of the gating signals are thus expressed as the vector equations cid29cid15  cid5cid6cid7cid15  cid9cid15cid24cid25  cid12cid13 cid27cid15  cid6cid7cid15  cid9cid15cid24cid25  cid12 cid15  cid5cid6cid7cid15  cid9cid15cid24cid25  cid12cid13 where  is the logistic nonlinearity and the parameters for each gate consist of two matrices and a bias vector Thus the total number of parameters represented as matrices and bias vectors for the 3 gates and the memory cell structure are respectively cid6 cid9 cid12 cid6 cid9 cid12 cid6 cid9 cid12 cid6cid31 cid9cid31 cid17cid18cid19 cid12cid31 These parameters are all updated at each training step and stored It is immediately noted that the number of parameters in the LSTM model is increased 4folds from the simple RNN model in Eqn 1 Assume that the cell state is ndimensional Note that the activation and all the gates have the same dimensions Assume\n",
      "* [SIM=0.897213] sequence modeling tasks such as natural language processing speech recognition and sentiment analysis It has shown promising results in capturing complex patterns and dependencies in sequential data making it a popular choice for tasks that require an understanding of both past and future context 333 Gated Recurrent Unit GRU The Gated Recurrent Unit GRU is another variant of the RNN architecture that addresses the shortterm memory issue and offers a simpler structure compared to LSTM 59 GRU combines the input gate and forget gate of LSTM into a single update gate resulting in a more streamlined design Unlike LSTM GRU does not include a separate cell state A GRU unit consists of three main components an update gate a reset gate and the current memory content These gates enable the GRU to selectively update and utilize information from previous time steps allowing it to capture longterm dependencies in sequences 78 Fig 12 illustrates the structure of a GRU unit 79 The update gate Eq 6 determines how much of the past information should be retained and combined with the current input at a specific time step It is computed based on the concatenation of the previous hidden state 1 and the current input  followed by a linear transformation and a sigmoid activation function   1    6 The reset gate Eq 7 decides how much of the past information should be forgotten It is computed in a similar manner to the update gate using the concatenation of the previous hidden state and the current input   1    7 The current memory content Eq 8 is calculated based on the reset gate and the concatenation of the transformed previous hidden state and the current input The result is passed through a hyperbolic tangent activation function to produce the candidate activation   1  8 10 F M Shiri et al Figure 12 The structure of a GRU unit 79 Finally the final memory state  is determined by a combination of the previous hidden state and the candidate activation Eq 9 The update gate determines the balance between the previous hidden state and the candidate activation Additionally an output gate  can be introduced to control the information flow from the current memory content to the output Eq 10 The output gate is computed using the current memory state  and is typically followed by an activation function such as the sigmoid function   1  1   9      10 where the weight matrix of the output layer is  and the bias vector of the output layer is  GRU offers a simpler alternative to LSTM with fewer tensor operations allowing for faster training However the choice between GRU and LSTM depends on the specific use case and problem at hand Both architectures have their advantages and disadvantages and their performance may vary depending on the nature of the task 59 334 Bidirectional GRU The Bidirectional Gated Recurrent Unit BiGRU 80 improves upon the conventional GRU architecture through the integration of contexts from the past and future in sequential modeling tasks In contrast to\n",
      "* [SIM=0.971134] of the internal state on the system 60 73 8 F M Shiri et al a b Figure 10 a The highlevel architecture of LSTM b The inner structure of LSTM unit 60 Fig 10 b illustrates the update mechanism within the inner structure of an LSTM The update for the LSTM unit is expressed by Eq 5     1     1      1     5     1         1      where  and  represent the activation functions of the system state and internal state typically utilizing the hyperbolic tangent function The gating operation denoted as g is a feedforward neural network with a sigmoid activation function ensuring output values within the range of 0 1 which are interpreted as a set of weights The subscripts   and  correspond to the input gate output gate and forget gate respectively 1    While standard LSTM has demonstrated promising performance in various tasks it may struggle to comprehend input structures that are more complex than a sequential format To address this limitation a treestructured LSTM network known as SLSTM was proposed by 74 S LSTM consists of memory blocks comprising an input gate two forget gates a cell gate and an output gate While SLSTM exhibits superior performance in challenging sequential modeling problems it comes with higher computational complexity compared to standard LSTM 75 332 Bidirectional LSTM Bidirectional Long ShortTerm Memory BiLSTM is an extension of the LSTM architecture that addresses the limitation of standard LSTM models by considering both past and future context in sequence modeling tasks While traditional LSTM models process input data only in the forward direction BiLSTM overcomes this limitation by training the model in two directions forward and backward 76 A BiLSTM consists of two parallel LSTM layers one processes the input sequence in the forward direction while the other processes it in the backward direction The forward LSTM layer reads the input data from left to right as indicated by the green arrow in Fig 11 Simultaneously the backward LSTM layer reads the input data from right to left as represented by the red arrow 77 This bidirectional processing enables the model to capture information from both past and future contexts allowing for a more comprehensive understanding of temporal dependencies within the sequence 9 A Comprehensive Overview and Comparative Analysis on Deep Learning Models Figure 11 The architecture of a Bidirectional LSTM model 76 During the training phase the forward and backward LSTM layers independently extract features and update their internal states based on the input sequence The output of each LSTM layer at each time step is a prediction score These prediction scores are then combined using a weighted sum to generate the final output result 77 By incorporating information from both directions BiLSTM models can capture a broader context and improve the models ability to model temporal dependencies in sequential data BiLSTM has been widely applied in various sequence modeling tasks such as natural language\n",
      "* [SIM=0.943760] many important problems are best expressed with sequences whose lengths are not known apriori For example speech recognition and machine translation are sequential problems Likewise ques tion answering can also be seen as mapping a sequence of words representing the question to a 1 sequence of words representing the answer It is therefore clear that a domainindependent method that learns to map sequences to sequences would be useful Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and outputs is known and xed In this paper we show that a straightforward application of the Long ShortTerm Memory LSTM architecture 16 can solve general sequence to sequence problems The idea is to use one LSTM to read the input sequence one timestep at a time to obtain large xed dimensional vector representation and then to use another LSTM to extract the output sequence from that vector g 1 The second LSTM is essentially a recurrent neural network language model 28 23 30 except that it is conditioned on the input sequence The LSTMs ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application due to the considerable time lag between the inputs and their corresponding outputs g 1 There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks Our approach is closely related to Kalchbrenner and Blunsom 18 who were the rst to map the entire input sentence to vector and is related to Cho et al 5 although the latter was used only for rescoring hypotheses produced by a phrasebased system Graves 10 introduced a novel differentiable attention mechanism that allows neural networks to focus on dif ferent parts of their input and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al 2 The Connectionist Sequence Classication is another popular technique for mapping sequences to sequences with neural networks but it assumes a monotonic alignment between the inputs and the outputs 11 Figure 1 Our model reads an input sentence ABC and produces WXYZ as the output sentence The model stops making predictions after outputting the endofsentence token Note that the LSTM reads the input sentence in reverse because doing so introduces many short term dependencies in the data that make the optimization problem much easier The main result of this work is the following On the WMT14 English to French translation task we obtained a BLEU score of 3481 by directly extracting translations from an ensemble of 5 deep LSTMs with 384M parameters and 8000 dimensional state each using a simple lefttoright beam search decoder This is by far the best result achieved by direct translation with large neural net works For comparison the BLEU score of an SMT baseline on this dataset is 3330 29 The 3481 BLEU score was achieved by an LSTM with a vocabulary of 80k words so the score was penalized\n",
      "* [SIM=1.030722] 4 1 0 2 c e D 4 1  L C  s c  3 v 5 1 2 3  9 0 4 1  v i X r a Sequence to Sequence Learning with Neural Networks Ilya Sutskever Google ilyasugooglecom Oriol Vinyals Google vinyalsgooglecom Quoc V Le Google qvlgooglecom Abstract Deep Neural Networks DNNs are powerful models that have achieved excel lent performance on difcult learning tasks Although DNNs work well whenever large labeled training sets are available they cannot be used to map sequences to sequences In this paper we present a general endtoend approach to sequence learning that makes minimal assumptions on the sequence structure Our method uses a multilayered Long ShortTerm Memory LSTM to map the input sequence to a vector of a xed dimensionality and then another deep LSTM to decode the target sequence from the vector Our main result is that on an English to French translation task from the WMT14 dataset the translations produced by the LSTM achieve a BLEU score of 348 on the entire test set where the LSTMs BLEU score was penalized on outofvocabulary words Additionally the LSTM did not have difculty on long sentences For comparison a phrasebased SMT system achieves a BLEU score of 333 on the same dataset When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system its BLEU score increases to 365 which is close to the previous best result on this task The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas sive voice Finally we found that reversing the order of the words in all source sentences but not target sentences improved the LSTMs performance markedly because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier 1 Introduction Deep Neural Networks DNNs are extremely powerful machine learning models that achieve ex cellent performance on difcult problems such as speech recognition 13 7 and visual object recog nition 19 6 21 20 DNNs are powerful because they can perform arbitrary parallel computation for a modest number of steps A surprising example of the power of DNNs is their ability to sort N N bit numbers using only 2 hidden layers of quadratic size 27 So while neural networks are related to conventional statistical models they learn an intricate computation Furthermore large DNNs can be trained with supervised backpropagation whenever the labeled training set has enough information to specify the networks parameters Thus if there exists a parameter setting of a large DNN that achieves good results for example because humans can solve the task very rapidly supervised backpropagation will nd these parameters and solve the problem Despite their exibility and power DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of xed dimensionality It is a signicant limitation since many important problems are best expressed with\n",
      "* [SIM=0.744378] on a recurrent attention mechanism instead of sequence aligned recurrence and have been shown to perform well on simplelanguage question answering and language modeling tasks 34 To the best of our knowledge however the Transformer is the first transduction model relying entirely on selfattention to compute representations of its input and output without using sequence aligned RNNs or convolution In the following sections we will describe the Transformer motivate selfattention and discuss its advantages over models such as 17 18 and 9 3 Model Architecture Most competitive neural sequence transduction models have an encoderdecoder structure 5 2 35 Here the encoder maps an input sequence of symbol representations x1  xn to a sequence of continuous representations z  z1  zn Given z the decoder then generates an output sequence y1  ym of symbols one element at a time At each step the model is autoregressive 10 consuming the previously generated symbols as additional input when generating the next 2 Figure 1 The Transformer  model architecture The Transformer follows this overall architecture using stacked selfattention and pointwise fully connected layers for both the encoder and decoder shown in the left and right halves of Figure 1 respectively 31 Encoder and Decoder Stacks Encoder The encoder is composed of a stack of N  6 identical layers Each layer has two sublayers The first is a multihead selfattention mechanism and the second is a simple position wise fully connected feedforward network We employ a residual connection 11 around each of the two sublayers followed by layer normalization 1 That is the output of each sublayer is LayerNormx  Sublayerx where Sublayerx is the function implemented by the sublayer itself To facilitate these residual connections all sublayers in the model as well as the embedding layers produce outputs of dimension dmodel  512 Decoder The decoder is also composed of a stack of N  6 identical layers In addition to the two sublayers in each encoder layer the decoder inserts a third sublayer which performs multihead attention over the output of the encoder stack Similar to the encoder we employ residual connections around each of the sublayers followed by layer normalization We also modify the selfattention sublayer in the decoder stack to prevent positions from attending to subsequent positions This masking combined with fact that the output embeddings are offset by one position ensures that the predictions for position i can depend only on the known outputs at positions less than i 32 Attention An attention function can be described as mapping a query and a set of keyvalue pairs to an output where the query keys values and output are all vectors The output is computed as a weighted sum 3 Scaled DotProduct Attention MultiHead Attention Figure 2 left Scaled DotProduct Attention right MultiHead Attention consists of several attention layers running in parallel of the values where the weight assigned to each value is\n",
      "* [SIM=0.856663] of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation 35 2 5 Numerous efforts have since continued to push the boundaries of recurrent language models and encoderdecoder architectures 38 24 15 Recurrent models typically factor computation along the symbol positions of the input and output sequences Aligning the positions to steps in computation time they generate a sequence of hidden states ht as a function of the previous hidden state ht1 and the input for position t This inherently sequential nature precludes parallelization within training examples which becomes critical at longer sequence lengths as memory constraints limit batching across examples Recent work has achieved significant improvements in computational efficiency through factorization tricks 21 and conditional computation 32 while also improving model performance in case of the latter The fundamental constraint of sequential computation however remains Attention mechanisms have become an integral part of compelling sequence modeling and transduc tion models in various tasks allowing modeling of dependencies without regard to their distance in the input or output sequences 2 19 In all but a few cases 27 however such attention mechanisms are used in conjunction with a recurrent network In this work we propose the Transformer a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 16 ByteNet 18 and ConvS2S 9 all of which use convolutional neural networks as basic building block computing hidden representations in parallel for all input and output positions In these models the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions linearly for ConvS2S and logarithmically for ByteNet This makes it more difficult to learn dependencies between distant positions 12 In the Transformer this is reduced to a constant number of operations albeit at the cost of reduced effective resolution due to averaging attentionweighted positions an effect we counteract with MultiHead Attention as described in section 32 Selfattention sometimes called intraattention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence Selfattention has been used successfully in a variety of tasks including reading comprehension abstractive summarization textual entailment and learning taskindependent sentence representations 4 27 28 22 Endtoend memory networks are based on a recurrent attention mechanism instead of\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Description\n",
    "Perform similarity search for each source and accumulate relevant content.\n",
    "\n",
    "This code performs a similarity search for each unique source in the `sources` list by querying the vector store\n",
    "with the given query (`Query`) and filtering based on the `source` metadata. For each source, the top `k=5` most\n",
    "relevant results are retrieved, and their page content is aggregated into a single string. The resulting text for\n",
    "each source is appended to the `final_chunk` list, which will contain the concatenated content for each source.\n",
    "\n",
    "Args:\n",
    "    sources (list): A list of unique source names (e.g., filenames, URLs) from which documents were processed.\n",
    "    Query (str): The search query string used to find relevant documents in the vector store.\n",
    "    vector_store (Chroma): The vector store object containing the document embeddings.\n",
    "\n",
    "Returns:\n",
    "    final_chunk (list): A list where each element is a string containing the aggregated text from relevant\n",
    "                        documents for each unique source.\n",
    "\n",
    "Usage:\n",
    "    This approach allows retrieving, displaying, and storing text that is most relevant to a query,\n",
    "    organized by source. It helps in collecting results based on source metadata, which can then be analyzed or used\n",
    "    for further processing.\n",
    "\n",
    "Note:\n",
    "    The search results for each source are aggregated into a single text, with the similarity scores printed\n",
    "    alongside each page content. The final `final_chunk` list contains the combined content from the top `k` results\n",
    "    for each source.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "final_chunk=[]\n",
    "for source in sources:\n",
    "    results = vector_store.similarity_search_with_score(Query, k=2, filter={\"source\": source}\n",
    "    )\n",
    "    text=\"\"\n",
    "    for res, score in results:\n",
    "        print(f\"* [SIM={score:3f}] {res.page_content}\")\n",
    "        text+=res.page_content\n",
    "    final_chunk.append(text)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(final_chunk))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating  summary using Groq API endpoint and by using llama3-70b-8192 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer provided link to generate GROQ API key https://groq.com/\n",
    "# Set the Groq API key as an environment variable\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR-GROQ-API-KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries.\n",
    "from langchain_core.prompts import ChatPromptTemplate # Importing ChatPromptTemplate to create structured chat prompts\n",
    "from langchain_groq import ChatGroq # Importing ChatGroq to interact with Groq models\n",
    "\n",
    "# Initialize a ChatGroq object with the model name and temperature\n",
    "# 'temperature' controls the randomness of the model’s responses. A value of 0 will make the output deterministic, i.e., the same output for the same input.\n",
    "# 'model_name' specifies the language model to use for generating responses. In this case, we are using the \"llama3-70b-8192\" model.\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer added\n",
      "answer added\n",
      "answer added\n",
      "answer added\n",
      "answer added\n",
      "Here is a comprehensive explanation of the architecture of traditional sequence-to-sequence models like LSTM, GRU, and RNN, along with how Transformers improved upon these models, and Transformer architecture details:\n",
      "\n",
      "**Traditional Sequence-to-Sequence Models:**\n",
      "\n",
      "1. **Recurrent Neural Networks (RNNs):** RNNs are a type of neural network designed to handle sequential data. They have a feedback loop that allows information to persist, enabling the network to capture temporal dependencies in the data. RNNs are composed of three main components: an input gate, a hidden state, and an output gate. The hidden state captures information from previous time steps, and the output gate generates the output at each time step.\n",
      "\n",
      "2. **Long Short-Term Memory (LSTM) Networks:** LSTMs are a variant of RNNs that address the vanishing gradient problem, which occurs when gradients are backpropagated through time, causing them to become smaller and less effective. LSTMs introduce memory cells that can store information for long periods, allowing the network to learn long-term dependencies. LSTMs have three gates: an input gate, an output gate, and a forget gate, which control the flow of information into and out of the memory cells.\n",
      "\n",
      "3. **Gated Recurrent Units (GRUs):** GRUs are another variant of RNNs that simplify the architecture of LSTMs. They have only two gates: a reset gate and an update gate, which control the flow of information into the hidden state. GRUs are faster to train and require fewer parameters than LSTMs, but may not perform as well on tasks that require long-term dependencies.\n",
      "\n",
      "**Limitations of Traditional Sequence-to-Sequence Models:**\n",
      "\n",
      "Traditional sequence-to-sequence models like RNNs, LSTMs, and GRUs have several limitations:\n",
      "\n",
      "* **Vanishing gradients:** Gradients become smaller as they are backpropagated through time, making it difficult to train deep networks.\n",
      "* **Sequential processing:** RNNs process sequences sequentially, which can be slow and inefficient.\n",
      "* **Fixed-length context:** RNNs have a fixed-length context, which can limit their ability to model long-range dependencies.\n",
      "\n",
      "**Transformer Architecture:**\n",
      "\n",
      "The Transformer architecture, introduced in the paper \"Attention is All You Need\" by Vaswani et al. (2017), addresses the limitations of traditional sequence-to-sequence models. The Transformer architecture is based on self-attention mechanisms, which allow the model to attend to different parts of the input sequence simultaneously and weigh their importance.\n",
      "\n",
      "The Transformer architecture consists of an encoder and a decoder. The encoder takes in a sequence of tokens and outputs a sequence of vectors, called \"keys,\" \"values,\" and \"queries.\" The decoder generates the output sequence, one token at a time, by attending to the encoder output and generating a weighted sum of the values.\n",
      "\n",
      "**Transformer Advantages:**\n",
      "\n",
      "The Transformer architecture has several advantages over traditional sequence-to-sequence models:\n",
      "\n",
      "* **Parallelization:** The Transformer architecture can be parallelized more easily, as the self-attention mechanism allows the model to process the input sequence in parallel.\n",
      "* **Long-range dependencies:** The Transformer architecture can model long-range dependencies more effectively, as the self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously.\n",
      "* **Flexibility:** The Transformer architecture is more flexible, as it can be used for a wide range of sequence-to-sequence tasks, such as machine translation, text summarization, and text generation.\n",
      "\n",
      "In summary, traditional sequence-to-sequence models like RNNs, LSTMs, and GRUs have limitations such as vanishing gradients, sequential processing, and fixed-length context. The Transformer architecture addresses these limitations by using self-attention mechanisms, parallelization, and flexibility, making it a more powerful and efficient model for sequence-to-sequence tasks.\n",
      "\n",
      "The Transformer architecture consists of an encoder and a decoder. The encoder takes in a sequence of tokens and outputs a sequence of vectors, called \"keys,\" \"values,\" and \"queries.\" The decoder generates the output sequence, one token at a time, by attending to the encoder output and generating a weighted sum of the values.\n",
      "\n",
      "The key components of the Transformer architecture are:\n",
      "\n",
      "1. **Self-Attention Mechanism:** This mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance.\n",
      "2. **Multi-Head Attention:** This mechanism allows the model to attend to different representation subspaces at different positions.\n",
      "3. **Encoder-Decoder Structure:** The Transformer architecture uses an encoder-decoder structure, where the encoder takes in a sequence of tokens and outputs a sequence of vectors, and the decoder takes in the output of the encoder and generates a sequence of tokens.\n",
      "\n",
      "The Transformer architecture has several advantages over traditional sequence-to-sequence models, including parallelization, long-range dependencies, and flexibility. It has been widely used in many natural language processing tasks, such as machine translation, text summarization, and text generation.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script configures a language model to answer questions based on a set of research paper chunks, aiming to provide detailed, descriptive responses. \n",
    "It processes each chunk individually, gathers insights, and then combines these insights into a cohesive final answer. \n",
    "\n",
    "Code Description:\n",
    "1. **System Initialization**: Sets up the language model with a system message to guide it to act as a research-focused question-answering expert. \n",
    "   The system prompt incorporates the main query as context to help guide each answer towards the question's needs. \n",
    "\n",
    "2. **Chunk-Level Answer Generation**:\n",
    "   - For each chunk of the research papers, a prompt is created with a human message that combines the query and the specific chunk. \n",
    "   - This prompt is processed through the model to generate a relevant answer.\n",
    "   - Each generated answer is appended to a list, `answers`, to collect responses from each chunk.\n",
    "\n",
    "3. **Combining Chunk Answers**:\n",
    "   - Once all chunks have been processed, the answers from each chunk are joined to form a shortened, combined text. This consolidated text serves as a summary of insights across all chunks.\n",
    "\n",
    "4. **Final Answer Generation**:\n",
    "   - A final prompt is created with the combined text to generate a complete and well-structured answer, synthesizing insights across all chunks to respond to the question effectively.\n",
    "   \n",
    "5. **Output**:\n",
    "   - The final, cohesive answer is printed, providing a detailed response that draws on the entire set of research paper chunks.\n",
    "\n",
    "This code ensures that the final answer is not only factually accurate but also clear, comprehensive, and directly relevant to the initial query.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# System message setting up the model for providing detailed, descriptive answers based on research papers\n",
    "system = (\n",
    "    \"You are an expert in question answering based on research paper analysis. Your task is to examine and address questions \\\n",
    "     by synthesizing detailed information from the provided research paper chunks. Use the context provided by the query: {}, \\\n",
    "     to carefully analyze each chunk, focusing on the most relevant key insights that answer the question directly. \\\n",
    "     provide comprehensive and well-structured descriptive answers with minimum tokens, ensuring clarity, coherence, and depth of explanation. \"\n",
    ").format(Query)\n",
    "\n",
    "# List to collect individual answers from each chunk\n",
    "answers = []\n",
    "for chunk in final_chunk:\n",
    "    # Setting up the prompt for each chunk with both system and human context\n",
    "    human = f\"Given the question: {Query}, answer based on the following context: {chunk}\"\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"text\": f\"{chunk}\"})\n",
    "    output_response = clean_text(response.content)\n",
    "    answers.append(response.content)\n",
    "    print(\"answer added\")\n",
    "\n",
    "# Combine all individual answers into a smaller version of the full response\n",
    "shortened_text = \"\\n\".join(answers)\n",
    "\n",
    "# Final summary to provide a cohesive answer from all chunks combined\n",
    "final_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system), (\"human\", f\"Answer to the question: {Query} based on the following context: {shortened_text}\")]\n",
    ")\n",
    "final_chain = final_answer_prompt | llm\n",
    "final_response = final_chain.invoke({\"text\": shortened_text})\n",
    "\n",
    "# Print the final answer\n",
    "print(final_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to answer.txt.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Description\n",
    "This script formats and saves the summary text generated by the model into a text file, ensuring the output is readable and well-structured.\n",
    "\n",
    "Steps:\n",
    "1. **Output Text Assignment**: The `final_response.content` is assigned to the `output_text` variable, which holds the content to be formatted and saved.\n",
    "\n",
    "2. **File Preparation**: The `output_filename` is set to `\"summary_output.txt\"`, which specifies the file where the formatted text will be saved.\n",
    "\n",
    "3. **Text Formatting**:\n",
    "    - A `TextWrapper` object is used to wrap the text to a specific width (140 characters), with proper indentation (`4 spaces` for both initial and subsequent lines).\n",
    "    - Additional space is added before the main content to improve readability, followed by wrapping the text to the specified width.\n",
    "  \n",
    "4. **Text Writing**: The formatted text is then written to the file. The extra spaces ensure key points stand out, and the content is neatly wrapped for easier reading.\n",
    "\n",
    "5. **Final Output**: The script saves the formatted content to the file and prints a confirmation message indicating the file's location.\n",
    "\n",
    "This process ensures that the summarized content is not only stored but is also presented in a way that is easy to read and understand, with well-structured formatting.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import textwrap\n",
    "\n",
    "# Sample output text (replace this with your actual response)\n",
    "output_text = final_response.content\n",
    "\n",
    "# Define file name\n",
    "output_filename = \"answer.txt\"\n",
    "\n",
    "# Open the file in write mode and format the text\n",
    "with open(output_filename, \"w\") as file:\n",
    "    # Create a wrapper for text formatting with 140 character width and indentation\n",
    "    wrapper = textwrap.TextWrapper(width=140, initial_indent=\"    \", subsequent_indent=\"    \")\n",
    "\n",
    "    # Add space before key points to improve readability\n",
    "    formatted_text = \"\\n\\n\"  # Adds space before the content\n",
    "\n",
    "    # Wrap the main content text for readability\n",
    "    wrapped_text = wrapper.fill(str(output_text))\n",
    "    \n",
    "    # Add some extra space for main points separation\n",
    "    formatted_text += wrapped_text\n",
    "\n",
    "    # Write the formatted text to the file\n",
    "    file.write(formatted_text)\n",
    "\n",
    "print(f\"Output saved to {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#command to clear vector database\n",
    "#vector_store.delete(ids=uuids) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
